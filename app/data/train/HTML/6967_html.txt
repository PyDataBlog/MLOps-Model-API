<!doctype html>
<html lang="en">

	<head>
		<meta charset="utf-8">

		<title>Exploring Deep Learning Methods</title>

		<meta name="description" content="Initial presentation
                                                  of literature review">
		<meta name="author" content="Andreas Koestler">

		<meta name="apple-mobile-web-app-capable" content="yes" />
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/black.css" id="theme">

		<!-- Code syntax highlighting -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>

		<!--[if lt IE 9]>
		<script src="lib/js/html5shiv.js"></script>
		<![endif]-->
	</head>

	<body>
          
	  <div class="reveal">
            
	    <!-- Any section element inside of this container is displayed as a slide -->
	    <div class="slides">
	      <section>
		<h1>Exploring Deep Learning Methods</h1>
		<h3>for Discovering Features in Transaction Events</h3>
		<p>
		  <small>Andreas Koestler </small>
		</p>
	      </section>
              
	      <!-- Example of nested vertical slides -->
	      <section>
		<h2>Introduction</h2>
                <p>
                  <ul>
                    <b>Deep learning has revolutionalised</b>
                    <li>
                      Speech recognition
                    </li>
                    <li>
                      Object recognition
                    </li>
                    <b>Starting to impact</b>
                    <li>
                      NLP
                    </li>
                    <li>
                      Computer based translation
                    </li>
                    <li>
                      Reinforcemant learning, etc...
                    </li>
                  </ul>
                </p>
	      </section>
              <section>
                <h2>Motivation #1</h2>
                <p>
                  <ul>
                    <b> Learning (distributed)
                      representations </b>
                    <li>
                      Handcrafting features is
                      often difficult (Sift,
                      edge-detectore, etc)
                    </li>
                    <li>
                      Task/domain/context specific
                    </li>
                    <li>
                      Humans develop representations
                      for learning and reasoning
                    </li>
                    <li>
                      Deep learning provides ways of
                      doing this!
                    </li>
                  </ul>
                </p>
              </section>
              <section>
                <h2>Motivation #2</h2>
                <p>
                  <ul>
                    <b> The need for distributed
                      representations </b>
                    <li>
                      Combat curse of dimensionality
                    </li>
                    <li>
                      Represent many dimensions of
                      similarity
                    </li>                                      
                  </ul>
                </p>
              </section>
              <section>
                <h2>Local representations</h2>
                <p>
                  (Sparse) One-shot encoding
                  <ul>
                    <li>
                      $hotel=(0,0,0,0,0,0,0,0,1,0,0,0,0,0,0)$
                      $motel=(0,1,0,0,0,0,0,0,0,0,0,0,0,0,0)$
                    </li>
                    <li>
                      Generalizing locally requires examples for all
                      relevant variations
                    </li>
                  </ul>
                </p>
              </section>
              <section>
                <h2>Distributed representations</h2>
                <p>
		  <ul>
                    <li>
                      $hotel=(1,0,1,1,1,0)$
                    </li>
                    <li>
                      Concept central to
                      connectionism
                    </li>
                    <li>
                      <q>
                        "In a
                        connectionist network, a
                        distributed representation
                        occurs when some concept or
                        meaning is represented by the
                        network, but that meaning is
                        represented by a pattern of
                        activity across a number of
                        processing units." (Hinton et al,
                        1986)
                      </q>
                    </li>
                    <li>
                      Learning distributed features
                      can be exponentially more
                      efficient than
                      nearest-neighbor-like or
                      clustering-like models (curse
                      of dimensionality)
                    </li>
                  </ul>
                </p>
              </section>
              <section>
                <h2>Motivation #3</h2>
                <p>
                  <ul>
                    <b> Today, most practical ML
                      methods are supervised learning</b>
                    <li>
                      But almost all data is unlabeled
                    </li>
                    <b> Most information must be
                      acquired unsupervised</b>
                    <li>
                      Fortunately, unsupervised learning
                      a good model of observed data
                      boost classification performance
                    </li>
                  </ul>
                </p>
              </section>
              <section>
                <h2>Motivation #4</h2>
                <p>
                  <ul>
                    <b>Learning multiple levels of
                      representation/abstraction</b>
                    <li>
                      Intermediate representations
                      that can be shared across tasks                                      
                    </li>                                    
                    <li>
                      Multiple levels of latent variables
                      allow combinatorial sharing
                    </li>
                  </ul>
                </p>
              </section>
              <section>
                <h2>Motivation #5</h2>
                <p>
                  <ul>
                    <b>Why now?</b>
                    <li>
                      New methods for unsupervised
                      pre-training have been
                      developed (RBMs,
                      autoencoders, etc.)
                    </li>
                    <li>
                      More efficient parameter estimation methods
                    </li>
                    <li>
                      Better understanding of model
                      regularization (e.g. dropout)
                    </li>
                  </ul>
                </p>
              </section>
	      <section>
		<h2>Definitions #1</h2>
		<ul>
                  <li>
                    Finite set of Nodes $N = {\{u_1,u_2,...\}}$
                    and connections $H \subseteq
                    {N \times N}$
                  </li>
                  <li>
                    FNN acyclic, RNN cyclic
                  </li>
                  <li>
                    In FNN, $k-th$ layer $(k \gt
                    1)$ set of nodes $u \in N$
                    such that $\exists Path$ of
                    length $l \leq (k - 1)$
                  </li>
                  <li>
                    NN’s behavior is determined by
                    set of real-valued,
                    modifiable, parameters $w_i (i
                    = 1, ... , n)$
                  </li>
                  <li>
                    The NN’s behavior fully specified by a
                    set of parameters  $w_i (i = 1, ... ,
                    n), w_i \in \mathbb{R}$ and real-valued
                    activation function $f$,
                    typically $\tanh$ or the
                    logistic function.
                  </li>
                </ul>
	      </section>
	      <section>
		<h2>Definitions #2</h2>
		<p>
                  <ul>
                    <li>
                      <q>
                        “A field within machine
                        learning that is based on
                        algorithms for learning
                        multiple levels of
                        representation in order to
                        model complex relationships
                        among data. Higher-level
                        features and concepts are
                        thus defined in terms of
                        lower-level ones, and such a
                        hierarchy of features is
                        called a deep architec-
                        ture.”
                      </q>
                    </li>
                    <li>
                      <q>
                        “Deep Learning is a new area
                        of Machine Learning research,
                        which has been introduced with
                        the objective of moving
                        Machine Learning closer to one
                        of its original goals:
                        Artificial Intelligence."
                      </q>
                    </li>
                    <li>
                      <q>
                        "Machine learning
                        algorithms inspired by brains,
                        based on learning multiple
                        layers or
                        representation/abstraction."
                      </q>
                    </li>
                  </ul>
		</p>
	      </section>
              <section>
                <h2>Types of Deep Architectures</h2>
                <p>
                  <ul>
                    <li>
                      Autoencoders (denoising, sparse,
                      contracting, variational) (Bengio)
                    </li>
                    <li>
                      RBMs (Smolensky, 1986; Hinton)
                    </li>
                    <li>
                      DBNs (Hinton)
                    </li>
                    <li>
                      ... many more
                    </li>
                  </ul>
                </p>
              </section>
	      <section>
		<h2>Data</h2>
		<p>
                  <table>
                    <tbody>
                      <tr>
                        <td>event_id</td>
                        <td>dr_account_number</td>
                        <td>account_access_card</td>
            	      </tr>
                      <tr>
                        <td>event_start_time</td>
                        <td>cr_account_number</td>
                        <td>transaction_amount</td>
                      </tr>
                      <tr>
                        <td>event_end_time</td>
                        <td>dr_refn_trcd</td>
                        <td>event_subtype</td>
                      </tr>
                      <tr>
                        <td>trans_time</td>
                        <td>cr_refn_trcd</td>
                        <td>description</td>
                      </tr>
                      <tr>
                        <td>event_class</td>
                        <td>terminal_id </td>
                        <td>event_source_id</td>
                      </tr>
                      <tr>
                        <td>event_type</td>
                        <td>terminal_location</td>
                        <td>entity_roles</td>
                      </tr>
                      <tr>
                        <td>sa1</td>
                        <td>latitude</td>
                        <td>longitude</td>
                      </tr>
                    </tbody>
                  </table>
                </p>
	      </section>
              
	      <section>
		<h2>Data</h2>
                <p>
		  <ul>
		    <li>CC events, EFTPOS, BPAY,
		      Trasnfers, ATM
                    </li>
                    <li>Captures 40% of all
                      transactions in Australia
                    </li>
		  </ul>
                </p>
	      </section>
              <section>
		<h2>Directions</h2>
		<p>
		  <ul>
                    <b>The expectation is that</b>
                    <li>
                      high-level concepts/representations arise from the
                      internal representations in
                      the networks (e.g., customer price elasticity,
                      interests, need, etc.)
                    </li>
                    <li>
                      high-level
                      concepts/represenations can be
                      shared across tasks
                      (e.g. fraud detection,
                      purchase prediction, etc.)
                    </li>
                  </ul>
		</p>
	      </section>
              <section>
		<h2>Directions</h2>
		<p>
                  <ul>
                    <b>High-level tasks</b>
                    <li>
                      Tooling
                    </li>
                    <li>
                      Baseline model
                    </li>
                    <li>
                      Incremental improvement using DL
                    </li>
                    <b>Focus on</b>
                    <li>
                      Unsupervised (representation)
                      learning
                    </li>
                    <li>
                      Representation transfer
                    </li>
                    <li>
                      Efficient learning in highly
                      distributed (cluster) environment
                    </li>
                  </ul>
		</p>
	      </section>
              
              <section>
		<h2>Outlook</h2>
		<p>
                  <ul>
		    <b>Next steps</b>
                    <li>
                      Finish literature review
                    </li>
                    <li>
                      Write research proposal
                    </li>
                    <li>
                      Literature db, progress,
                      etc: <a href="http://github.com/AndreasKostler/sisyphus">Sisyphus</a>
                    </li>
                  </ul>
		</p>
	      </section>
	    </div>
            
	  </div>
          
	  <script src="lib/js/head.min.js"></script>
	  <script src="js/reveal.js"></script>
          
	  <script>
            
	    // Full list of configuration options available at:
	    // https://github.com/hakimel/reveal.js#configuration
	    Reveal.initialize({
            controls: true,
            progress: true,
	    history: true,
	    center: true,
            math: {
            mathjax: 'http://cdn.mathjax.org/mathjax/latest/MathJax.js',
            config: 'TeX-AMS_HTML-full'  // See http://docs.mathjax.org/en/latest/config-files.html
            },
            
	    transition: 'slide', // none/fade/slide/convex/concave/zoom
            
	    // Optional reveal.js plugins
	    dependencies: [
	    { src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
	    { src: 'plugin/math/math.js', async: true },
            { src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
	    { src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
	    { src: 'plugin/highlight/highlight.js', async: true, condition: function() { return !!document.querySelector( 'pre code' ); }, callback: function() { hljs.initHighlightingOnLoad(); } },
	    { src: 'plugin/zoom-js/zoom.js', async: true },
	    { src: 'plugin/notes/notes.js', async: true }
	    ]
	    });
            
	  </script>
          
	</body>
</html>

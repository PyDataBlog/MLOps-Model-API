\documentclass[11pt, oneside]{report}
\usepackage[utf8]{inputenc} 
\usepackage[T1]{fontenc}
\usepackage{geometry}
\geometry{a4paper} 
\geometry{
  left=40mm,
  right=40mm,
  top=35mm
}

\usepackage{floatrow}
\usepackage{xcolor}
\usepackage{amssymb}
\usepackage{listings}
\usepackage{caption}
\usepackage{hyperref}
\usepackage[scaled=0.85]{beramono}
\usepackage[scaled]{beraserif}
\usepackage[scaled]{berasans}
\usepackage{graphicx} 
\usepackage{lastpage}
\usepackage{fancyhdr}
\usepackage{setspace}
\usepackage{tabularx}
\usepackage{minted}
\usemintedstyle{borland}

\renewcommand{\listingscaption}{Code}

\renewcommand*\familydefault{\rmdefault} 
 
\floatsetup[listing]{style=Plaintop}
\lstset{
	numbers=left,
	basicstyle=\ttfamily\scriptsize,
	breakatwhitespace=false,
	frame=single,
	breaklines=true, 
	captionpos=t, 
	numbersep=10pt,
	numberstyle=\tiny,
	showspaces=false,
	tabsize=2,  
	language=C,
	label=DescriptiveLabel,
	commentstyle=\color{gray},
	keywordstyle=\color{blue}
}

\usepackage{titling}
\newcommand{\subtitle}[1]{
  \posttitle{
	\par\end{center}
	\begin{center}\large#1\end{center}
	\vskip0.5em}
}
\DeclareCaptionFormat{listing}{\rule{\dimexpr\textwidth+17pt\relax}{0.4pt}\vskip1pt#1#2#3}



% PAGE STYLE
\pagestyle{fancy}
\fancyhf{}

\lhead{ProgAlg}
\rhead{Implémentation d'un algorithme de tri}
\cfoot{Page \thepage \hspace{1pt} sur \pageref{LastPage}}

\fancypagestyle{plain}{
    \fancyhf{}
    \fancyhead[R]{Parallel computation and algorithms -- Projet pratique}
    \fancyfoot[C]{Page \thepage \hspace{1pt} sur \pageref{LastPage}}
    \renewcommand{\headrulewidth}{0pt}
    \renewcommand{\footrulewidth}{0pt}
}



% TITLE PAGE
\title{Algorithme de tri distribué}
\subtitle{Analyse et implémentation d'un algorithme de tri distribué}
\author{Alshweiki Mhd Ali \thanks{\ mhdali.alshweiki@master.hes-so.ch}\\ 
Gugger Joël \thanks{\ joel.gugger@master.hes-so.ch}\\ 
Marguet Steve-David \thanks{\ stevedavid.marguet@master.hes-so.ch} \\ \\ user: ggroup20@grid11}
\date{\today}



% TABLE OF CONTENTS
\renewcommand{\contentsname}{Contenu}


% CHAPTER NAME & TRADUCTIONS
\renewcommand{\chaptername}{Chapitre}
\renewcommand\appendixname{Annexe}
\renewcommand{\abstractname}{Résumé}


% PAGINATE
\pagenumbering{arabic}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                             DOCUMENT
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
\maketitle


\begin{abstract}
Le but de ce laboratoire est d'écrire un algorithme de tri qui s'exécute avec avec la meilleure performance possible sur les 10 machines à quatre coeurs mise à disposition.
\end{abstract}


\tableofcontents


\chapter{Choix de l'algorithme}
Notre choix s'est porté sur un tri bitonique. C'est un algorithme facilement parallélisable et réputé pour son efficacité. Il a été mis au point par Ken Batcher en 1968.

L'algorithme de tri proprement dit requiert un traitement préalable des données, afin de les agencer en une suite dite bitonique. C'est-à-dire une suite ascendante \textit{S1}, puis descendante \textit{S2} ou l'inverse. Comme chaque sous-suite \textit{S1} ou \textit{S2} est aussi une suite bitonique moyennant un décalage, il est aisé de décomposer une longue suite en de très nombreuses sous-suites en parallèle et de trier ensuite les sous-suite à l'image d'un mergesort. C'est le cas idéal où chaque élément est attribué à une seule unité de calcul.

Dans le cas où les dernières unités reçoivent une vecteur de scalaire et non un simple scalaire, le choix de la méthode de tri sur ces unités n'est pas défini par l'algorithme.

\chapter{Implémentation parallèle}
L'idée générale pour notre implémentation de l'algorithme est d'utiliser MPI pour distribuer le calcule sur les machines de la grille, puis OpenMP pour multithreader sur chaque machine. Le langage de programmation utilisé est le C.

Nous avons choisi une implémentation particulière en cela qu'elle éco-nomise le temps d'envoi du premier vecteur, en demandant à chaque nœud de calculer aléatoirement un vecteur de taille: $ \frac{taille\_totale}{nombre\_de\_noeuds} $, où la \textit{taille totale} est largement supérieure au \textit{nombre de nœuds}.

Sur la base du code de l'implémentation que nous avons trouvée, avons essayé de mettre en place une parallélisation sur chacune des machines avec OpenMP, mais sans succès car le code ne permettait pas assez de souplesse. Nous avons donc fait l'impasse sur ce second degrés de parallélisation. En outre nous avons eu des problèmes pour distribuer les calculs sur les machines de la grille, les calculs plantant de façon aléatoire. Notre code s'exécute cependant parfaitement sur une seule machine.

\newpage
\section{Initialisation}
Nous avons choisi d'initialiser le vecteur à trier dans chaque instance lancée par MPI. Ce qui nous permet d'initialiser une part proportionnel au nombre d'instances. 

Sur les vecteurs de grande taille nous gagnons en performances lorsque le nombre d'instance augmente.

\begin{listing}[H]    
	\caption{sortBit.c} 
	\inputminted[framesep=2mm,
	baselinestretch=1,
	fontsize=\footnotesize,
	linenos,
	firstline=250, lastline=260]{c}{../code/sortBit.c} 
\end{listing} 

Cette manière de faire n'est pas viable dans un cas concret, elle ne permet pas de travailler avec un vecteur passé en paramètre. Cependant, nous avons tout de même choisi cette implémentation car nous souhaitions analyser l'impact sur les performances. 

En effet, nous gagnions sur les temps d'allocation mémoire pour les grandes tailles de vecteurs, mais nous évitions aussi la phase de transfert des données qui est le goulot d'étranglement de cette première phase. En outre, plus le nombre de machine sur lesquelles le calcul est distribué est grand, plus le temps d'allocation est petit.

Nous avons ensuite une barrière de synchronisation pour nous assurer que les vecteurs sont tous initialisés avant le début du tri.

\begin{listing}[H]    
	\caption{sortBit.c} 
	\inputminted[framesep=2mm,
	baselinestretch=1,
	fontsize=\footnotesize,
	linenos,
	firstline=264, lastline=265]{c}{../code/sortBit.c} 
\end{listing} 

\section{Calcul}
Une fois la première phase terminée, on prépare les paramètres pour notre tri bitonique. Pour ce faire, un premier tri est appliqué au paramètre.

\begin{listing}[H]    
	\caption{sortBit.c} 
	\inputminted[framesep=2mm,
	baselinestretch=1,
	fontsize=\footnotesize,
	linenos,
	firstline=278, lastline=279]{c}{../code/sortBit.c} 
\end{listing} 

La fonction utilisée pour le tri permet de créer la suite bitonique. Elle est appelée sur le tableau initialisé par chaque instance.

\begin{listing}[H]    
	\caption{sortBit.c} 
	\inputminted[framesep=2mm,
	baselinestretch=1,
	fontsize=\footnotesize,
	linenos,
	firstline=45, lastline=50]{c}{../code/sortBit.c} 
\end{listing} 

Le tri bitonic est ensuite effectué. La variable "dimensions" de la première boucle  $= \log_2 numtasks$. La variable $numtasks$ étant égale au nombre de processus lancé par MPI.

\begin{listing}[H]
	\caption{sortBit.c}
	\inputminted[framesep=2mm,
	baselinestretch=1,
	fontsize=\footnotesize,
	breaklines=true,
	linenos,
	firstline=283, lastline=295]{c}{../code/sortBit.c}
\end{listing}

\chapter{Résultats \& Analyse}
\section{Résultats}
Les tailles de matrices ont été choisie de manière linéaire $f(x) = x * 10$, en partant de 70'000. Les résultats obtenus en séquentiel, avec l'argument \lstinline[]|mpirun -n 1|, sont les suivants :


\[
    \begin{tabular}{| r | r |}
        \hline
        Array size & Computing time (sec)\\
        \hline
        0.07M	& 0.013666667 \\
        0.7M		& 0.103833333 \\
        7M		& 1.219666667 \\
        70M		& 13.99233333 \\
        700M	& 162.2895  \\
        \hline
    \end{tabular}
\]

Lorsque l'on ramène l'échelle verticale du graphique en $log_{10}$, on obtient un résultat quasi linéaire. 

On constate un résultat normal étant donné que le temps d'exécution au pire des cas de l'algorithme sous sa forme parallèle est de $\theta (log^2 n)$ et que le résultat obtenu est équivalent à $\theta (n)$.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\linewidth]{graphs/compTime-seq}
    \caption[Temps séquentiel d'execution]{Temps séquentiel d'execution}
    \label{fig:SeqCompTime}
\end{figure}

\newpage

Lorsque l'on analyse le temps de calcul en fonction du nombre de cœurs, on constate un problème d'implémentation majeur. À partir de 8, les temps de calculs augmentent, ce qui n'est pas normal. Cependant, comme nous exécutons le code sur une seule machine quadricoeur, ce résultat est cohérent

La création de 4 processus fonctionne bien car les machines ont 4 cœurs, mais au-delà, la distribution ne sert plus à grand-chose.

L'échelle de l'axe vertical est la aussi en $log_{10}$.

\begin{figure}[ht]
    \centering
    \includegraphics[width=1\linewidth]{graphs/compTime-all}
    \caption[Temps d'execution en fonction du nombre de threads]{Temps d'execution en fonction du nombre de threads}
    \label{fig:AllCompTime}
\end{figure}

\newpage
\section{Speedup}
Le speedup mesuré reflète bien la problématique énoncée auparavant. À partir de quatre threads, le speedup diminue pour tomber à zéro dans le meilleur des cas.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\linewidth]{graphs/speedup}
    \caption[SpeedUp du temps d'execution]{SpeedUp du temps d'execution}
    \label{fig:CompSpeedup}
\end{figure}


\section{Efficacité}

Encore une fois, l'efficacité mesurée reflète bien le problème d'implémentation. Celle-ci chute drastiquement lorsque l'on passe le nombre de 4 processus.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\linewidth]{graphs/efficiency}
    \caption[Efficacité du temps d'execution]{Efficacité du temps d'execution}
    \label{fig:CompEfficiency}
\end{figure}

\chapter{Conclusion}
Suite au problème d'implémentation, nous n'avons pas pu distribuer le calcul correctement sur toute la grille. Le nombre de "thread" mesuré n'est en réalité pas représentatif du nombre de cœurs utilisé et nous n'utilisons pas MPI de manière optimale.

Malgré tout, nous pouvons constater des résultats cohérents lorsque l'on varie de 1 à 4 processus lancés avec MPI. Ce qui semble indiquer une mise en pratique cohérente de l'algorithme bitonique.


\newpage
\vspace*{6 cm}
\begin{center}
\textbf{Sources du projet} \\
\ \\
Le code source du projet est disponible sur GitHub à l'adresse suivante : \\
\href{https://github.com/T-ProgAlg/ProgAlg-Lab4}{https://github.com/T-ProgAlg/ProgAlg-Lab4}
\end{center}


\end{document}  

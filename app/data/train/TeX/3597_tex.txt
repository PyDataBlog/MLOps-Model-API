% uw-wkrpt-se.tex - An example work report that uses uw-wkrpt.cls
% Copyright (C) 2002,2003  Simon Law
% 
% This program is free software; you can redistribute it and/or modify
% it under the terms of the GNU General Public License as published by
% the Free Software Foundation; either version 2 of the License, or
% (at your option) any later version.
% 
% This program is distributed in the hope that it will be useful,
% but WITHOUT ANY WARRANTY; without even the implied warranty of
% MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
% GNU General Public License for more details.
% 
% You should have received a copy of the GNU General Public License
% along with this program; if not, write to the Free Software
% Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% We begin by calling the workreport class which includes all the
% definitions for the macros we will use.
\documentclass[se]{uw-wkrpt}

% We will use some packages to add functionality
\usepackage{graphicx} % Include graphic importing

% Now we will begin writing the document.
\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% IMPORTANT INFORMATION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%% First we, should create a title page.  This is done below:
% Fill in the title of your report.
\title{Design of an Interprocess Communication Library}

% Fill in your employer's name.
\employer{Virtium Incorporated}

% Fill in your employer's city and province.
\employeraddress{Rancho Santa Margarita, CA}

% Fill in your term.
\term{3B}

% If you want to specify the date, fill it in here.  If you comment out
% this line, today's date will be substituted.
%\date{April 26, 2003}

% If you are writing a "Confidential 1" report, uncomment the next line.
%\confidential{Confidential-1}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Information that wont change
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\author{Jake Nielsen}

\uwid{20338042}

\address{40 Karch st.,\\*
         Cambridge, ON\ \ N3C 1Y5}

\school{University of Waterloo}

\faculty{Mechanical and Mechatronics Engineering}

\email{jake.k.nielsen@gmail.com}

\program{Mechatronics Engineering}

\chair{Dr.\ Pearl\ Sullivan}

\chairaddress{Mechanical and Mechatronics Engineering Department,\\*
              University of Waterloo,\\*
	      Waterloo, ON\ \ N2L 3G1}



% Now, we ask LaTeX to generate the title.
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% FRONT MATTER
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% \frontmatter will make the \section commands ignore their numbering,
%% it will also use roman page numbers.
\frontmatter

% After this, we must create a letter of submission.
\begin{letter}
I have just completed my fifth work term, following my \theterm{} term.
Please find enclosed my first work term report entitled:
``\thetitle'' at \theemployer.  

Virtium Inc. is a company that specializes in the assembly and programming of
NAND flash solid state drives. As one of the software engineers on the 
firmware team at Virtium Inc. I was mainly responsible for the simulation 
system used to test firmware code as it was developed, but without having 
to fatigue a physical SSD.

In the writing of this report, I have had no direct assistance from anyone. 
I do wish to thank Ho-Fan Kang, Trevor Chau, Lan Phan, and my excellent fellow
interns at Virtium for their exceptional support and assistance as we learned
about producing highly performant code in C++. To date, this has been the best
experience that I have had on a co-op term, due in large part to the excellent
people that I was working with.

% Note that I do not need to type out the boilerplate confirmation,
% nor do I need to write a signature block.  This is generated for me.
% We are now finished with the letter.
\end{letter}

% Next, we need to make a Table of Contents, List of Figures and 
% List of Tables.  You will most likely need to run LaTeX twice to
% get these correct.  The first pass for LaTeX to figure out the
% labels, and the second pass to put in the right references.
\tableofcontents

\section{Summary}

The purpose of this report is to examine one of the design choices that I
had to make during this term. The simulation system that was used in place
of a real SSD was a separate process from the testing framework that was
used to interact with the drive. As such, an interprocess communication
library needed to be made to facilitate the interaction between the two processes.
Due to the volume and complexity of the tests, it was essential that this
library provide as high throughput as possible. This is the challenge that will
be examined throughout this report.

Since throughput was the most important metric, the first unit tests that
were added to the library were actually benchmark tests. The tests allocated
two threads that used the library to send variously sized packets unidirectionally
from one thread to the other until 4GB had been received by the listening thread.
Using that time and the fact that 4GB had been communicated, the bandwidth could be
calculated. The size of mapped memory to use, and the size of the communicated chunks
were the only two variables that could be changed, so a series of benchmark tests using
64KB, 1MB, 4MB, and 32MB of mapped memory, and 512B, 4KB, 16KB, and 64KB packet sizes
were run every time the library changed.

By the end of the term, speeds of up to 7GB/s could be achieved using
mapped memory (with 1MB of mapped memory, and 64KB packet size). It turned
out that after around 1MB of mapped memory was allocated, there was almost no
benefit from increasing the memory size. Performance increased as the packet size
increased, except once the packet size reached around 1/4 of the allocated memory size.

Throughout the process, unit tests were added to test as much of the library
as possible. These included tests that allocated non-even memory sizes for the mapped
memory and for the packet sizes, tests that verify the received information matched
the passed information for threads communicating simultaneously and bi-directionally through the library,
tests that allocated 10 communicating thread-pairs that also verified data integrity,
tests that ensured that timeout exceptions were thrown in the event that one of the
threads dies. 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% REPORT BODY
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% \main will make the \section commands numbered again,
%% it will also use arabic page numbers.
\mainmatter

% You must have an Introduction
\section{Introduction}\label{sec:intro}


The problem of interprocess communication is not uncommon, and there
are several ways to implement it. The different implementations have
trade-offs, but they're all useful under the correct circumstances. For low-bandwidth
applications, message passing is the easiest to implement, and can be used to pass
whole objects from process-to-process directly out-of-the-box. For mid-bandwidth applications,
named pipes can be used. Any data passed into a named pipe will make it to the listening process
in the same order that it was written, which greatly simplifies the use of named-pipes as compared
to mapped memory. When high bandwidth is absolutely necessary, then mapped memory offers extremely
high bandwidth, but is very difficult to use properly.

In order to understand which is the most ideal, the use-case and programming
environment needs to be examined first. In this case, both communicating
processes are written in C++, and both processes are running on the same
machine. Further, the host machine is running post-2007 Windows, although
we don't have any assurances about which release of Windows. From boost::mapped\_region::get\_page\_size();,
it was determined that the page size is 16KB.

\subsection{Constraints and Criteria}

The requirements for this library are as follows: The two processes need to be able to send serialized data in both directions,
and the data streams can be anywhere from several bytes, to several gigabytes
at a time. The data is assured to be byte-aligned, but there are no assurances
about how many bytes will be transfered at a time. One process is assured to be initialized before the other communicating process begins.
The second process may be created and destroyed multiple times, and in that event,
the second process needs to be able to "pick up where it left off". Although
there are no hard constraints on throughput, the library must be as high throughput as possible.

With these constraints in mind, three common methods of interprocess communication
will be examined and evaluated to determine their viability for this application.

\section{Common Interprocess Communication Techniques}

\subsection{Message Passing}

\subsubsection{Overview}

Message passing is one of the most common and simple ways of implementing
interprocess communication. Message passing is a service provided by the operating
system that allows you to send and receive messages between processes. In most
implementations (such as boost::interprocess::message\_queue) all that's required
to connect two processes is for both of them to create a message\_queue object with
the same key. After the object is created, messages can be inserted or extracted from
the message queue, thereby providing a way for the two processes to communicate.

\subsubsection{Benefits}

Message passing is one of the easiest ways of implementing interprocess communication.
It requires no explicit process synchronization (that's all handled by the library). It's
flexible because the "messages" can be text, or full-fledged objects. Message passing
can be a form of communication between more than just two processes (many speakers, many listeners). Message passing is
cross-platform with the right libraries (like boost).

\subsubsection{Costs}

Message passing tends to have very low bandwidth. Its intended purpose is not to transfer
a lot of data, it's to signal other processes to start/stop tasks, request the status
of processes, notify processes of events occurring, and the like.

\subsection{Named Pipes}

\subsubsection{Overview}

Named pipes is a fairly bandwidth-performant choice for interprocess communication, and
is easier to use than memory mapping. When using named pipes, the communicating processes 
need to "open" a pipe, and agree on the name of the pipe. For example, using the windows API
one process could create a pipe using the CreateNamedPipe function (which returns a handle to the pipe),
amd the connecting process could connect to the pipe by using the CreateFile function (which also returns a handle to the
pipe). Depending on the initialization parameters given to CreateNamedPipe, the pipe may
be one-way, or it may be bi-directional. The two processes can then send or receive data byte-by-byte
with their pipe handle.

\subsubsection{Benefits}

Named pipes are reasonably fast. From the tests that I performed during the term, you can 
expect transfer speeds in the range of 200 MB/s using named pipes (Unfortunately I no longer have
access to the codebase that I worked on, and as such I cannot formally include this test in the report).
Named pipes still abstract synchronization issues from the user, and lend themselves towards raw data 
transfer between processes. Named pipes are available in any operating system.

\subsubsection{Costs}

Although they are available in any operating system, the implementation varies, so for true
cross-platform use, a wrapper library would be required. Named pipes are harder to use
than message passing. Some glue-code is required to send objects down a named pipe.

\subsection{Mapped Memory}

\subsubsection{Overview}

Mapped memory is a method of interprocess communication that nearly allows both processes to access
the same memory simultaneously. The truth of the matter is, both processes have access to their own
memory space, but there is a very performant data propagation process that synchronizes the two memory
spaces. That is a very obscure, but very important detail about mapped memory that absolutely cannot be
neglected in the design of an IPC system that uses it. Memory mapping is unique as an IPC method in that
it is passive. No explicit calls need to be made to propagate data from one process to the other, the data
simply passively propagates. This is both a pro, and a con, because it's extremely performant, but also 
extremely difficult to design around. Because there are no assurances about the order in which data propagates
from one process to the other, some other form of IPC must also be used to co-ordinate data transfer using
memory mapping.

\subsubsection{Benefits}

Mapped memory is very easy to set up. The boost library provides a shared\_memory\_object and mapped\_region class
that make it very easy to instantiate a region of shared memory between the two processes that wish to communicate.
From further testing that I've done, speeds of up to 7 GB/s can be achieved with mapped memory, and hence memory mapping
is by far the most performant choice for interprocess communication.

\subsubsection{Costs}

Absolutely no synchronization is assured. When the memory is being synchronized between processes, there are no
assurances of the order in which the memory regions will propagate, and as such, it is extremely difficult to
ensure data integrity between the two processes. Building a custom library around the mapped region is, for
all intents and purposes, not optional, and extremely non-trivial.

\section{Implementation Details}

\subsection{Overview}

For this particular application, mapped memory is the ideal choice. Despite all of the implementation nastiness
of making a system that relies upon mapped memory, the performance is orders of magnitude better than any other IPC
method. In the following sections, many of the challenges that emerge from using mapped memory will be examined, and
solutions will be proposed to solve these challenges.

\subsection{General Design}

The first challenge is the complete lack of structure within a memory mapped region. Because it is literally just
lightning-fast self-propagating memory, there are many strategies that can be used to use this in a meaningful way,
and the ideal strategy is very problem-specific. It can be used to house objects that will automatically propagate
from process to process, or it can be manipulated like raw-data to send streams of data between processes. 

In this case,
A large stream of data needs to be sent from process to process, and specifically, it must be a two-way communication
platform. One of the easiest and most performant ways to create a one-way communication platform on top of shared memory,
is to use a circular buffer. Since this is only limited by the size of the buffer, two circular buffers can be constructed
in shared memory so that two-way communication can take place through the two buffers. Each buffer requires some metadata
in order to keep track of the next byte to write/read. These are called the producer and consumer pointers.
They are stored in the mapped memory as well so that the two processes can confer on which bytes need reading, and 
which can be written.

\subsection{Data Integrity}

This general design is not bad, but due to the unknowable order of propagation of the memory between the processes,
the integrity of the data cannot be guaranteed. There are several ways to solve this, but most kill performance. Adding
a checksum to the end of the metadata in the mapped region can help the reading process to know that the data it's reading
is actually in a good state, but it massively impacts performance. Luckily, there is one fact about this propagation that
we know. It happens one page at a time (page sizes are system-specific, but in this case, a page is 16KB). There's no way 
to know the order of which pages propagate when, but if a mapped region were 16KB or less in size, then it can be assured
that data will update all at once, and hence data integrity is assured without damaging performance.

\subsection{Synchronization}

Yet another performance killer is the problem of synchronization. On a system with 8 cores, it may be reasonable to 
assume that the two communicating processes may be getting CPU time at the same time. If this is the case, then they
can tight poll for data to be available, and for data to be writable with almost no penalty, because the other
communicating process is literally running at the same time. The issue of synchronization starts to arise when the
two processes are not scheduled at the same time. In that case, the writing process will fill the buffer, and wait
for data to be read so that it can write more, but it will be waiting indefinitely until either it is done its time
slice, and the next process is scheduled on that core, or until its partner process is given time on another core.
The worst case scenario is when 7 of the 8 cores are occupied with other, persistent processes, and the communicating
processes are both sharing the same core. In that case, they'll each get scheduled, read/write as much as they can
until the buffer is full, and get stuck until the OS decides that they've had enough time on the CPU. This is
not acceptable, and absolutely kills performance. 

The solution, is to use a special subset of message passing
called events. Events can block in an efficient way. In particular, when one process is done reading as much as 
it can, but knows that there is more data to be read, it can block on a write event, that way, the OS knows to
unschedule that process immediately, and not to schedule it again until its partner process has triggered the write
event. Similarly, the writing process can block on a read event when it runs out of space in the buffer, and be 
unscheduled by the OS until the other process can get time on the CPU and trigger the read event.

\subsection{Deadlocking}

The solution so far seems robust on the surface, but it's not. There is a tricky assumption that is being relied
on in this solution. The assumption is that the mapped memory will propagate before the event propagates. Unfortunately,
that assumption does not hold. Even if the writing process writes to the mapped memory, and then triggers the event, there
is no guarantee that the memory it wrote will propagate before the event does. The reason that this causes problems
is that the reading process may see the event trigger, look at the number of bytes available, and see that 12 bytes are
available to be read. If the read process is actually waiting on 20 bytes, then it will read the 12 bytes, and wait for
a write event so that it can read the remaining 8 bytes. Meanwhile, the write process has actually already sent the full
20 bytes, but the data did not propagate fast enough for the read process to see it. Now the read process is waiting
on an event that will never get triggered.

Ironically, the solution to this is to use message queues. Since message queues can also block efficiently, messages can
be used as the communication channel to co-ordinate the memmap data transfer level. Every time that the writing process
writes, it can send a message saying how many bytes it wrote. The reading process can then wait patiently for the mapped
memory to update until it sees that the correct number of bytes is available, read that many bytes, and send a message
back to the writing process to let it know that its bytes have been read. This can get very messy. There are actually
4 types of messages being sent here. Each process needs to send a read or write message to the other process.

\section{Conclusions}

Performant interprocess communication is difficult, and a lot of time can be saved by reasoning about the system
before writing a single piece of code. I was fortunate to be working for a patient company that understands that
good code takes time, and even still I ran into many challenges that I was not expecting. Take heed that synchronization
is not easy, and is made harder by the fact that it is time dependent, and therefore harder to debug. 

In applications that don't require high throughput, message passing is ideal. In applications that require higher bandwidth, but
don't require extremely high bandwidth, named pipes should be used. In applications that absolutely require high bandwidth,
mapped memory should be used.

\section{Recommendations}

Because of the relative ease of message passing over named pipes, for synchronization and simple, low-bandwidth purposes
message passing is ideal. Named pipes are reasonably performant, and orders of magnitude easier to implement than
mapped memory, so for applications that need to transfer data streams, but are not bottlenecked by it, named pipes are
ideal. For applications that absolutely demand performance, a mapped memory library is the ideal solution, although certainly
not the easy one.

\end{document}

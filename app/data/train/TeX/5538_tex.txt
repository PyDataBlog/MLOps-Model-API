\documentclass[11pt]{article}

\usepackage[american]{babel}
\usepackage{amsmath,amssymb,amsthm,enumitem}

\usepackage[left=2cm, right=2cm, top=3cm, bottom=2cm, head=1cm, foot=1.0cm]{geometry}
\geometry{a4paper}

\title{Summary of the Generalized Delta Rule \\ for Backpropagation in Neural Network}
\author{Jaeyeong Yang}
\date{April 26, 2017}

\begin{document}

\maketitle

\section{Derivation}

Let $x_p$ and $t_p$ be the input and the target respectively for the $p$th training session, and $z_{pi}$ and $a_{pi}$ be the net input and the activation value respectively for $i$th node.

Then, we can compute $z_{pj}$ and $a_{pj}$ with the activation function $f$ as follows:
\begin{align}
  z_{pj} &= \sum_i w_{ji} o_{pi}, \\
  a_{pj} &= f(z_{pj}).
\end{align}

With the given cost function $E$, define $\Delta w_{ji} = - \eta \frac{\partial E_p}{\partial w_{ji}}$ where $E_p$ be the cost of $p$th training session and $\eta$ be the learning rate.

\begin{align}
  - \frac{\partial E_p}{\partial w_{ji}}
  &= - \underbrace{\frac{\partial E_p}{\partial a_{pj}}}_{A}
  \underbrace{\frac{\partial a_{pj}}{\partial z_{pj}}}_{B}
  \underbrace{\frac{\partial z_{pj}}{\partial w_{ji}}}_{C}
  = \delta_{pj} \frac{\partial z_{pj}}{\partial w_{ji}} \\
  \delta_{pj} &= -AB
  = -\frac{\partial E_p}{\partial a_{pj}} \frac{\partial a_{pj}}{\partial z_{pj}}
\end{align}

\begin{align}
  A = \frac{\partial E_p}{\partial a_{pj}} &=
  \begin{cases}
    \frac{\partial E_p}{\partial a_{pj}}
      & j \in L_\text{output} \\
    \sum_k \delta_{pk} w_{kj}
      & j \in L_\text{hidden} \\
  \end{cases} \\
  B = \frac{\partial a_{pj}}{\partial z_{pj}} &= f'(z_{pj}) \\
  C = \frac{\partial z_{pj}}{\partial w_{ji}} &= a_{pi}
\end{align}

\begin{align}
  \delta_{pj} &=
  \begin{cases}
    - f'(z_{pj}) \frac{\partial E_p}{\partial a_{pj}}
      & j \in L_\text{output} \\
    f'(z_{pj}) \sum_k \delta_{pk} w_{kj}
      & j \in L_\text{hidden} \\
  \end{cases} \\
  - \frac{\partial E_p}{\partial w_{ji}} &= \delta_{pj} a_{pi} \\
  \Delta w_{ji} &= \eta \delta_{pj} a_{pi}
\end{align}

\pagebreak
\section{Using MSE as Cost Function}

Define $E$ be {\bf mean squared error(MSE)} such that can be calculated as below:
\begin{align}
  E_p &= \frac{1}{2} \sum_{j \in L_\text{out}} (t_{pj} - a_{pj})^2 \\
  \frac{\partial E_p}{\partial a_{pj}} &= - (t_{pj} - a_{pj})
\end{align}
Then, $\delta_{pj}$ and $\Delta w_{ji}$ are equated as follows:
\begin{align}
  \delta_{pj} &= \begin{cases}
    f'(z_{pj}) (t_{pj} - a_{pj})
      & j \in L_\text{output} \\
    f'(z_{pj}) \sum_k \delta_{pk} w_{kj}
      & j \in L_\text{hidden} \\
  \end{cases} \\
  \Delta w_{ji} &= \begin{cases}
    \eta f'(z_{pj}) (t_{pj} - a_{pj}) a_{pi}
      & j \in L_\text{output} \\
    \eta f'(z_{pj}) \left[ \sum_k \delta_{pk} w_{kj} \right] a_{pi}
      & j \in L_\text{hidden} \\
  \end{cases}
\end{align}

\section{Using CE as Cost Function}

Assume that all output activation $a_{pj}$ are located in the interval $[0, 1]$.
Define $E$ be {\bf cross-entropy error(CE)} such that can be calculated as below:
\begin{align}
  E_p &= - \sum_{j \in L_\text{out}} \left[
    t_{pj} \log a_{pj} + (1 - t_{pj}) \log (1 - a_{pj})
  \right] \\
  \frac{\partial E_p}{\partial a_{pj}}
  &= - \left( \frac{t_{pj}}{a_{pj}} - \frac{1 - t_{pj}}{1 - a_{pj}} \right)
  = - \frac{t_{pj} - a_{pj}}{a_{pj}(1 - a_{pj})}
\end{align}
where $N$ is the size of output.
Then, $\delta_{pj}$ and $\Delta w_{ji}$ are equated as follows:
\begin{align}
  \delta_{pj} &= \begin{cases}
    f'(z_{pj}) \frac{t_{pj} - a_{pj}}{a_{pj}(1 - a_{pj})}
      & j \in L_\text{output} \\
    f'(z_{pj}) \sum_k \delta_{pk} w_{kj}
      & j \in L_\text{hidden} \\
  \end{cases} \\
  \Delta w_{ji} &= \begin{cases}
    \eta f'(z_{pj}) \frac{t_{pj} - a_{pj}}{a_{pj}(1 - a_{pj})} a_{pi}
      & j \in L_\text{output} \\
    \eta f'(z_{pj}) \left[ \sum_k \delta_{pk} w_{kj} \right] a_{pi}
      & j \in L_\text{hidden} \\
  \end{cases}
\end{align}

If the activation function $f$ is the sigmoid function $f(x) = \frac{1}{1 + \exp(-x)}$,
since $f'(z_{pj}) = f(z_{pj}) (1 - f(z_{pj})) = a_{pj} (1 - a_{pj})$,
\begin{align}
  \delta_{pj} &= \begin{cases}
    t_{pj} - a_{pj}
      & j \in L_\text{output} \\
    f'(z_{pj}) \sum_k \delta_{pk} w_{kj}
      & j \in L_\text{hidden} \\
  \end{cases} \\
  \Delta w_{ji} &= \begin{cases}
    \eta (t_{pj} - a_{pj}) a_{pi}
      & j \in L_\text{output} \\
    \eta f'(z_{pj}) \left[ \sum_k \delta_{pk} w_{kj} \right] a_{pi}
      & j \in L_\text{hidden} \\
  \end{cases}
\end{align}

\end{document}

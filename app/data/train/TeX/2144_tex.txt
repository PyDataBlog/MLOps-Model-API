\documentclass{report}

%\usepackage{../latex_sources/packages/these-pkg}

\usepackage{these-pkg}

\graphicspath{ {../images/} {../images/fig_paper/} }


\newcommand{\PhysicalSystem}{\mathcal{G}}
\newcommand{\model}{\mathcal{M}}
\newcommand{\equations}{\mathbf{F}}
\newcommand{\ParamSpace}{\mathcal{P}}
\newcommand{\param}{P}

\newcommand{\PrediSpace}{\mathcal{Q}}
\newcommand{\predi}{q}
\newcommand{\DataSpace}{\mathcal{D}}
\newcommand{\data}{d}

\newcommand{\getref}[1]{{\color{red} \large REF : #1}}
\newcommand{\getpaperref}[1]{{\color{red} \large REF : paper : #1}}
\newcommand{\needref}{{\color{red} \large REF}}
\newcommand{\needinfo}[1]{{\color{red} \large INFO : #1}}
\newcommand{\getlabel}{{\color{blue} \large Get label}}

\newcommand{\params}{P^*}
\newcommand{\osubpredi}{q^{'}}

\newcommand{\Espace}{\mathcal{E}}
\newcommand{\refeq}[1]{Eq. (\ref{#1})}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% new commands used in the paper : %%%%%%%%

\newcommand{\ubaru}{\langle \bar u u \rangle}
%\newcommand{\mrm}{\mathrm}
%\newcommand{\dd}{\mathrm{d}}
\newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\pdd}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\beq}{\begin{eqnarray}}
\newcommand{\eeq}{\end{eqnarray}}
\usepackage{xcolor}
\newcommand{\changing}[1] {{\color{red} #1 }}

\newcommand{\idest}{\textit{i.e.}~}
\newcommand{\paperparam}{\Lambda,m_0,G}


\newcommand{\reffig}[1]{{\color{black}\mbox{Fig. (\ref{#1})}}}
\newcommand{\reftab}[1]{{\color{black}\mbox{Tab. \ref{#1}}}}
%\newcommand{\refeq}[1]{{\color{black}\mbox{Eq. (\ref{#1})}}}
\newcommand{\refeqs}[3]{{\color{black}\mbox{Eqs. (\ref{#1},\ref{#2},\ref{#3})}}}
\newcommand{\refsec}[1]{{\color{black}\mbox{Sec. (\ref{#1})}}}
\newcommand{\refapp}[1]{{\color{black}\mbox{Sec. (\ref{#1})}}}

%%%%%%%%%%%%%

\usepackage{grffile}



\begin{document}

\selectlanguage{french}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Sensibilité de certaines prédictions des modèles de type NJL}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}


L'étude d'un système physique peut être décomposé en deux parties :
étude expérimentale et étude théorique. Les expériences induisent des
théories, et les théories permettent de prédire les résultats des
expériences. La comparaison entre les résultats prédits par une
théorie et les resultats obtenus par une expérience, permet en
principe d'améliorer la théorie. Ils permettent aussi de réfuter une
théorie si la différence entre les résultats est inacceptable
\getref{Tarantola: Popper 1959 }.
% ISBN 3-16-148410-X 
% ISBN 0-415-27844-9
% link to pdf : http://strangebeautiful.com/other-texts/popper-logic-scientific-discovery.pdf
% http://www.worldlibrary.org/Articles/The%20Logic%20of%20Scientific%20Discovery?&Words=%22Logik%20der%20Forschung%22%20%22The%20Logic%20of%20Research%22

Le fait d'utiliser une théorie pour faire des prédictions est appelé
problème direct. Le problème inverse consiste à utiliser des résultats
pour déterminer la valeur des paramètres d'un modèle. Bien qu'en
physique deterministe, le problème direct ait une solution unique, ce
n'est en général pas le cas pour le problème inverse. D'après
\getref{Kern: 41}, deux problèmes sont dits inverses l'un de l'autre
si la formulation de l'un met en cause celle de l'autre. De manière
plus pragmatique, le problème direct consiste à determiner les effets
connaissant les causes, alors que le problème inverse consiste à
retrouver les causes de certains effets. Avec cette définition, il est
facile de voir que le problème inverse peut avoir plusieurs solutions
: des causes différentes peuvent avoir les mêmes effets. Pour une
discussion détaillée des problèmes inverses, et de leurs résolution,
l'on peut se référer au livre \getref{Tarantola}.

En se référant à \getref{Kern 32}, un problème est dit \emph{bien
  posé} lorsque (i) une solution au problème existe, (ii) qu'elle est
unique et (iii) qu'elle dépend continument des données. De part sa
définition, un problème inverse est donc en général mal posé, ne
remplissant pas l'une ou l'autre de ces conditions, et dans la plupart
des cas, les trois ensembles. Dans le cas des problèmes inverse, la
non-existence d'une solution n'est pas un problème \emph{per se} : la
notion de solution pouvant toujours être modifiée. La non-unicité de
la solution est un problème plus sérieux, car si plusieurs solutions
existent, il faut pouvoir faire un choix. Pour cela, il faut la
connaissance d'une information \emph{a priori}. La non-continuité est
le problème le plus sérieux, puisque dans un tel cas, il n'est pas
possible d'approcher la solution convenablement. 

Avant d'aller plus avant, il est necessaire de fixer la
notation. Notons $\PhysicalSystem$ le système physique étudié. Notons
$\model$, le modèle théorique permettant son étude. Par construction,
le modèle dépend d'un ensemble de paramètres $\param$, et par essence,
d'un ensemble d'équations $\equations$, en général non-linéaires,
dépendantes des paramètres. L'application des équations aux paramètres
permet d'obtenir l'ensemble des prédictions $\PrediSpace$ du
modèle. L'étape précédente est typiquement ce que l'on a appelé le
problème direct dans les paragraphes précédent. L'ensemble des données
connues relatives au système physique $\PhysicalSystem$ considéré, est
noté $\DataSpace$, et contient aussi bien des données expérimentales,
que des prédictions faites par d'autres modèles.

Résoudre le problème inverse consiste à choisir un sous-ensemble des
données $\data \subset \DataSpace$ et un sous-ensemble des prédictions
$\predi \subset \PrediSpace$, en bijection avec $\data$.  Résoudre le
problème inverse consiste à résoudre $\equations^{-1}(\data) =
\param$ (où $\textrm{dim}(\data) = \textrm{dim}(\param)$). Comme
expliqué précédement, lorsque ce système n'a pas de solution, il est
possible de relaxer la notion de solution, en définissant la notion de
distance entre un élément de $\data$ et son homologue dans
$\predi$, et en définissant une fonction de mérite, dépendant des
paramètres du modèle. Il suffit alors de chercher les paramètres
minimisant la distance entre chaque éléments des sous-ensembles
choisis.

Généralement, les quantités prédites par le modèle ne peuvent pas être
identiques aux données choisies pour deux raisons : les incertitudes
expérimentales, et les imperfections du modèle. Il semble qu'il soit
admis que ces deux différentes sources d'erreur produisent des
incertitudes de même ordre : lorsque de nouvelles méthodes
expérimentales permettent de diminuer l'incertitude sur les mesures,
des théories sont créees ou modifiées pour rendre compte plus
précisement de ces nouveaux résultats. Une analyse rigoureuse des
erreurs dues aux imperfections du modèle doit être faite pour que le
problème inverse associé soit le mieux posé possible. Bien que la
manière d'évaluer les incertitudes de mesure est un sujet bien
compris, ce n'est pas le cas pour la modélisation des erreurs
théoriques, pour lesquelles il n'existe pas de conventions acceptées
par tout le monde. La point de vue développé dans \getref{Tarantola}
est que toute source d'information, aussi bien théorique
qu'expérimentale, est décrite par une densité de probabilité. En ce
sens, le problème inverse est vu comme la conjonction de deux états
d'information, et sa solution représentée par une densité de
probabilité $\sigma_{\model}$, permettant d'obtenir la
probabilité pour que les paramètres du modèle appartiennent à une
region de l'espace où ils sont définis, connaissant les données
utilisées et leurs incertitudes.

L'analyse des incertitudes de modélisation se fait alors par l'étude
de $\sigma_{\model}$. Lorsque l'espace des paramètres est linéaire, et
que cette densité de probabilité est proche d'une distribution
gaussienne, il est possible d'obtenir des informations pertinentes en
calculant les différents moments de $\sigma_{\model}$. Lorsque ce
n'est pas le cas, les moments de la distribution perdent de leur sens,
et il convient d'utiliser d'autres méthodes pour l'étudier. 

La résolution des problèmes inverses est un sujet intéressant en soi,
et a été partie intégrante du travail de thèse. Dans les sections
\getlabel, une possibilité de résolution est décrite de manière assez
formelle, sans spécifier le système physique à l'étude. Le lecteur
seulement intéressé par les résultats obtenus dans le cas des modèles
de type NJL, peut directement passer aux sections \getlabel.

L'étude des problèmes inverses est un domaine de recherche qui a pris
naissance dans les années 30, avec un papier \getref{Zeitschrift fur
  Physik, Vol. 53, Nos. 9-10, p. 690-695, 1929}, dans lequel les
auteurs essaient de retrouver l'équation aux dérivées partielles
menant à certaines classes de valeurs propres. Bien que ce papier
n'eut pas un succès retentissant à sa publication, il est, à ma
connaissance, la base des travaux concernant la résolution des
problèmes inverses. 

Telle que nous la formulons ici, la résolution des problèmes inverses
nécessite des connaissances en théorie des probabilités. Concernant
les probabilités, j'ai utilisé \getref{books on probability theory,
  Tarantola Jeffreys 1929, Tarantola Jaynes
  2003,http://science.jrank.org/pages/10865/Probability-BIBLIOGRAPHY.html
}. Concernant les problèmes inverses et leur résolution, il existe de
très nombreuses références récentes, parmis lesquelles j'ai utilisé
\getref{books on inverse problem theory}. Historiquement, les travaux
\getref{Tarantola (sec 6.15) Backus 1970a,b,c ; Tarantola : Backus and
  Gilbert 1970}, sont parmis les premiers a traiter de la résolution
des problèmes inverses d'un point de vue probabiliste (dans le cas de
problêmes linéaires cependant).





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Solution générale d'un problème inverse}

Avant d'étudier la solution $\sigma_{\model}$, il convient de la
construire. Nous restons pour le moment assez général, et nous nous
focaliserons sur des exemples précis dans les sections
suivantes. Reprenons les notations précédement introduites. Nous
étudions un système physique $\PhysicalSystem$, grâce à un modèle
$\model$ dépendant de paramètres $\param$ vivant dans l'espace
$\ParamSpace$. Ces paramètres sont à comparer avec des données $\data$,
vivant dans l'espace $\DataSpace$. Pour simplifier, nous ne
considérons que le cas où $\ParamSpace$ et $\DataSpace$ sont de
dimension finie.

Comme nous l'avons dit dans l'introduction, nous postulons que toute
source d'information peut être décrite par une distribution de
probabilité. L'on peut donc définir une distribution de probabilité
sur $\ParamSpace$, c'est à dire une fonction $P$ associant à n'importe
quel sous-ensemble de $\ParamSpace$ un nombre réel positif, avec la
normalisation $P(\ParamSpace)=1$. Une telle distribution de
probabilité peut etre définie sur n'importe quel espace de dimension
finie, et indépendament d'un choix spécifique d'un des elements de
l'espace. Si un point de l'espace a été choisie, c'est à dire une
paramétrisation donnée, il est possible de décrire cette distribution
de probabilité grâce à une densité de probabilité. Nous pouvons
remarquer au passage qu'une paramétrisation donnée $\param$ n'est
réprésenté par un vecteur que si l'espace $\ParamSpace$ est
linéaire. Dans la plupart des cas, $\ParamSpace$ est non-linéaire.

Pour obtenir des informations sur les paramètres du modèle, il faut
choisir des données indépendantes du modèle. Dans le cas usuel, le
terme \emph{données} correspond effectivement aux données
expérimentales. Dans ce cas, l'espace $\DataSpace$ peut etre vu comme
l'ensemble des résultats pouvant être obtenu lors de n'importe quelle
expérience. Un point précis de cet espace correspond à un résultat
\emph{exact} d'une expérience. De manière générale, le résultat d'une
expérience donne accès à la valeur de plusieurs observables. Donc
chaque point de $\DataSpace$ correspond à un ensemble de valeurs pour
des observables. De la même manière que pour l'espace des paramètres,
un point de $\DataSpace$ ne peut être vu comme un vecteur que si
l'espace dans lequel il est défini est linéaire. Cependant, les
données expérimentales sont le résultat d'expériences et sont donc
sujettes à des incertitudes. Le résultat d'une mesure n'est donc pas
seulement la valeur d'un ensemble d'observable, mais un état
d'information pouvant être représenté par une densité de probabilité.

Comme dit dans l'introduction, l'espace des données contient également
les résultats obtenus par d'autre modèles théoriques. Si l'on accepte
que chaque modèle possède des incertitudes, alors l'on peut se
convaincre que le fait d'ajouter des résultats théoriques à l'ensemble
des données est possible, et n'est pas un nonsens.

À partir des deux ensembles $\ParamSpace$ et $\DataSpace$, l'on peut
définir l'espace ``physique'' du problème, construit par le produit
cartésien $\ParamSpace \times \DataSpace$. De la même manière une
distribution de probabilité peut être choisie sur cet espace, et une
densité de probabilité peut lui être associée lorsqu'un point de cet
espace est choisi.





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Rappel de définitions}

Considérons un espace $\Espace$ pouvant être aussi bien l'espace
des données, que l'espace des paramètres. Considérons $\Espace_1$
et $\Espace_2$ deux sous-ensembles de $\Espace$. Ces
sous-ensembles peuvent tout aussi bien être un seul point, ou un
ensemble discontinu de points, ou encore une région entière de
$\Espace$ (c'est à dire un ensemble continu de points). En théorie
des probabilités un tel sous-ensemble est appelé un événement. Le
champ des événements est appelé un champ $\sigma$. Cette notion permet
d'introduire la notion de probabilité de manière générale, pour des
espaces de dimensions finies et infinie \getref{cf inside tex}.
%http://d-nb.info/1017788650
%;https://openlibrary.org/books/OL2707518M/Real_and_complex_analysis ;
%http://d-nb.info/963746200 ;
%https://www.worldcat.org/search?qt=wikipedia&q=isbn\%3A9780470317952}.
Comme nous nous restreignons aux ensembles finis, il n'est pas
nécessaire de les introduire.


%%%%%%%%%%%%%%%
\paragraph{Probabilité :\\}

Par définition, on appelle mesure sur $\Espace$, une application $P$
qui associe un nombre réel positif à n'importe quel élément
$\Espace_i$ de $\Espace$. Pour être une mesure, $P$ doit satisfaire
les axiomes de Kolmogorov \getref{ Doob, J. L. "The Development of
  Rigor in Mathematical Probability (1900-1950)." Amer. Math. Monthly
  103, 586-595, 1996. ; Papoulis, A. Probability, Random Variables,
  and Stochastic Processes, 2nd ed. New York: McGraw-Hill, pp. 26-28,
  1984. }: (i) $0 \leq P(\Espace_i) \leq 1$, (ii) $P(\Espace_i) +
P(\bar{\Espace}_1) = 0$, où $\bar{\Espace}_1$ est le complement de
$\Espace_1$ dans $\Espace$, et (iii) $P(\Espace_1 \bigcup \Espace_2
\bigcup \Espace_3 \dots) = \sum_i P (\Espace_i)$, où les $\Espace_i$
sont des sous-ensembles disjoints. De (ii), il vient immédiatement que
la probabilité de l'ensemble vide est nul, et de (iii), il vient que
pour deux sous ensembles $\Espace_1$ et $\Espace_2$ non-nécessairement
disjoints que $P(\Espace_1 \bigcup \Espace_2) = P(\Espace_1) +
P(\Espace_2) - P ( \Espace_1 \bigcap \Espace_2 )$. Pour que $P$ puisse
être considérée comme une probabilité, il faut que $P(\Espace) =
1$\footnote{Ou du moins soit finie, et dans ce cas normalisé à
  $1$.}. Dans ce cas, $P$ est appelée une distribution de probabilité,
et $P(\Espace_1)$ est appelée la probabilité de \emph{l'évenement}
$\Espace_1$.

Si un système de coordonnées $x = \{x_1, x_2, \dots\}$ a été choisie
sur l'ensemble $\Espace$, et étant donné $P$ une distribution de
probabilité sur cet espace, il existe, de part le théorème de
Radon-Nikodyn \getref{Fundamenta Mathematicae 15: 131–179}, une
fonction $g(x)$ positive tel que la probabilité d'un sous-ensemble
$\Espace_1 \subseteq \Espace$, $P(\Espace_1)$ est donné par
l'intégrale :
\begin{equation}
  \label{eq-densite-proba}
  P(\Espace_1) = \int_{\Espace_1} \; \dd x \; g(x) \;.
\end{equation} 
La fonction $g(x)$ est appelée densité de probabilité représentant la
distribution de probabilité $P$ dans un système de coordonnées
choisi. En général, $g(x)$ est une distribution, mais nous ne ferons
pas de distinction.

Dans le cadre du problème inverse, il n'est pas nécéssaire d'avoir accès
à la probabilité d'un évenement donné. La plupart du temps, nous
sommes seulement intéréssés par la probabilité relative de deux
évenements, qui peut être obtenue en considérant des mesures qui ne
sont pas des probabilités. Nous ne ferons pas de distinction et nous
appelerons aussi, par abus de langage, probabilité, des mesures qui
n'en sont pas.


%%%%%%%%%%%%%%%
\paragraph{Distribution de probabilité homogène: \\} 

Considérons que la notion de volume existe dans $\Espace$ : à chaque
sous-ensemble $\Espace_i \subseteq \Espace$, l'on peut associer un
volume $V(\Espace_1)$. Dans un système de coordonnée $x$ donné, l'on
peut écrire l'élement différentiel de volume comme
\begin{equation}
  \dd V(\Espace_i) = v(x) \; \dd x \;,
\end{equation}
où $v(x)$ est appelée densité volumique de $\Espace_i$ dans le système
de coordonnée $x$. Le volume d'un évenement $\Espace_i$ est alors donné par
\begin{equation}
  V(\Espace_i) = \int_{\Espace_i} \; \dd x \; v(x) \;.
\end{equation}

Si le volume de $\Espace$ est fini
\begin{equation}
  \label{eq:def-volume-evenement}
  V(\Espace) = V = \int_{\Espace} \; \dd x \; v(x) \;,
\end{equation}
l'on peut définir une densité de probabilité normalisée
\begin{equation}
  \label{eq:def-densite-prob-homo}
  \mu(x) = v(x) / V \;,
\end{equation}
associant à n'importe quel sous-ensemble $\Espace_i
\subseteq \Espace$ la probabilité 
\begin{equation}
  P_h(\Espace_i) = \int_{\Espace_i} \; \dd x \; \mu(x) \;.
\end{equation}
De part la définition de $\mu(x)$, cette probabilité est
proportionelle au volume de $\Espace_i$. La probabilité $P_h$ et la
densité de probabilité $\mu(x)$ sont dites \emph{homogène}. Dans cette
définition, homogène ne signifie pas constante mais signifie
proportionelle au volume d'un évenement \getref{Tarantola : Mosegaard
  and Tarantola 2002}. Si le système de coordonné choisi est
cartésien, alors l'élement différentielle de volume est $\dd V =
\prod_i \dd x_i$, et la densité de probabilité $\mu(x)$ est
constante. Pour un système de coordonnées arbitraire, la densité de
probabilité est proportionnelle à la densité volumique $v(x)$ (qui
peut ne pas être constante).

Lorsque la notion de volume peut être définie, il peut être plus aisé
de représenter la probabilité d'un évenement $\Espace_1
\subseteq \Espace$, dans un système de coordonnées $x$ donné, à l'aide
de la probabilité volumique $G(x)$:
\begin{equation}
  P(\Espace_1) = \int_{\Espace_1} \; \dd V(x) \; G(x) \;.
\end{equation}
Alors que la densité de probabilité $g(x)$ définie par
\refeq{eq-densite-proba} est dépendante du système de coordonnée, la
probabilité volumique ne l'est pas. Ainsi, la probabilité volumique
homogène associée àla distribution de probabilité homogène $P_h$ est
constante quelque soit le système de coordonnées choisi. En remplaçant
simplement $\dd V(x)$ par son expression
\refeq{eq:def-volume-evenement}, on trouve que $G(x) = g(x) /
v(x)$. De part sa définition \refeq{eq:def-densite-prob-homo}, l'on
peut également contruire la fonction
\begin{equation}
  \gamma (x) = \frac{g(x)}{\mu(x)} \;,
\end{equation}
ayant les mêmes propriétés que la probabilité volumique et entrant
dans la définition de la mesure de l'information contenue dans $g(x)$
\getref{Tarantola Shannon 1984 ; Ellis, R. S. Entropy, Large
  Deviations, and Statistical Mechanics. New York: Springer-Verlag,
  1985.; Khinchin, A. I. Mathematical Foundations of Information
  Theory. New York: Dover, 1957. ; Rothstein, J. "Information,
  Measurement, and Quantum Mechanics." Science 114, 171-175, 1951. ;
  Shannon, C. E. "A Mathematical Theory of Communication." The Bell
  System Technical J. 27, 379-423 and 623-656, July and
  Oct. 1948. ;Shannon, C. E. and Weaver, W. Mathematical Theory of
  Communication. Urbana, IL: University of Illinois Press, 1963. }:
\begin{equation}
  I ( g, \mu ) = \int \; \dd x \; g(x) \log ( \gamma(x) ) \;.
\end{equation}



%%%%%%%%%%%%%%%
\paragraph{Probabilité conditionelle : \\}

Pour introduire de manière logique la notion de probabilité
conditionnelle, introduisons les notions de conjonction et de
disjontion de probabilité, représentées par les opérateurs logiques
$\wedge$ (ET), et $\vee$ (OU). Pour $P_1$ et $P_2$ deux distributions
de probabilité sur $\Espace$ et $\Espace_1 \subseteq \Espace$, ces
opérateurs logiques doivent être commutatifs $(P_1 \wedge
P_2)(\Espace_1) = (P_2 \wedge P_1)(\Espace_1)$ et $(P_1 \vee
P_2)(\Espace_1) = (P_2 \vee P_1)(\Espace_1)$, et $P_h$, la
distribution de probabilité homogène sert d'élément neutre pour
$\wedge$ : $(P_1 \wedge P_h) = P_1$. Nous cherchons une expression
pour la conjonction et disjonction des densités de probabilité
$g_1(x)$ et $g_2(x)$ associées aux distributions de probabilités $P_1$
et $P_2$ respectivement. \needinfo{Pour cela, nous reproduisons dans
  les grandes lignes} \getref{Tarantola : Tarantola and Valette
  1982a}.

Par définition, pour tous $\Espace_1 \subseteq \Espace$ :
\begin{align}
  \label{eq:disjonction}
  (P_1 \wedge P_2)(\Espace_1) &= \int_{\Espace_1} \; \dd x \;
  (f_1\wedge f_2) (x) \;;\\ 
  \label{eq:conjonction}
  (P_1 \vee P_2)(\Espace_1) &=
  \int_{\Espace_1} \; \dd x \; (f_1\vee f_2) (x) \;.
\end{align}

Dans \getref{Tarantola : Tarantola and Valette (1982a)}, les auteurs
prouvent que les solutions les plus simples sont :
\begin{align}
  (f_1 \vee f_2)(x) &= \frac{1}{2} \left( f_1(x) + f_2(x) \right) \;;
  \\ (f_1 \wedge f_2)(x) &= \frac{1}{\int_\Espace \; \dd \; x
    \frac{f_1(x)f_2(x)}{\mu(x)}} \frac{f_1(x)f_2(x)}{\mu(x)} \;.
\end{align}
La dernière égalité n'est définie que si le produit $f_1(x) f_2(x)
\neq 0$ $\forall x$.

Ces deux égalités permettent de définir la notion de probabilité
conditionnelle. 

Considérons un évenement $\Espace_1$ de $\Espace$.  Appelons $P_1$ et
$P_2$ deux distributions de probabilité définies dans $\Espace$, et
$f_1$ et $f_2$ les densités de probabilités associées. À chaque
événement $\Espace_1$, on peut associer une distribution de
probabilité particulière $H_{\Espace_1}$, nommée en anglais $p-event$,
caractérisée par la densité de probabilité $\mu_{\Espace_1}$, nulle de
partout sauf à l'intérieur de $\Espace_1$ où elle est proportionelle à
la densité de probabilité homogène $\mu(x)$, c'est à dire définie par :
\begin{equation}
  \mu_{\Espace_1} =
  \left\{
  \begin{aligned}
    c \mu(x) &\quad \forall x \in \Espace_1 \;, \\
    0 &\quad \forall x \notin \Espace_1 \;.
  \end{aligned}
  \right.
\end{equation}
où $c \in \mathbb{R}$. Comme la densité homogène $\mu(x)$ est relié à
l'élement de volume de $\Espace$, la probabilité
$H_{\Espace_1}(\Espace_2) = \int_{\Espace_2} \, \dd x \,
\mu_{\Espace_1}(x)$ d'un autre évenement $\Espace_2 \subseteq \Espace$
est réliée au volume de l'intersection des deux ensembles $\Espace_1$
et $\Espace_2$ : $\Espace_1 \cap \Espace_2$. Étant donné un évenement
$\Espace_1 \subseteq \Espace$ et sa distribution p-event associée
$H_{\Espace_1}$, ainsi qu'une distribution de probabilité $P$ définie
sur $\Espace$, la conjonction des distributions $P$ et $H_{\Espace_1}$
est appelé distribution de probabilité conditionnelle de $P$ sachant
$\Espace_1$. En utilisant la formule \refeq{eq:conjonction}, et en
considérant $P(\Espace_1) \neq 0$ on trouve :
\begin{equation}
  ( P \wedge H_{\Espace_1} )(\Espace_2) = \frac{P(\Espace_2
    \bigcap \Espace_1) }{P(\Espace_1)} \;.
\end{equation}
Le membre de droite de cette dernière équation est en général posé par
définition de la distribution de probabilité conditionelle :
\begin{equation}
  \label{eq:probabilite-conditionnelle}
  P ( \Espace_1 | \Espace_2 ) = \frac{P(\Espace_1
    \bigcap \Espace_2) }{P(\Espace_2)} \;,
\end{equation}
où $\Espace_1$ et $\Espace_2$ sont deux éléments de $\Espace$. On
retrouve alors $P(\Espace_1 | \Espace_2)$ comme cas particulier de la
conjonction $( P \wedge Q  )(\Espace_1)$, où $Q = H_{\Espace_2}$. 

En renversant l'équation \refeq{eq:probabilite-conditionnelle}, et en
se souvenant que l'opérateur de conjonction $\wedge$ est commutatif,
on retrouver le théorème de Bayes \getref{Papoulis, A. "Bayes' Theorem
  in Statistics" and "Bayes' Theorem in Statistics (Reexamined)." §3-5
  and 4-4 in Probability, Random Variables, and Stochastic Processes,
  2nd ed. New York: McGraw-Hill, pp. 38-39, 78-81, and 112-114,
  1984. }:
\begin{equation}
  P ( \Espace_2 | \Espace_1 ) = \frac{P ( \Espace_1 | \Espace_2 ) P
    ( \Espace_2 )}{P(\Espace_1)} \;.
\end{equation}
Ce théorème permet d'obtenir la probabilité d'une cause $\Espace_2$
produisant un effet donné $\Espace_1$ connaissant les probabilités des
évenements $\Espace_1$ et $\Espace_2$ séparément et de la probabilité
de l'effet $\Espace_1$ lorsque la cause est connue et est
$\Espace_2$\footnote{Citons en tant qu'article historique
  \getref{http://rstl.royalsocietypublishing.org/content/53/370}, dans
  lequel on peut trouver la phrase, peut-être plus compréhensive
  (traduite en anglais courant) : ``[...] to find out a method by
  which we might judge concerning the probability that an event has to
  happen, in given circumstances, uppon supposition that we know
  nothing concerning it but that, under the same circumstances it has
  happen a certain number of times, and failed another certain number
  of times.'' } . Lorsque deux évenements sont indépendents, la
probabilité de l'intersection de ces deux évenements est le produit
des probabilités de chacun des évenements $P ( \Espace_1
\cap \Espace_2 ) = P( \Espace_1) P (\Espace_2)$. Il en suit que, dans
ce cas, les probabilités conditionnelles sont égales au probabilité :
$P (\Espace_2 | \Espace_1) = P ( \Espace_2)$.  probabilités
conditionelles égales aux probabilités inconditionnelles.



%%%%%%%%%%%%%%%
\paragraph{Densité de probabilité marginale : \\}

Considérons à nouveau l'espace $\Espace$, mais cette fois-ci construit
comme le produit cartésien de deux ensembles distincts $\Espace_1$
dont les points sont donnés par $x_1$ et $\Espace_2$ dont les points
sont donnés par $x_2$. $\Espace = \Espace_1 \times \Espace_2$ et ses
points sont donnés par $y = \{x_1,x_2\}$. Une distribution de
probabilité sur $\Espace$ est représentée par une densité de
probabilité $f(y) = f(x_1,x_2)$ (considérée normalisée). L'on peut
définir les densités de probabilités marginales comme la densité de
probabilité d'une variables moyennant l'information que nous avons sur
l'autre :
\begin{align}
  f_{\Espace_1} (x_1) &= \int_{\Espace_2} \; \dd x_2 \; f(x_1,x_2) \;;\\
  f_{\Espace_2} (x_2) &= \int_{\Espace_1} \; \dd x_1 \; f(x_1,x_2) \;.\\
\end{align}
La densité de probabilité $f(y) = f(x_1,x_2)$ est appelée densité de
probabilité jointe. Elle est égale au produit des deux distributions
marginales lorsque les variables $x_1$ et $x_2$ sont
indépendentes.

De la même manière que nous avons introduit les distributions de
probabilité conditionnelle, nous pouvons introctuire les densités de
probabilité conditionnelle. Pour cela, il faut nous donner une
application $g$ permettant de relier les elements de $\Espace_1$ aux
éléments de $\Espace_2$. Cette application est à voir comme une
condition permettant de selectionner les élements de $f(x_1,x_2)$ qui
satisfont à cette condition, les autres étant nuls. Introduire de
manière générale les densité de probabilité conditionnelle est
compliqué car dépendant du choix de l'application. Dans le cas simple,
lorsque l'application choisie est linéaire, et où les points $x_1^i$
et $x_2^i$ forment des coordonnées cartésiennes, l'on peut
écrire\footnote{Cette définition reste valide \getref{Tarantola :
    Mosegaard and Tarantola (2002)} lorsque l'application $g$ n'est
  pas trop non-linéaire, et que les coordonnées ne sont pas trop
  non-cartésiennes. Un exemple de problème ``pas trop'' non linéaire
  est donné en \refsec{sec : sensibilite de la prediction du CEP dans
    un modele NJL simple}} :
\begin{equation}
  f_{\Espace_1 | \Espace_2} ( x_1 | x_2 = g ( x_1 ) ) = \frac{f(x_1,
    g(x_1))}{ \int_{\Espace_1} \;\dd x\; f(x,g(x))} \;.
\end{equation}

Dans le cas où l'application $g$ associe une valeur $x_2^0$ constante
à toutes valeur de $x_1$, on peut écrire la densité de probabilté
jointe comme le produit de la densité de probabilité conditionnelle et
la densité de probabilité marginale :
\begin{equation}
  \label{eq:jointe-condi}
  f(x_1,x_2^0) = f_{\Espace_1 | \Espace_2} ( x_1 | x_2^0 ) f_{\Espace_2}(x_2^0) \;.
\end{equation}

En introduisant une autre applciation $g^{'}$ associant à chaque
élément de $\Espace_2$ un élément de $\Espace_1$ constant, $x_1^0$, on
peut réécrire le théorème de Bayes en utilisant les densités de
probabilté conditionnelle :
\begin{equation}
  f_{\Espace_1 | \Espace_2} (x_1 | x_2^0) = \frac{ f_{\Espace_2
      | \Espace_1}(x_2 | x_1^0) f_{\Espace_1}(x_1^0)}{f_{\Espace_2}(x_2^0)} \;.
\end{equation} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Résolution du problème du problème inverse}

Nous reprenons les notations utilisées dans
l'introduction. $\ParamSpace$ est l'espace des paramètres d'un modèle
$\model$ décrivant un système physique $\PhysicalSystem$. Le modèle
est caractérisé par un ensemble de paramètres $\param$ et par un
ensemble d'équations $\equations$. $\DataSpace$ est l'ensemble des
données connues concernant le système physique étudié. L'espace des
paramètres est caractérisé par un système de coordonnées $p = \{ p_1,
p_2, \dots \}$, et par une densité de probabilité homogène $\mu_P
(p)$. De la même manière, l'espace des données est caractérisée par
des coordonnées $d = \{d_1, \dots\}$ et par une densité de probabilité
homogène $\mu_D(d)$. L'espace physique $\Espace$ est donné par le
produit cartésien de $\ParamSpace \times \DataSpace$ et est
caractérisé par les coordonnés $e = (p,d) = \{ p_1, p_2, \dots, d_1,
d_2,\dots\}$, et par sa densité de probabilité homogène jointe
$\mu_E(e) = \mu(p,d) = \mu_D(d) \mu_P(p)$. Notons $\Theta(p,d)$ la
densité de probabilité jointe décrivant les liens entre les données et
les paramètres du modèle. Elle correspond à l'application $g$ utilisée
pour définir les densités de probabilité conditionnelle dans le
paragraphe précédent. Si les hypothèses faites dans le paragraphe
précédent sont vérifées, alors on peut toujours écrire la densité de
probabilité jointe $\Theta(p,d)$ comme le produit d'une densité de
probabilité conditionnelle $\theta(d|p)$ et d'une densité de
probabilité marginale. En choisissant la densité homogène pour les
paramètres $\mu_P(p)$ comme densité marginale, l'on peut écrire :
\begin{equation}
  \label{eq:correlation-data-model}
  \Theta(p,d) = \theta(d|p) \mu_P(p) \;.
\end{equation}

Comme expliqué dans l'introduction, chaque mesure est affectée par une
incertitude, et le résultat d'une mesure peut être décrite par une
densité de probabilité $\rho_D(d)$ définie sur $\DataSpace$. 

D'autre part, les informations, a priori, que l'on a sur les
paramètres du modèles peuvent être décrite par une densité de
probabilité $\rho_P(p)$ définie sur $\ParamSpace$. Lorsqu'aucune
informations n'est connue concernant les paramètres du modèle, autre
que leur définition, cette densité de probabilité est la densité de
probabilité homogène : $\rho_P(p) = \mu_P(p)$.

La totalité des informations a priori que l'on a peuvent décrite dans
$\Espace$ par la densité de proabilité jointe $\rho(p,d)$. Dans le cas
où les informations a priori concernant les données et les a priori
concernant le modèle sont indépendantes, on peut écrire :
\begin{equation}
  \label{eq:joint-prior-density}
  \rho(p,d) = \rho_P(p) \rho_D(d) \;.
\end{equation}

Le point de vue développé dans \getref{Tarantola: Tarantola and
  Valette (1982a)} est que l'information \emph{a posteriori} est donné
par la conjonction des informations a priori $\rho(p,d)$ et
$\Theta(p,d)$:
\begin{equation}
  \sigma(p,d) = c \; \frac{\rho(p,d) \Theta(p,d)}{\mu(p,d)} \;,
\end{equation}
où $\mu(p,d)$ représente la densité homogène définie sur $\Espace
= \ParamSpace \times \DataSpace$, et où $c$ est une facteur de
normalisation. L'on peut alors définir les densités de probabilités
marginale concernant les informations a posteriori dans $\ParamSpace$:
\begin{equation}
  \sigma_P(p) = \int_\DataSpace \; \dd d \; \sigma ( p,d ) \;, 
\end{equation}
et les informations a posteriori dans $\DataSpace$:
\begin{equation}
  \sigma_D(d) = \int_\ParamSpace \; \dd p \; \sigma ( p,d) \;.
\end{equation}

Si les hypothèses du prarapraphe précédents sont vérifiées, c'est à
dire que l'égalité donnée par \refeq{eq:correlation-data-model} est
vérifée, et si l'on considère ques les informations a priori
concernant les données et les paramètres sont indépendentes (l'égalité
donnée par \refeq{eq:joint-prior-density} est vérifiée), alors l'on
peut écrire l'information a posteriori dans l'espace $\ParamSpace$
comme :
\begin{equation}
  \label{eq:inverse-problem-solution}
  \sigma_P(p) = c \rho_P(p) \; \int_\DataSpace \; \dd d\;
  \frac{\rho_D(d) \theta(d|p)}{\mu_D(d)} \;.
\end{equation}
Le membre de droite de cette dernière equation ne dépend que des
paramètres, et peut être écrit :
\begin{equation}
  L(p) = \int_\DataSpace \; \dd d\;
  \frac{\rho_D(d) \theta(d|p)}{\mu_D(d)} \;,
\end{equation}
donnant une mesure de la manière qu'ont les paramètres d'expliquer les
données, en d'autres terme, il s'agit d'une fonction de
\emph{likelihood}.

L'\refeq{eq:inverse-problem-solution} est la solution générale d'un
problème inverse à partir de laquelle on peut accéder à toutes les
informations dont on pourrait avoir besoin concernant les paramètres
du modèles, comme des valeurs moyennes, des barres d'erreurs, etc.
L'on peut également avoir accès à la probabilité ques les paramètres
satisfassent certaines conditions en intégrant $\sigma_P(p)$ sur le
sous-espace de $\ParamSpace$ de l'ensemble des paramètres qui
satisfont cette condition.

Lorsque le point de vue développé préccédement est adopté, l'existence
de la solution veut dire que $\sigma_P(p)$ n'est pas nulle. Si elle
l'était, cela pourrait vouloir dire que les résultats expérimentaux,
les a priori, et les informations théoriques sont incompatibles. Si
l'on considère la densité de probabilité en tant que solution au
problème, alors la solution, construite comme la conjonction de deux
densités de probabilité, est unique. 

Une fois que la densité de probabilité a posteriori $\sigma_P(p)$ est
connnue, il faut l'étudier pour en tirer le maximum
d'informations. Lorsque les espaces $\DataSpace$ et $\ParamSpace$ sont
des espaces linéaires, et que $\sigma_P(p)$ est une gaussienne, les
incertitudes sur les paramètres peuvent être obtenues en comparant les
matrices de covariances pour la densité a posteriori ($\sigma_P$) et
celle pour la densité a priori $\rho_P$. Cependant, lorsque la densité
n'est pas une gaussienne, les moments perdent de leur
sens\footnote{Par exemple, la valeur moyenne obtenue à partir d'une
  densité de probabilité de type double gaussienne est un point pour
  lequel la probabilité peut être faible.}. Dans ce cas, il faut opter
pour d'autres méthodes. 

Bien qu'il existe un grand nombre de méthodes possibles pour la
résolution des problèmes inverses, elles ont toutes pour objectif de
trouver la paramètrisation optimale, permettant d'expliquer au mieux
les données, c'est à dire, de trouver le $p$ telle que la densité
$\sigma_P(p)$ soit maximum. 

Pour ce faire, il existe plusieurs classes de méthodes. L'on peut
choisir d'explorer la totalité de l'espace $\ParamSpace$ afin d'avoir
la densité $\sigma_P$ en tout points, permettant d'obtenir le maximum
global. Cette méthode est la plus simple à mettre en oeuvre
numériquement, et est relativement efficace pour des problèmes de
dimension peu élevée et à condition que le calcul de $\sigma_P$ ne
soit pas trop lourd. En général, ce n'est pas le cas : soit le nombre
de paramètres est grand, soit le calcul de la densité est long ; la
plupart du temps les deux à la fois. Dans ce cas, il existe deux
autres possibilités, formant deux autres classes de méthodes. La
possibilité la plus proche de la méthode précédente, consiste toujours
à explorer l'espace $\ParamSpace$, mais cette fois-ci en utilisant un
processus pseudo-aléatoire permettant de n'explorer que certaines
zones de l'espace $\ParamSpace$. Ces méthodes sont dénommées
Monte-Carlo. Pour plus de détails quant à leur utilisation pour la
résolution des problèmes inverses, l'on peut se référer à
\getref{Tarantola : page 42 ;
  http://www.cwp.mines.edu/Meetings/Tutorial08/ip23r1.pdf et
  ref. therein ;
  %http://www.ipgp.fr/~tarantola/Files/Professional/Papers_PDF/MonteCarlo_latex.pdf
}. Ces méthodes nécessitent tout de même le calcul de $\sigma_P$ un
grand nombre de fois, et peuvent donc être inappropriées lorsque
$\sigma_P$ est trop compliquée. Dans ce cas, l'utilisation de la
dernière classe de méthode peut être utile. Ces méthodes consistent à
définir une fonctionnelle de la distance dans $\Espace$ entre les
données et les prédictions, et à la minimiser. Le résultat va
évidement dépendre de la manière dont la distance a été définie. Les
méthodes traditionnelement utilisées définissent la distance à partir
de la norme $l_p$, pour $p \in [1, \infty]$\footnote{Remarque : Les
  espaces $L_p$ sont les espaces \emph{vectoriel} des fonctions
  exposant $p$ sont sommables.}. En physique, le choix $p=2$ est
souvent fait, et dans ce cas la minimisation se fait au sens des
moindres carrés. \getref{moindres carrés : méthodes, en physique
  (nucléaire) }. Lorsque le choix $p=1$ est fait, la minimisation se
fait au sens des moindres valeurs absolues. Le choix $p=\infty$,
correspondant aux fonctions bornées, peut se faire lorsque les
incertitudes sur les données, et les incertitudes a priori sont
bornées. Il est possible de montrer que dans ce cas la densité
$\sigma_P$ est constante (n'a donc pas de maximum). Dans ce cas, il
est possible de définir une fonction associant aux paramètres le
maximum de la distance, au sens de la norme $l_1$, entre les données
et le modèle. Les paramètres optimaux sont ceux pour lesquels cette
fonction est minimum (méthode minimax)\footnote{Je n'ai pas trouvé
  d'exemple en physique nucléaire où se critère est utilisé.}. 

En général, en physique, lorsque l'on cherche à résoudre le problème
inverse, l'on fait sans le dire l'hypothèse que les espaces considérés
sont des espaces vectoriels, et l'on cherche la solution comme celle
minimisant la distance au sens des moindres carrés. Cela n'est pas
forcément étonnant, car, implicitement, les espaces considérés sont de
Hilbert (c'est à dire des espaces de fonctions de norme $l_2$). Ce
choix peut paraitre étonnant, puisque la physique impose a priori des
contraintes sur les paramètres utilisés : les paramètres sont donc des
fonctions bornées (vivant dans $L_\infty$). 

D'une manière générale, la norme $l_2$ a tendance a donner plus de
poids aux points abérrants, et l'utilisation du critère de moindre
carré dans le cas où les distributions ne sont pas exactement
gaussienne peut être dangereuse. Au contraire, le critère de moindre
valeur absolue est plus robuste face à ce genre de problème
\getref{Tarantola : Claerbout and Muir (1973)}.

Une dernière remarque peut être faite. Dans la définition de Hadamard
d'un problème bien posé, l'unicité de la solution est un critère. Dans
le point de vue adopté ici, la solution au problème inverse est la
densité de probabilité $\sigma_P$ dans son ensemble. De ce point de
vue, la solution est unique. Cependant, comme dit juste au dessus,
nous cherchons les paramètres pour lesquels le modèle reproduit au
mieux les données, c'est à dire la maximum de $\sigma_P$. Si l'on
considère comme solution au problème inverse une paramétrisation
donnée obtenue grâce à une des méthodes précédentes, il n'est pas
possible d'assurer l'unicité : $\sigma_P$ peut avoir plusieurs maxima. \\

Un des objectifs du travail de thèse a été d'appliquer certaines de
ces méthodes dans le cadre des modèles de type NJL. Il s'est avéré
cependant que, dans le cas du modèle NJL le plus simple, l'utilisation
de ces méthodes ne fut pas nécessaire, car le problème inverse peut se
résoudre, et ce fut une grande surprise, analytiquement. J'ai tout de
même pris le parti d'exposer certains aspects de la résolution des
problèmes inverses dans le but, entre autres, que cela puisse servir à
mon successeur.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Sensibilté de la prédiction du CEP dans un modèle NJL simple}
\label{sec : sensibilite de la prediction du CEP dans un modele NJL simple}

Cette partie est une reproduction de l'article \getref{notre
  article}. Afin de la rendre auto-suffisante, certains thèmes déjà
abordés le sont à nouveau.

La mesure de la position du point critique chiral (CEP) dans le
diagramme de phase de QCD fait l'objet d'un début intense. Bien qu'il
soit possible de prédire sa position en utilisant des modèles
effectifs, construits pour reproduire certains aspects de QCD, la
qualité des prédictions obtenues dans de tels modèles dépend du
conditionnement du problème inverse, directement lié à son caractère
bien ou mal posé. Lorsque le problème est mal conditionné, des
variations infinitésimales des données d'entrées, \emph{inputs}, peut
entraîner de larges variations pour les prédictions. Le problème
devient plus grave lorsque les inputs proviennent de données
expérimentales par exemple, où les incertitudes sont finies.

Dans la suite, nous appliquons ce raisonnement sur les prédictions
d'un modèle NJL particulier, dans l'approximation de champ-moyen +
boucle, en prêtant une attention particulière à la prédiction de la
position du CEP dans le plan $(T,\mu)$. 

Nous verrons que des variations infinitésimales des inputs engendre de
grandes variations de la coordonnée de température du CEP, alors que
les variations concernant sa coordonnées de potentiel chimique sont
plus faibles. La conséquence principale, est que des variations des
inputs de l'ordre de $0.6$\% suffisent à faire disparaitre le CEP du
diagramme de phase.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Introduction}

Le point critique chiral (CEP) a d'abord été proposé à la fin des
années 80 \getref{paper : 1}, et est depuis un sujet de discussion
important : à température et potentiel chimique le diagramme le plus
commun est composé d'une transition chirale de premier ordre, séparant
le monde hadronique de la phase quark. Cette ligne de premier ordre
s'achève au CEP, où la transition est de second ordre, et, à mesure
que la température augmente et que le potentiel chimique diminue, la
transition devient de type \emph{crossover}.

L'existence du CEP est toujours une question ouverte du point de vue
théorique, et sa recherche expérimentale est en cours \getref{paper :
  2-10}.

À cause de son importance pour le diagramme de phase de QCD, le CEP
fait l'objet de recherche importante dans différents programmes
expérimentaux de collision d'ions lourds : sa localisation, ainsi que
la transition de déconfinement \getref{paper: 7-9} sont étudiées au
SPS au CERN \getref{paper : 10}, au BNL à RHIC \getref{paper :
  2-4}. Elles seront également étudiées dans les prochaines
expériences FAIR au GSI et NICA au JIRN \getref{11}. L'éventuelle
preuve de l'existence du CEP serait l'une des premières observations
dans le milieu d'une observable de QCD, et serait une contrainte forte
pour de nombreux modèles effectifs. 

Du point de vue théorique, il n'existe pas de consensus concernant
l'existence du CEP, même si d'anciens résultats de QCD sur réseau
(Lattice-QCD ou LQCD) prédisent son existence \getref{paper : 12} :
une fois que la transition à potentiel chimique nul est un crossover
\getref{paper : 13,14}, il est tout à fait possible que la transition
reste de ce type lorsque le potentiel chimique augmente. 

La plupart des modèles de type NJL, avec ou sans couplage à la boucle
de Polyakov, \getref{paper : 15-23}, ainsi que les modèles de type
quarks-meson \getref{paper :24,25}, présentent une transition chirale
de premier ordre se terminant au CEP. Même si ces modèles prédisent
l'existence du CEP, chacun d'entre eux le localise à une position
différente dans le diagramme de phase. Cette position peut dépendre de
la paramétrisation choisie \getpaperref{22}, de la valeur des
constantes de couplage dans le canal vecteur, de l'anomalie de 't
Hooft \getpaperref{26}, ou même du couplage à la boucle de
Polyakov. Face à ces résultats plusieurs tentatives de contraintes ont
été essayées, pour comprendre si le CEP existe ou non
\getpaperref{27-29}. Dans \getpaperref{27}, les auteurs ont fixé la
constante de couplage vecteur de telle manière à ce que la pente de la
ligne critique obtenue sur réseau dans \getpaperref{30} à faible
potentiel chimique soit reproduite. Concernant l'existence du CEP, une
autre contrainte pourrait provenir de l'étude des étoiles compactes :
s'il était possible de montrer l'existence d'une transition de premier
ordre à l'intérieur de ces objets stellaires, cela impliquerait
l'existence d'une transition de premier ordre dans le diagramme de
phase de QCD, et donc impliquerait l'existence d'un CEP \getpaperref{31}.

Lorsque l'étude du CEP se fait grâce aux modèles effectifs, l'approche
usuelle pour comprendre les mécanismes générant un CEP est de faire
varier certains paramètres pour voir si la physique controllée par ces
paramètres est pertinent pour la prédiction de la position du CEP
(voir \getpaperref{22} par exemple). Ces études permettent de
comprendre ces mécanismes qualitivement. Le principal inconvénient de
ce type de méthode est que lorsque les paramètres sont variés dans un
domaine donné, il se peut très bien que les prédictions du modèle
utilisées pour fixer les paramètres ne soient plus du tout en accord
avec les données expérimentales correspondantes.

Ici, nous proposons une nouvelle méthode permettant d'étudier la
sensibilité de certaine prédiction d'un modèle NJL, en se basant sur
les méthodes de résolution du problème inverse. Ces méthodes sont déjà
utilisées en physique nucléaire, où elles ont prouvé leur utilité
\getpaperref{34-37}. Dans un cadre différent de celui que nous
considérons ici, les auteurs de \getpaperref{37}, ont montré que le
fait de varier les paramètres du modèle indépendement peut être
trompeur si le problème inverse correspondant est mal posé. Dans
\getpaperref{36}, l'utilité d'une analyse systématique de l'espace des
paramètres est discutée. Ces deux papiers mettent en exergue que, bien
que la valeur de la fonction de mérite utilisée (un $\chi^2$ en
l'occurence) en son minimum donne une mesure de la manière dont le
modèle est capable de reproduire les données, il existe d'autres
informations intéressantes pouvant êter étudiées. En particulier, la
stabilité des prédictions peut être relié à la courbure la fonction de
mérite en son minimum. 

Dans le cadre des modèles effectifs, une fois que la procédure
d'ajustements des paramètres a été faite, une question centrale est
d'évaluer si les autres prédictions pouvant être faites avec ce modèle
gardent leur sens. En d'autres termes, est-il raisonnable d'utiliser
un modèle effectif pour faire des prédictions qui n'ont pas été
utilisées dans la procédure d'ajustement : le modèle garde-t-il son
pouvoir prédictif, et jusqu'où peut-il être utilisé ? Par exemple, si
les données utilisées sont obtenues dans le vide, les prédictions
gardent-elles un sens lorsque faites dans le milieu ?

Pour essayer de répondre à ces questions, nous introduisons un
paramètre de sensibilité permettant d'évaluer si une prédiction est
sensible à la valeurs des inputs utilisés. Si une prédiction est très
sensible, alors il est possible que sa valeur prédite ne soit pas
bonne. Ce type d'analyse de sensibilité est assez commun en physique
nucléaire \getpaperref{34-38}. Bien que la définition du paramètre de
sensibilité varie d'un travail à l'autre, il permet toujours d'estimer
la stabilité des prédictions d'un modèle. Les modèles de physique
nucléaire utilisés dans les papiers précédement cités dépendent de
nombreux paramètres et l'inversion est faite en utilisant un
$\chi^2$. Dans le cas du modèle NJL que nous utiliserons par la suite,
il se trouve que l'inversion peut se faire analytiquement, nous
permettant de définir un paramètre de sensibilité mesurant l'impact
sur les prédictions de variations infinitésimales des inputs.

Il se trouve que le paramètre de sensibilité que nous définirons dans
la suite est relié de très près à un critère utilisé en informatique
théorique permettant de determiner si le résultat d'un calcul
numérique va être endommagé par la propagation des erreurs
d'arrondi\footnote{En informatique, l'erreur d'arrondi,
  \emph{round-off error}, est la différence entre un nombre réel et sa
  valeur approximée par l'ordinateur à une précision donnée. En
  l'occurence, lorsque des calculs numériques, sujets à des erreurs
  d'arrondi, sont fait les uns à la suite des autres, ces erreurs
  peuvent s'accumuler jusqu'à dominer le résultat lorsque le problème
  est mal conditionné. En analyse numérique, le critère utilisé
  rendant compte du conditionnement d'un problème est relié au
  \emph{condition number} qui mesure la manière dont un nombre calculé
  numériquement depend de la valeur des arguments d'entrée. Quand ce
  ``nombre-condition'' est grand, le problème est dit mal-conditionné,
  lorsqu'il est petit, il est dit bien conditionné. Notre paramètre de
  sensibilité est défini de cette manière. Numériquement, une
  possibilité d'accéder à ce nombre-condition, est d'utiliser les
  techniques de différentiation automatique. Pour plus de détails
  concernant la différentiation automatique et son utilisation pour le
  calcul du condition number, ou pour la résolution de systèmes
  (non-)linéaires, ou de problèmes d'optimisation, l'on peut se
  référer, par exemple, aux livres \getref{ 10.1007/978-3-642-78423-1
    ; 10.1007/3-540-28438-9 ; 10.1007/978-1-4613-0075-5}. Très
  grossièrement, la différentiation automatique utilise le fait que
  tout calcul numérique se fait par la composition de fonctions de
  base comme l'addition, la multiplication, etc. La procédure est donc
  d'utiliser les formules de dérivations de fonctions composées pour
  obtenir la dérivée du résultat d'un algorithme par rapport à ses
  arguments d'entrée. Par curiosité, le lecteur peut utiliser
  l'utilitaire de l'INRIA, TAPENADE,
  \url{http://www-sop.inria.fr/tropics/tapenade.html}. Il suffit de
  fournir un algorithme écrit en C ou Fortran, pour obtenir un
  algorithme ``différentié''. L'avantage énorme de la différentiation
  automatique est que la dérivée est calculée à la précision de la
  machine : les problèmes liés aux calculs de dérivées numériques avec
  les différences finies disparaissent. De plus, les algorithmes crées
  sont beaucoup plus rapides que ceux que nous pourrions écrire en
  implémentant la dérivée analytique d'une fonction. Le problème
  principal arrive cependant lorsque notre algorithme dépend de
  routines dont on ne connait pas le code source.}.

Le raisonnement utilisé lorsque l'on considère les problèmes inverses,
et les méthodes qui leurs sont associées peuvent être assez
compliquées. Aussi, comme à notre connaissance, ils n'ont jamais été
utilisé pour l'étude des phases de QCD, nous présentons ici une étude
simple, basée sur l'étude du paramètre de sensibilité et sur l'analyse
des correlations. 

Nous choissisons le modèle NJL à deux saveurs dans l'approximation
d'isospion exact, en considérant les intéractions dans le canal
scalaire uniquement, dans l'approximation de champ moyen + boucle. Les
inputs que nous choisissons sont le condensat de quark, $\qbarq$, la
masse du pion dans le vide $m_\pi$, et la constante de désintégration
du pion dans le vide $f_\pi$. Ce modèle a l'avantage de bien
reproduire les propriétés chirales de base de QCD, tout en restant
suffisement simple : la résolution du problème inverse associé peut se
faire analytiquement. La simplicité du modèle choisi permet de se
concentrer sur l'analyse de sensibilité, et de vérifier la validité de
nos calculs, tout en pouvant obtenir des informations concernant le
CEP.

Le reste de cette section est organisée comme suit. Dans la première
partie, nous rappelerons les bases du modèle NJL nécessaire pour la
suite. Le paramètre de sensibilité sera ensuite défini, et nous le
calculerons dans le cadre du modèle NJL pour plusieurs prédictions
dans le vide : la masse du méson sigma et la constante de couplage
effective pion-quark-antiquark, et dans le milieu : la position du
point critique chiral dans le plan $(T,\mu)$. Nous présenterons
également une analyse de la situation lorsque la contrainte sur le
modèle liée à la valeur du condensat de quark est relaxée. 

Dans la deuxième partie, des variations finies sur les inputs $m_\pi$,
$f_\pi$ et $\qbarq$ sont considérées et discutées dans le cadre des
résultats obtenues dans la première partie de l'étude. 

Pour ne pas surcharger l'exposé des résultats, les détails des calculs
sont donnés après la conclusion.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Sensibilité des prédictions du modèle NJL}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Paramétrisation du modèle NJL : observables}

Nous considérons le modèle NJL dans sa version locale, à deux saveurs,
dans l'approximation d'isospin $SU(2)$ exact, dont la densité
lagrangienne est donné par \getpaperref{41-44} :

\begin{equation}
  \label{eq:NJL-Lagrangian}
  %
  \mathcal{L}_{NJL} = \bar{\psi} ( i\gamma^\mu \partial_\mu - m_0 ) \psi 
  + G \left[ (\bar{\psi}\psi)^2  
    + ( \bar\psi i \gamma_5 \boldsymbol\tau \psi )^2 \right] \;. 
  %
\end{equation}

Ce lagrangien dépend de trois paramètres : $m_0$ est la masse des
quarks $u$ et $d$ dans l'approximation d'isopsin $SU(2)$ en GeV ; $G$,
la constante de couplage en GeV$^{-2}$, et $\Lambda$, le cutoff 3D ,
en GeV, imitant la liberté asymptotique des quarks. Comme le modèle
NJL ne peut pas être construit directement à partir de QCD, ces
paramètres sont des paramètres libres. Cependant, il existe des
contraintes molles sur ces paramètres : $m_0$ doit être de l'ordre de
la masse des quarks $u$ et $d$. $\Lambda$ est relié à l'échelle
$\Lambda_{\textrm{QCD}}$. La constante de couplage $G$ est très peu
contrainte, mais si on la considère de la même manière que la
constante de couplage pour la théorie de Fermi,$G \simeq g/M^2 \simeq
\tilde g / \Lambda^2$, alors $G\Lambda^2$ doit etre dans l'intervalle $[1,10]$.

Usuellement, ces paramètres sont ajustés sur les valeurs de la masse
du pion, $m_pi$, de la constante de désintégration du pion $f_\pi$, et
du condensate de quark normalisé pour être positif et ayant la
dimension d'une énergie : $c = -\qbarq^{1/3}$. Le condensat de quark
est relié à une autre quantité, que l'on appelle la masse dynamique
des quarks $m$, ne pouvant pas être considérée comme une observable,
mais donnant une image de la physique du monde hadronique, où la
symmétrie chirale est spontanément brisée, en terme de
quasi-particules de masse $m \gg m_0$. Elle donne aussi une
approximation grossière de la masse du nucléon comme trois fois la
masse d'un quasi-quark de masse $m$.

Nous choisissons l'approximation de champ moyen pour le calcul du
condensat, et l'approximation des boucles pour le calcul des quantités
mésoniques \getpaperref{41}.

La masse effective au champ moyen, $m(\paperparam)$, est déterminée par la
résolution de l'équation du gap :
%
\begin{equation}
  \label{eq:gap-equation}
  m_0 - m + 8iGN_cN_f m I_1 = 0 \;
\end{equation}
% 
où $I_1$ est l'intégrale du propagateur à une ligne fermionique
apparaissant dans les diagrammes \emph{tadpole} :
\begin{equation}
  \label{eq:I1}
  I_1 = -i \int^\Lambda \frac{\dd^3 p }{(2\pi)^3} \frac{1}{2E_p} \mbox{
    (with $E_p^2 = p^2 + m^2$)}.
\end{equation}
La forme du condensat de quark au champ moyen \mbox{$\qbarq (\paperparam)$} est : 
\begin{equation}
  \label{eq:cond}
  \qbarq  =  \frac{m_0 - m}{2G}\;.
\end{equation}
% 
Enfin, la masse du pion $m_\pi(\paperparam)$ et la constante de
désintégration du pion $f_\pi(\paperparam)$ dans le vide sont donnés
par :
%
\begin{align}
  \label{eq:mpi}
  m_\pi^2   & =   - \frac{m_0}{m} \frac{1}{4 i G N_c N_f I_2(0)} \;, \\
  \label{eq:fpi}
  f_\pi^2   & =   -4i N_c m^2 I_2(0)  \;,
\end{align}
% 
où $I_2$ est l'intégrale à deux lignes fermionique provenant des
diagrammes de boucle de quark dans l'approximation des boucles. En
plus de cette approximation, nous faisons l'approximation de
quasi-boson de Goldstone : la masse du pion est négligée dans le
calcul de l'intégrale : $k^2 = 0$ (et non pas $k^2 =
m_\pi^2$). Explicitement :
%
\begin{equation}
  \label{eq:I2}
  I_2(0) = -i \int^\Lambda \frac{\dd^3 p }{(2\pi)^3} \frac{1}{4E_p^3}.
\end{equation}
\medskip
Quand $(m_\pi,\ f_\pi, \qbarq)$ sont fixés à leur valeur
phénoménologique, le problème inverse est résolue quand le système :
\begin{align}
  \label{eq:inverse-problem-sys-njl-1}
  m_\pi    (\paperparam) & =  m_\pi  \;, \\
  \label{eq:inverse-problem-sys-njl-2}
  f_\pi    (\paperparam) & =  f_\pi  \;,   \\
  \label{eq:inverse-problem-sys-njl-3}
  \qbarq (\paperparam) & =  \qbarq \;, 
\end{align}
% 
est résolu pour les paramètres $\Lambda$, $m_0$ et $G$. Grâce à
l'approximation de quasi-boson de Goldstone, ce sytème a des solutions
semi-analytique, voir \refapp{app:inverse-pb}. Comme nous le montrons
dans cette partie, le système précédent a des solutions que si le
ratio $\alpha = f_\pi^3 / \qbarq$ est plus grand qu'une valeur
critique constante (ceci est relié à la discussion de la partie 2.2.2
de \getpaperref{43}). Parmi les solutions, une seule est physique. 

Quand les paramètres sont fixés, le modèle NJL considéré peut décrire
de la phénoménologie simple dans le vide, comme la masse du méson
sigma $m_\sigma$ et la constante de couplage effective $g_{\pi \bar q
  q}$ (voir \getpaperref{41} pour plus de détails) : 
\begin{align}
  m_\sigma &= \sqrt{4 m^2 + m_\pi^2} \;, \nonumber \\
  g_{\pi \bar q q} &= \frac{1}{\sqrt{-4i N_c I_2(0)}} \;.
  \label{eq:vac-predictions}
\end{align}

Ce modèle NJL est aussi capable de prédire, pour certaines valeurs de
ses paramètres, une transition de premier ordre vers une phase où la
symmétrie chirale est partiellement restaurée. Cette ligne de
transition s'achève au point critique chiral, dont les propriétés sont
décrites en \refapp{app:cep-calc}, dans le plan $(T,\mu)$. Dans cette
partie, nous décrivons également un algorithme rapide et stable pour
le calculer.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Sensibilité et problème mal-conditionné}

Pour étudier la sensibilité d'une prédiction donnée, nous utiliserons
un nombre-condition \getpaperref{40} du problème servant de mesure de
la sensibilité d'une solution à ce problème par rapport à des
variations infinitésimale de ses données d'entrée. Ici, nous
utiliserons un nombre-condition relatif : s'il est infini, le problème
sera dit mal posé ; s'il est fini mais grand, le problème sera dit mal
conditionné. 

Le choix que nous avons fait pour le calculer est basé sur la
propagation statistique des erreurs : c'est le choix naturel pour
calculer des variations (déviation standard) d'une quantité calculée
par rapport à des variation des données utilisées (considérées non
corrélées) pour la calculer.

Soit $X$ un prédiction dépendantes de deux inputs $a$ et $b$. La
déviation standard de la prédiction $X$ est calulée en propageant les
variations $\sigma(a)$ et $\sigma(b)$ des inputs :
\begin{equation}
  \sigma^2(X) = \left(\frac{\partial X}{\partial a}\right)^2 \sigma^2(a)
  + \left(\frac{\partial X}{\partial b}\right)^2 \sigma^2(b) \;.
\end{equation}
La sensibilité est le ratio de la déviation standard relative et de la
moyenne des variations relatives des inputs :
\begin{equation}
  \Sigma(X) = \lim_{\sigma \rightarrow 0} \frac{\sigma_{rel}(X)} {\sigma_{rel}^{I}}
\end{equation}
où,
\begin{align}
  \sigma_{rel}(X) &= \frac{\sigma(X)} X \\
  \sigma_{rel}^{I} &= \frac 1 2 \left( \frac{\sigma(a)} a + \frac{\sigma(b)} b \right)\,,
\end{align}
et où $\lim_{\sigma \rightarrow 0}$ représente le fait que nous
considérons des variations infinitésimales. La manière dont cette
limite est approchée doit être spécifiée. Nous choisissons de prendre
des dispersions relatives des inputs infinitésimales. C'est à dire,
pour $I = a$ ou $b$, $\sigma(I) / I = p$ et $p \rightarrow 0$. Afin de
vérifier nos résultats, nous discuterons briévement d'une autre
manière d'approcher cette limite : en prenant des dispersions absolues
infinitésimales. Dans le cas où tous les inputs ont la même dimension,
cela revient à considérer : $\sigma(I) = d$ et $d \rightarrow 0$.

Explicitement et dans le cas de la dispersion relative, pour les inputs $(m_\pi,
f_\pi, c)$, la sensibilité d'une prédiction $X$ s'écrit : 
\begin{equation}
  \label{eq:sensitivity}
  \Sigma(X) = \sqrt{
    \left(\frac{\partial X}{\partial m_\pi}\right)^2 \frac{m_\pi ^2}{X^2}
    + \left(\frac{\partial X}{\partial f_\pi}\right)^2 \frac{f_\pi ^2}{X^2}
    + \left(\frac{\partial X}{\partial c}\right)^2 \frac{c^2}{X^2}
  } 
  \;,
\end{equation}
où nous pouvons remarquer que : 
\begin{equation}
  \left( \frac{\partial X}{\partial a} \frac{a}{X}\right)^2 = \left(
  \frac{\partial \ln X}{\partial \ln a} \right)^2 \;.
\end{equation}

Dans la suite, nous soutiendrons l'hypothèse que si la sensibilité
d'une prédiction est grande (lorsque le calcul est mal conditionné),
alors l'on ne peut (doit) pas donner de crédit à cette prédiction, car
une erreur infinitésimale sur les données, se traduit par une
modification importante de la valeur de cette prédiction : la
prédiction n'est pas stable. Au contraire, une prédiction dont la
sensibilité est faible est considérée stable, et sa valeur a un
sens\footnote{Une remarque ici est nécessaire. Nous ne disons pas
  qu'une prédiction, faite grâce à un modèle donné, dont la
  sensibilité serait grande est fausse, et nous ne disons pas qu'une
  autre prédiction dont la sensibilité serait faible est vraie. Ce que
  nous disons en revanche, est que si l'on souhaite comparer les
  prédictions du modèle avec des mesures expérimentales (faites ou à
  venir), il vaut mieux comparer celles qui ont une sensibilité
  faible. Si un modèle donne un prédiction stable d'une quantité, et
  qu'il s'avère que cette quantité n'est pas celle mesurée, alors l'on
  peut dire que le modèle ne marche pas bien. On ne peut pas en faire
  autant pour une prédiction instable. De la même manière, s'il
  s'avère qu'une prédiction instable est proche de celle mesurée,
  alors cela est plus dû à la chance qu'a la compréhension profonde de
  la physique sous-jacente.}.

%%%%%%%%%%%%%%%
\paragraph{Remarque sur le calcul de sensibilité : \\ }

Il est en principe possible de calculer analytiquement la sensibilité
d'une prédiction. Nous le ferons pour les prédictions dans le vide, où
cela peut être fait assez aisément car le problème inverse est
exacte. Les détails concernant le calcul de la sensibilité de la masse
du sigma, $\Sigma(m_\sigma)$ peut être trouvé en
\refapp{app:msigma-sensitivity}. Il serait toujours possible de
calculer $\Sigma(\Tcep)$ et $\Sigma(\mucep)$, bien que beaucoup plus
difficile car cela nécessiterait le calcul des dérivées d'une système
de trois fonctions implicites (voir \refapp{app:cep-calc}). Le calcul
de ces dérivées devient encore plus difficile dans le cadre de modèle
plus réalistes, prenant en compte des intéractions de la canal
vecteur, ou prenant en compte le couplage à la boucle de Polyakov, en
particulier car la dimension du système d'équations augmente. Bien que
nous ayons choisi le présent modèle pour sa simplicité, ce travail
sert aussi de point de départ pour des études futures. Pour cette
raison, nous avons choisi d'utiliser une méthode type Monte-Carlo pour
le calcul des sensibilités. De plus, comme nous utilisons la formule
de propagation statistique des erreurs, le cadre Monte-Carlo est celui
le plus adapté pour calculer moyennes et déviations standards. Enfin,
le dernier avantage d'un calcul Monte-Carlo, est qu'il permet de
visualiser les données facilement dans le cas de dispersions finies
des inputs. Plus de détails seront donnés dans la partie correspondante. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Reultats : sensibilité de certaines prédiction du modèle NJL}

Venons en au premier résultat de ce travail. Nous choisissons comme
valeur des inputs :
\begin{equation} 
  \label{eq:mean-input-values}
  \begin{pmatrix}
    m_\pi \\
    f_\pi\\
    \qbarq^{1/3}
  \end{pmatrix}
  = 
  \begin{pmatrix}
    137 & \textrm{MeV} \\
    93 & \textrm{MeV} \\
    -315 & \textrm{MeV}
  \end{pmatrix}
  \;.
\end{equation}
La valeur choisie pour le condensat de quark est équivalente à la
valeur \mbox{$\ubaru^{1/3} \simeq -250$ MeV} pour le condensat de
quark $u$. Cette valeur pour le condensat de quark $u$ est en bon
accord avec les limites obtenues avec les règles de somme, \mbox{$190
  \;\mathrm{MeV} \le - \langle \bar{u}u\rangle^{1/3} \le 260
  \;\mathrm{MeV}$} obtenus à l'echelle de renormalisation $1$ GeV
\getpaperref{45}, et celles obtenues à une échelle de renormalisation
de $2$ GeV, \mbox{$\langle \bar{u}u\rangle^{1/3} = -270
  \;\mathrm{MeV}$} \getpaperref{46}. Cette valeur est également en
accord avec celle obtenue sur réseau \mbox{$\langle
  \bar{u}u\rangle^{1/3} = -269(08)$ MeV} \getpaperref{47}.


%%%%%%%%%%%%%%%
\paragraph{Paramétrisation }

Dans le contexte considéré, les vrais inputs du modèle sont les
observables phénoménologiques $m_\pi$, $f_\pi$ et $\qbarq$ à partir
desquels les paramètres sont définis de manière unique. En ce sens, il
est possible de calculer une sensibilité pour les paramètres. Comme
déjà remarqué dans \getpaperref{36,37}, une variation autour de la
solution du problème inverse permet d'obtenir des informations
concernant la précision du modèle. Si les paramètres ont une grande
sensibilité, cela voudrait dire que dès le départ, le modèle ainsi
défini ne pourrait pas être utilisé pour faire des prédictions : le
modèle lui-même serait mal conditionné. Nous verrons en section
\ref{subsec:qq_var}, que dans le cas où $\alpha \rightarrow \alpha_c$
(où $\alpha_c$ est la valeur limite du ratio $f_\pi^3 / \qbarq$ au
delà de laquelle le problème inverse n'a plus de solution, voir
\refapp{app:inverse-pb}), toutes les sensibilités divergent. En ce
point précis, le problème devient mal posé, et plus rien ne peut être
fait avec le modèle.

Une étude des corrélation est également intéressante : des
corrélations faibles entre les paramètres et les inputs signalerait
que les inputs choisis ne sont pas pertinents pour contraindre les
paramètres \getpaperref{37}. 

Sur la tableau \reftab{tab:sensitivity}, l'on peut voir que les
sensibilités des paramètres sont en dessous de 5, ce qui signifie que
les inputs utilisés sont pertinents.

Il est à noter que la relation GMOR nous permet d'accéder à
$\Sigma(m_0)$ simplement. Au premier ordre en $m_0$, la relation GMOR est donnée par :
\begin{equation}
  m_\pi^2 f_\pi^2 = -m_0 \qbarq \;,
\end{equation}
que nous pouvons différentier :
\begin{equation}
  dm_0/m_0 = 2 dm_\pi/m_\pi + 2 df_\pi/f_\pi - 3 dc/c \;,
\end{equation}
nous permettant d'obtenir :
\begin{equation}
  \Sigma(m_0) = \sqrt{17} = 4.12 \;.
\end{equation}
La forme de la différentielle nous permet de voir que la sensibilité
de $m_0$ provient des trois dérivées logarithmiques des inputs. De
cette décomposition, l'on apprend que $m_0$ devrait être également
sensible face à la variation de n'importe quel input, ce qui n'était
absolument pas évident à partir de
\refeq{eq:app-inverse-problem-sys-njl-1}. En examinant la relation
GMOR, nous aurions pu penser que $m_0$ était surtout dépendente de
$m_\pi$, ce qui n'est pas le cas. L'étude des sensibilités permet
donc, même à un niveau basique, d'obtenir des informations
difficilement accessible en se basant sur des arguments physiques
seulement. Pour le cas de $m_0$, la relation GMOR est suffisante pour
révéler le lien caché entre les observables. Dans le cas de
prédictions pour compliquées, ce genre de lien ne peut pas se trouver
de manière aussi simple, et un calcul de sensibilité est nécessaire.
\begin{table}[!ht]
  \centering
  \begin{tabular}{c|c|l|l|}
    \cline{2-4} \multicolumn{1}{l|}{} &
    \multicolumn{2}{c|}{\begin{tabular}[c]{@{}c@{}}Sensibilités\end{tabular}}
    & Valeurs \\ \hline
    \multicolumn{1}{|c|}{\multirow{3}{*}{Paramètres}} & $\Lambda$ &
    $2.83$ & $0.653$ (GeV) \\ \cline{2-4} \multicolumn{1}{|c|}{} &
    $m_0$ & $4.11$ & $0.0051$ (GeV)\\ \cline{2-4}
    \multicolumn{1}{|c|}{} & $G\Lambda^2$ & $3.32$ & $2.11$\\ \hline
    \multicolumn{1}{|c|}{\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Prédictions
          \\ dans le vide\end{tabular}}} & $m$ & $6.72$ & $0.313$
    (GeV) \\ \cline{2-4} \multicolumn{1}{|c|}{} & $m_\sigma$ & $6.41$
    & $0.642$ (GeV) \\ \cline{2-4} \multicolumn{1}{|c|}{} & $g_{\pi
      \bar q q}$ & $5.97$ & $3.37$\\ \hline
    \multicolumn{1}{|c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Prédiction\\ dans
          le milieu\end{tabular}}} & $\Tcep$ & $71.5$ & $0.0299$ (GeV)
    \\ \cline{2-4} \multicolumn{1}{|c|}{} & $\mucep$ & $1.05$ &
    $0.327$ (GeV)\\ \hline
  \end{tabular}
  \caption{ \label{tab:sensitivity} {\small Sensibilités des
      paramètres, des prédictions dans le vide, et des prédictions
      dans le milieu, en considérant des variations infinitésimales
      des inputs. Les sensibilités des paramètres, des prédictions
      dans le vide, et de $\mucep$ sont proches de $1$. La sensibilité
      de la coordonnée de température du CEP est très grande. Ces
      valeurs ont été calculées avec un Monte-Carlo.
  }}
\end{table}


%%%%%%%%%%%%%%%
\paragraph{Sensibilités des prédictions }


Comme les paramètres du modèles sont ajustés sur des quantités
mesurées dans le vide, l'on s'attend à ce que les sensibilités soient
faibles pour les paramètres, plus élévées pour les prédictions dans le
vide et encore plus élevées pour les prédictions dans le milieu. Le
\reftab{tab:sensitivity} donne les sensibilités triés de haut en bas
des valeurs attendues les plus petites aux valeurs attendues les plus
grandes.

Commençons notre discussion sur l'utilité de l'analyse de sensibilité
en se concentrant sur les prédictions dans le vide de la masse du
méson sigma, et la constante de couplage effective
pion-quark-antiquark (voir \refeq{eq:vac-predictions}).

Comme attendu, les sensibilités des prédictions dans le vide sont plus
grandes que celles des paramètres. Elles restent cependant pas trop
grandes ($<7$). Les conclusions que l'on peut tirer de ce modèle
concernant le secteur mésonique dans le vide sont donc assez sûres. 

Concernant la masse du méson sigma, nous calculons $\Sigma(m_\sigma)$,
dans \refapp{app:msigma-sensitivity}, de manière analytique, vérifions
que la valeur obtenue avec le Monte-Carlo est correcte, et obtenons le
résultat pour la différentielle :
\begin{equation}
  \label{eq:sigma-mass-differential}
  \dd m_\sigma = 0.21 \dd m_\pi + 34 \dd f_\pi - 8.2 \dd c \;.
\end{equation}
Premièrement, nous voyons que la sensibilité provenant de la dérivée
partielle par rapport à la masse du pion est négligeable. Ceci n'est
pas surprenant, le théorème de Goldstone étant approximativement
réalisé dans le modèle NJL à cause de la faible mais non nulle masse
des quarks nue, la masse du sigma étant essentiellement deux fois la
masse habillée des quarks. De manière générale, le pion étant un
quasi-boson de Goldstone, toutes les observables ne dépendant pas de
$m_0$ ne sont pas trop sensibles à sa masse. 

L'analyse de sensibilité peut mettre en exergue des relations plus
difficiles que celles obtenues à partir d'arguments de symétrie. Par
exemple, comme $m_\sigma^2 = 4m^2 + m_\pi^2$ et que $m$ est
principalement dûe à la brisure spontanée de la symétrie chirale dans
le modèle NJL, l'on pourrait penser que la masse du sigma est surtout
sensible à la valeur du condensat de quark. Les coefficients de la
différentielle \refeq{eq:sigma-mass-differential} montrent qu'elle est
aussi sensible à la valeur de $f_\pi$. Ceci est dû aux non-linéarités
couplant l'échelle $\Lambda$, $f_\pi$ et le condensat, et n'aurait pas
pu être prédit par simple observation des équations. Rappelons que
l'objectif ici est de montrer l'utilité de l'analyse de sensibilité
dans le cadre d'un modèle simple : les liens entre observables peuvent
ne pas être apparents lors de l'examination des équations. Dans le cas
de $f_\pi$, l'on peut voir, dans le cadre d'un modèle sigma par
exemple, que la valeur moyenne dans le vide du champ sigma et $f_\pi$
sont fortement corrélés : $v^2 = f_\pi^2 (1 + o(m_\pi^2) )$,
impliquant une forte corrélation entre $m$ et $f_\pi$. L'analyse de
sensibilité peut donc mettre en évidence des relations cachées par la
non-linérité du problème. Dans le cas du CEP, il est difficile de
prédire de telles corrélations, comme nous le verrons après, et
l'analyse de sensibilité que nous proposons trouve toute son utilité.

Examinons maintenant les sensibilités des prédicitons (dans le milieu)
des coordonnées du CEP ($\Tcep$ et $\mucep$). Toutes deux sont
surprenantes. D'une part la sensibilité de la coordonnée de
température du CEP est très grande ($\sim 70$), et celle de la
coordonnée de potentiel chimique est très proche de $1$ (en faisant
une prédiction plus stable que les paramètres du modèle eux-mêmes).

Concernant $\Tcep$, la conclusion est relativement simple. Le modèle
dans son ensemble, c'est à dire constitué des approximations faites et
du choix spécifique des inputs, est mal conditionné, rendant la
prédiction de $\Tcep$ très instable. Aucune conclusion ne peut être
tirée concernant $\Tcep$ dans ce contexte. Ceci sera illustré dans la
section où les résulutats de variations finies sont présentés.

Concernant $\mucep$, la conclusion est complétement différente. Sa
sensibilité étant proche de $1$,l'on peut dire que la physique
implémentée dans le modèle NJL étudié, \idest symétrie chirale et
secteur scalaire, fait partie de la physique contraignant la
coordonnées $\mu$ du CEP. Bien sûr, cela ne veut pas dire que le
$\mucep$ prédit est proche de celui qui sera peut-être observé dans la
nature. Cela veut simplement dire que la physique chirale générée par
le secteur scalaire est pertinent pour cette prédiction. A priori, le
fait d'ajouter d'autres effets physique, comme des intéractions dans
le canal vecteur, va certainement modifier la position de $\mucep$
dans le diagramme de phase (en le shiftant essentiellement
\getpaperref{41}), mais ne va pas changer drastiquement la sensibilité
de $\mucep$, ni résoudre le problème de la grande sensibilité de
$\Tcep$\footnote{Cette conjecture est basée sur la figure 1 de
  \getpaperref{48}, dans laquel on voit que le CEP reste mal
  contraint.}. Résoudre ce problème nécessite très certainement
l'ajout de contraintes dans le milieu.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Sensibilités pour différentes valeurs du condensat de quark}

Comme la valeur du condensat de quark est bien moins connu comparé à
la masse du pion et à la constante de désintégration du pion, nous
traçons les sensibilités de $m$, $\Tcep$ et $\mucep$ en fonction de la
valeur du condensat, pour des valeurs fixées de $m_\pi$ and $f_\pi$
données par \refeq{eq:mean-input-values}.

\begin{figure}[!ht]  
  \begin{center}
    \begin{tabular}{lr}
      \includegraphics[width=0.5\textwidth]{m-dist-sensitivity.pdf}&
      \includegraphics[width=0.5\textwidth]{Tcep-dist-sensitivity.pdf}\\
      \includegraphics[width=0.5\textwidth]{mucep-dist-sensitivity.pdf}& 
    \end{tabular}
  \end{center}
  \caption{\label{fig:sensitivities-fixed-qq} {\small La masse
      constituente des quarks $m$ (haut-gauche), la coordonnée de
      température du CEP (haut droite), et la coordonnée de potentiel
      chimique du CEP (bas), ainsi que leurs sensibilités tracées en
      fonction du condensat de quark : $- \qbarq^{1/3}$. La valeur du
      condensat de quark est limité au domaine où une solution au
      problème inverse existe. Quand le condensat de quark varie dans
      un domaine de $10$ MeV, la masse constituente des quarks varie
      dans un domaine de $85$ MeV, $\Tcep$ dans un domaine de $60$
      MeV, et $\mucep$ dans un domaine de $2.5$ MeV seulement. Ces
      fenêtres sont reliées à la sensibilité de ces prédictions, voir
      \reftab{tab:sensitivity}.}}
\end{figure}

La \reffig{fig:sensitivities-fixed-qq} représente la masse
constituente des quarks, $\Tcep$ et $\mucep$ et leur sensibilité
tracées en fonction de la valeur du condensat de quark dans le domaine
$[306 , 316]$ MeV. À $c = 306$ MeV, $\alpha \simeq \alpha_c$ et le
problème inverse n'a plus de solution. À $c \simeq 316$ MeV, $\Tcep
\simeq 0$ : le CEP disparaît du diagramme de phase. Comme nous l'avons
précédement mentionné, nous observons que les sensibilités divergent à
la limite basse du condensat : ceci est dû qu'en ce point le problème
inverse n'a pas de solution, le problème devient mal posé, et les
paramètres ne peuvent même plus être fixés.

Sur la partie basse de la \reffig{fig:sensitivities-fixed-qq}, l'on
peut voir que $\mucep$ ne varie que dans un domaine de $2.5$ MeV, ce
qui est cohérent avec ce qui est attendu pour une prédiction dont la
sensibilité est très proche de $1$. La prédiction de $\mucep$ est très
stable et de large variation du condensat n'ont qu'un effet faible sur
cette prédiction. Ceci est également confirmé par nos calculs de
corrélation de $\mucep$ avec le condensat, voir \reftab{tab:correl}.

Au contraire, la masse consituente des quarks varie dans une fenêtre
de $85$ MeV, et $\Tcep$ dans un domaine de $60$ MeV, ce qui est
cohérent avec les plus grandes sensibilités correspondantes à ces
prédictions.

Sur ce domaine de variation, $\Delta m / m \simeq \pm 11\%$ et $\Delta
T/T \simeq \pm 100 \%$. Bien sûr, ces valeurs ne correspondent pas
exactement aux sensibilités calculées précédement (les sensibilités
étant des quantités locales, reliées aux gradient dans l'espace des
inputs), mais elles montrent que l'accord est correct entre cette
variation finie et une extrapolation basée sur le premier ordre d'un
développement de Taylor dont les coefficients sont reliés aux
sensibilités. Cela montre que le problème est suffisement linéaire,
tout au moins dans la direction du condensat de quark, autour des
valeurs d'inputs pour que la sensibilité soit une quantité utile même
pour ce cas où le problème non-linéaire, \refeq{eq:mc-sensitivity}. 

Comme le condensat de quark est relié à la brisure spontanée de la
symétrie chirale, \idest à la masse dynamique des quarks, il est donc
relié à la création d'une transition de phase de premier ordre à
température nulle. En partant de cette remarque, nous pourrions
prédire que $\mucep$ et $m$ devraient varier de la même manière
lorsque le condensat est modifié. En comparant le graphique en haut à
gauche de la \reffig{fig:sensitivities-fixed-qq} avec celui du bas,
nous voyons que ce n'est pas le cas : la prédiction de $\mucep$ est
remarquablement stable. Ce résultat montre que les non-linéarité du
problème inverse peuvent affecter les prédictions de manière non
triviale, et l'analyse de sensibilié permet de le révéler. En
l'occurrence, nous verrons que $\mucep$ est très corrélé avec la
constante de désintégration du pion.

En regardant le graphique en haut à gauche de la
\reffig{fig:sensitivities-fixed-qq}, l'on peut remarquer que, à
$f_\pi$ et $m_\pi$ fixées, la masse constituente des quarks diminue
lorsque $c$ augmente. À partir de la relation approchée $m \simeq 2 G
c^3$, l'on aurait pû s'attendre à ce que $m$ augmente avec $c$, et à
ce que $\mucep$ augmente également comme une densité plus élevée
aurait été nécessaire pour détruire le condensat. Ici, les résultats
du calcul montrent que $m$ diminue, alors que $\mucep$ est
non-monotone. Cela montre également l'intérêt de varier les inputs du
modèle et non pas ses paramètres : la phénoménologie peut varier de
manière contre intuitive lorsque l'on se rend compte que les vraies
inputs d'un modèle sont les données. En fait, lorsque $m_\pi$ et
$f_\pi$ sont fixées, $G \equiv G(\qbarq)$, et $m \simeq -2G(\qbarq)
\qbarq$. Le comportement non-linéaire de $G$ rend la conclusion du
premier raisonnement fausse.

Mentionnons finalement que ce genre d'analyse peut servir de guide
pour des expériences. Comme remarqué dans \getpaperref{37}, l'analyse
des corrélations est, la plupart du temps, un moyen systématique de
déterminer quelles sont les observables les plus corrélées avec les
paramètres. Les expériences peuvent alors se concentrer sur les plus
accessibles de ces observables. Dans notre cas, nous pouvons faire
l'expérience de pensée suivante pour illustrer ce propos. Supposons
que le canal scalaire soit le seul pertinent et qu'il a été prouvé
qu'un CEP existe. Supposons également que les propriétés du pion
soient bien connues mais que le condensat de quark n'ait pas été
mésuré. Le résultat de cette partie montre que le condensat devrait
étre chercher dans la fenêtre $[306 , 316]$ MeV même si nous ne
pouvons pas dire où le CEP se trouve dans le diagramme de phase à
cause de la grande sensibilité de $\Tcep$.

%%%%%%%%%%%%%%%
\paragraph{Sur le choix de la disperion}

Nous avons choisi de prendre une dispersion relative égale pour les
inputs. Bien sûr, nous aurions pû choisir une dispersion absolue
égale, et cela aurait changé les résultats. Avec les valeurs que nous
avons utilisé pour les inputs, cela se serait traduit par une
variation trois fois plus grande pour le condensat que pour les autres
inputs. Cela pourrait être un bon choix étant donné que le condensat
est bien moins connu que les autres inputs, et devrait donc varier
plus. Nous avons vérifier que nos conclusions ne sont pas modifiées
par ce changement de choix de dispersion.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Conséquences d'une petite déviations des observables}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Pertinence de petites déviations}

Dans le cas où le problème inverse peut être résolue exactement, si
les inputs sont très bien connues, les prédictions peuvent ne varier
que très peu, même si leurs sensibilités est très grande. Ceci n'est
pas forcement vrai lorsque le problème invserse est résolue en
minimisant un $\chi^2$ : dans ce cas il n'existe pas de jeu de
paramètre reproduisant exactement les inputs.

Nous verrons cependant que des variations de $0.6\%$ des inputs autour
de leur valeur moyenne suffit à changer totalement la physique décrite
par le modèle que nous considérons : le CEP pouvant ne plus exister.

Ces variations finies sont justifiées d'une part par le fait que les
inputs ne sont pas connues exactement. D'autre part, les sensibilités
sont calculées pour des prédictions faites pour un modèle donné, dans
un schéma d'approximation donné. En comparant les résultats obtenus
dans le cas où les approximations sont moins fortes, l'on peut évaluer
grossiérement l'ordre de magnitude des erreurs systématiques générées
par les approximations. 

Par exemple, en relaxant l'approximation de quasi-boson de Goldstone
dans le calcul des intégrales $I_2$, \idest en prenant $k^2 = m_\pi^2$
dans \refeq{eq:mpi}), nous trouvons $m_\pi = 135.6$ MeV, c'est à dire
une variation d'environ $1\%$.

De la même manière si l'on inclut les boucles mésoniques dans le
calcul des équations du gap, voir \getpaperref{49}, l'on voit que des
corrections de l'ordre de $5\%$ sont apportées aux propriétés
pionique. 





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Inprédictibilité du CEP}

Pour pouvoir voir concretement les effets d'une sensibilité grande ou
petite, nous autorisons une variation de $1\%$ des inputs autour de
leur valeur moyenne donnée par
\refeq{eq:mean-input-values}. Explicitement, les fenêtres des valeurs
autorisées pour les inputs sont:
\begin{align}
  m_\pi \; &\in \; [135.6\,,\, 138.4]  \nonumber \\
  f_\pi \; &\in \; [ 92.07 \,,\, 93.93] \nonumber \\
  \qbarq^{1/3} \; &\in \; [-318.1 \,,\, -311.8]  \;.
\end{align}

Pour pouvoir accéder aux distributions des prédictions, nous avons
utilisé un algorithme de type Monte-Carlo dont nous expliquons
maintenant le fonctionnement. Un ensemble de $n$ configurations des
inputs est choisi aléatoirement en suivant une certaine densité de
probabilité. Nous avons considéré le cas d'une distribution uniforme
$\rho^u(X)$, particulièrement adapatée pour l'analyse des erreurs
deterministes, qui est constante autour de sa valeur moyenne $\bar{X}$:
\begin{equation}
  \label{eq:uniform-distribution}
  \rho^u(X) = {\cal N} \theta(X-X_{\textrm{max}}) \theta(X_{\textrm{min}} - X) \;,
\end{equation}
où $\Theta$ est la distribution de Heaviside, où $\cal N$ est une
constante de normalisation, et où $X_{\textrm{min}} = (1-p) \bar X$, et
$X_{\textrm{max}} = (1+p) \bar X$, avec $p=1\%$.

Nous avons également considéré le cas d'une distribution gaussienne
$\rho^G(X)$ utilisée lorsque la variable aléatoire est supposée être
distribuée suivant la loi normale autour de sa valeur moyenne avec une
déviation standard $\sigma$. L'intérêt de la distribution gaussienne
réside dans le fait que des points très loin de la moyenne peuvent
être considérés, permettant d'explorer un plus grand domaine que celui
autorisé par la distribution uniforme. Pour comparer les résultats
obtenus avec ceux obtenus dans le cas de la distribution uniforme,
nous utilisons $\sigma = p \bar X$, et la distribution gaussienne
prend la forme :
\begin{equation}
  \label{eq:gaussian-distribution}
  \rho^G(X) = {\cal N} e^{ (X-\bar X)/2\sigma^2 } \;,
\end{equation}
où $\cal N$ est une constante de normalisation. Nous avons vérifié que
les résultats ne changeait pas qualitativement lorsque l'une ou
l'autre de ces distribution est utilisée.

Une fois que l'ensembe de $n$ de configurations d'input est choisi, le
problème inverse est résolue pour chacune des configurations, donnant
accès à un ensemble de $n$ configuration de paramètres. Pour chacune
des configurations de paramètres sont calculées $m_\sigma$, $g_{\pi
  \bar q q}$ et aussi la position du point critique chirale dans le
plan $(T-\mu)$. À la fin, les distributions des prédictions sont
obtenues. Pour chaque distributions $\rho_X$ de la prédiction $X$ une
valeur moyenne $\bar X$, une déviation standard $\sigma(X)$ peuvent
être calculées. La sensibilité de la prédiction $X$
\refeq{eq:sensitivity} devient alors:
\begin{equation}
  \label{eq:mc-sensitivity}
  \Sigma(X) = \frac{\sigma(X)}{\bar X} \frac{1}{\sigma_{rel}^{I}} \;.
\end{equation}

Notons que les sensibilités dans la limite des dispersions
infinitésimales ont été obtenues avec ce même algorithme en
considérant une dispersion $p=0.005 \%$. Nous avons vérifié que cette
valeur de dispersion était suffisement faible pour pouvoir extropoler
vers la limite $p=0$.

Nous avons également vérifié que les résultats ne dépendaient pas
qualitativement du choix de la distribution.


%%%%%%%%%%%%%%%
\paragraph{Distributions des paramètres du modèle}

La distribution de probabilité de $\alpha$ est tracée en
\reffig{fig:alpha-dist} , où la distribution théorique calculée en
\refapp{app:ana-prob-dist} est représentée, permettant de vérifier les
résultats.

\begin{figure}[t]
  \begin{center}
    \includegraphics[width=0.6\textwidth]{alpha-dist-Uniform-p0.01-n50.pdf}
    \caption{ \label{fig:alpha-dist} {\small Distribution de probabilité
        de $\alpha$ avec une dispersion $p=1\%$ des inputs dans le cas où
        la distribution uniforme. La ligne en trait plein représente la
        densité de probabilité théorique donnée par
        l'\refeq{eq:app-rho-alpha-theo}. Les cercles et croix sont les
        valeurs d'un histogramme normalisé (l'intégrale est égale à $1$)
        obtenu avec le Monte-Carlo. Les croix représentent les
        configurations de paramètres acceptés, \idest ceux pour lesquels
        la relation $\alpha > \alpha_c$ est vérifiée. Les cercles
        représentent les configurations des paramètres ne vérifiant pas
        cette relation. }}
  \end{center}
\end{figure}

La \reffig{fig:params-dist} représente les distributions obtenues pour
les paramètres du modèle NJL. Les moyennes et déviations standard des
distributions des paramètres sont également représentées, et
permettent de vérifier que la statistique considérée $n=50^3$ est
suffisement grande pour avoir des erreurs statistiques sous
contrôle. Pour chaque variable $X$ est également représenté un point
situé en $\bar X$, mais dont les barres d'erreurs sont données par
$S(X)~=~\overline X~\sigma_{rel}^I$. Ce point permet de comparer la
déviation standard relative d'une quantité par rapport à
$\sigma_{rel}^I$. Quand $\sigma(X) > S(X)$, alors la dispersion
choisie pour les inputs donne une dispersion plus grande pour la
quantité $X$. La sensibilité est donnée par le rapport de ces deux
grandeurs : $\Sigma(X) = \sigma(X) / S(X)$.


\begin{figure}[!ht]
  \begin{tabular}{lr}
    \includegraphics[width=0.5\textwidth]{m0-dist-Uniform-p0.01-n50.pdf} &
    \includegraphics[width=0.5\textwidth]{Lambda-dist-Uniform-p0.01-n50.pdf} \\
    \includegraphics[width=0.5\textwidth]{GL2-dist-Uniform-p0.01-n50.pdf}&
  \end{tabular}
  \caption{ \label{fig:params-dist} {\small Les lignes en tiret parsemées de
      croix (dash-crossed line en anglais) représentent les distributions
      des paramètres du modèle : $m_0$ (en haut à gauche), $\Lambda$ (en
      haut à droite), et $G\Lambda^2$ (en bas). Chaque distribution est un
      histogramme non-normalisé, \idest l'intégrale donne le nombre de
      point utilisé. Les résultats sont obtenus avec l'algorithme
      Monte-Carlo pour une dispersion $p=1\%$, avec $n=50^3$
      configurations d'input. L'étoile supérieure correspond à la moyenne,
      et les barres d'erreur à la déviation standard. L'étoile inférieure
      correspond également à la moyenne, mais les barres d'erreurs sont
      données par la multiplication de la moyenne par la dispersion
      moyenne des inputs, $S(X)=\overline X \sigma_{rel}^I$. }}
\end{figure}

La valeur de la masse nue des quarks $m_0$ est comprise en $4$ et $6$
MeV, qui sont des valeurs typiques trouvées dans la littérature. Nous
remarquons que la dispersion absolue du paramètre $G$ est faible, et
que sa valeur minimum se coupe brusquement. Cette coupure induit une
forte asymétrie de la distribution comme on peut le voir par la
position de la moyenne. Les valeurs de $G\Lambda^2$ sont localisées
autour de $2.1$ avec $\sigma(G) \simeq 0.1$, en accord avec les
valeurs courement utilisées. Enfin, la distribution pour le paramètre
$\Lambda$ a une déviation standard relativement grande, permettant à
ce paramètre d'avoir des valeurs un peu plus grande que celles
trouvées dans la littérature.

%%%%%%%%%%%%%%%
\paragraph{Distribution de la prédiction de la masse du méson sigma}
\label{ssec:Distributions of the in-vacuum predictions}

\begin{figure}[!ht]
  \begin{center}
    \includegraphics[width=0.6\textwidth]{msigma-dist-Uniform-p0.01-n50.pdf}
    \caption{ \label{fig:sigma_mass} {\small La ligne en tiret
        représente la distribution de $m_\sigma$ (histogramme
        non-normalisé dont l'intégral donne le nombre de points),
        obtenues avec l'algorithme Monte-Carlo en considérant une
        dispersion $p=1\%$, et un nombre de configurations $n=50^3$ dans
        le cas de la distribution gaussienne. Les étoiles ont la même
        signification qu'en \reffig{fig:params-dist}. } }
  \end{center}
\end{figure}

La \reffig{fig:sigma_mass} présente la distribution de $m_\sigma$, sa
valeur moyenne et sa déviation standard. Ces quantités sont
rassemblées dans le \reftab{tab:in-vacuum-qual-results}, avec les
valeurs correspondantes pour la prédiction $g_{\pi \bar q q}$. Nous
voyons que la distribution possède une longue queue, bien que la
distribution initiale choisie fût la distribution uniforme. Ainsi des
valeurs de $700$ MeV ne sont pas exclues. Comme on peut le voir dans
le tableau, les résultats pour $m_{\sigma}$ et pour
$g_{\pi{\bar{q}q}}$ sont en accords avec la phénoménologie obtenue
dans le cadre de modèle de quark : $m_\sigma \simeq 600$ MeV est la
valeur attendue pour la masse du sigma dans ce cadre\footnote{Un telle
  valeur pour la masse du méson scalaire ne peut pas être comparée aux
  résultats expérimentaux voir \getpaperref{50,51} pour une
  discussion.}.

\begin{table}[h]
  \centering
  \begin{tabular}{|l|r|}
    \hline
    $ \overline{m_\sigma}$   & $(0.6439 \pm 0.0003)$ (GeV) \\ \hline
    $ \sigma (m_\sigma) $     & $(0.0246 \pm 0.0002)$ (GeV) \\ \hline
    $\sigma(m_\sigma)/\overline{m_\sigma}$ & $3.82$ $(\%)$ \\ \hline\hline
    %    &\\ \hline
    $ \overline{g_{\pi \bar q q}}$  & $ 3.3822 \pm 0.0001 $\\ \hline
    $ \sigma(g_{\pi \bar q q})$     & $0.209\pm 0.002$   \\ \hline
    $ \sigma(g_{\pi \bar q q})/ \overline{g_{\pi \bar q q}}$ & $ 6.18$
    $(\%)$\\ \hline
  \end{tabular}
  \caption{ \label{tab:in-vacuum-qual-results} {\small Résultats
      obtenus pour les prédictions dans le vide $m_\sigma$ et
      $g_{\pi\bar q q}$, dans le cas de la distribution uniforme, pour
      une dispersion de $1\%$, et une nombre de configuration de $n^3
      = 50^3$.} }
\end{table}


\paragraph{Distribution de la prédiction de la position du CEP}
\label{ssec:Distribution of the chiral critical end point prediction}

Comme expliqué précédement, il est possible que la solution au
problème inverse existe, mais qu'elle ne donne pas de CEP. Lorsque la
dispertion des inputs est aussi basse que $0.6\%$, le CEP commence à
disparaître du diagramme de phase. La sensibilité de la température du
CEP est si grande, que la prédiction de la position du CEP est déjà
gâchée lorsque les inputs varient dans un domaine de $0.6\%$ autour de
leur valeur moyenne. L'existence même du CEP, même s'il existe lorsque
les inputs sont fixées à leur valeur moyenne, ne peut pas être
discutée dans le modèle que nous considérons. Le CEP ne peut pas être
considéré comme une ``vraie'' prédiction du modèle. 

Le nombre de configurations de paramètres obtenues et le nombre de CEP
calculés pour des dispersions de $1\%$ et $0.5\%$ sont rassemblés dans
le \reftab{tab:nb-CEP}. 

\begin{table}[!ht]
  \centering
  \begin{tabular}{|c|c|c|c|}
    \hline
    \multicolumn{4}{|c|}{Distribution uniforme} \\ \hline
    $p$ (\%) & $n_{\textrm{config}}$ & $n_{\textrm{CEP}}$ & 
    $n_{\textrm{CEP}} / n_{\textrm{sets}}$  (\%) \\ \hline
    1 & 3375 & 3066 & 91 \\ \hline
    0.5 & 3375 & 3375 &  100 \\ \hline
  \end{tabular}
  \caption{
    \label{tab:nb-CEP}
          {\small À partir des $n^3 = 15^3$ configurations de inputs générées
            par le Monte-Carlo pour une dispersion $p$, $n_{\mathrm{config}}$
            configurations de paramètres ont pu être calculées (le problème
            inverse a une solution). Parmi ces configurations de paramètres,
            $n_{\mathrm{CEP}}$ admettent un CEP.}}
\end{table}

En \reffig{fig:CEP-dist}, les CEPs obtenus lors du calcul sont
représentés, avec les ellipses de confiance à $1-\sigma$ et
$2-\sigma$. Comme la densité de probabilité $\rho(T_{CEP}, \mu_{CEP})$
n'est pas une distribution gaussienne, ces ellipses de confiance sont
une approximation des vraies ellipses à $1-\sigma$ et
$2-\sigma$. Elles permettent juste d'avoir un moyen visuel de
représenter la matrice de covariance (le demi grand-axe et le demi
petit-axe sont les valeurs propres de cette matrice).

Lorsque la dispersion est de $1\%$ la distribution des CEPs s'étend
dans une large fenêtre de température $T_{\textrm{CEP}} \, \in \,
[0\,,55]$ MeV, qu'une telle disperion implique une variation de $3$
MeV maximum des inputs. La distribution s'étend dans une fenêtre
réduite de potentiel chimique $\mu_{\textrm{CEP}} \, \in \,
[324\,,332]$ MeV. 

Comme dit, précédement, il existe des configurations de paramètres ne
donnant pas de CEP. Pour toutes ces configurations, la transition à
température nulle est un crossover. Pour pouvoir visualiser le fait
que des paramétrisations ne donnent pas de CEP, nous ajoutons un point
dans le diagramme correspondant à un non-CEP. Ce point est placé à
température nulle, et son potentiel chimique est tel que $\dd \mu /
\dd m = 0$ at $T=0$\footnote{C'est le potentiel chimique
  caractéristique de la transition type crossover à température
  nulle.}. La \reffig{fig:cep-density} permet de visualiser la densité
de CEP dans le diagramme de phase, et a été crée en utilisant la
technique de \emph{Kernel density approximation} (KDE), décrite
rapidement en \refapp{app:Kernel density approximation}. Cette densité
est normalisée, l'intégrale sur $\mathbb{R}^2$ de cette distribution
est $1$, et a pour dimension [GeV$^{-2}$].

\begin{figure}[!ht]
  \begin{center}
    \includegraphics[width=0.6\textwidth]{cep-dist.pdf}
    \caption{\label{fig:CEP-dist}{\small Les CEPs obtenus pour la
        distribution uniforme avec une dispersion de $p = 1\%$ et un
        nombre de configurations d'inputs de $n=20^3$. Sont également
        données la position de la moyenne$(\mucep\,,\, \Tcep) = (
        0.327 \,,\, 0.026 )$ (GeV), les deviations standard
        $\sigma(T)=0.019$ GeV et $\sigma(\mu)=0.0076$ GeV, et les
        approximations des ellipses de confiance (voir texte) à $1-$
        et $2-\sigma$. Sont également représentées les directions
        principales de ces ellipses ($V_0=0.0013$ GeV and $V_1=0.0138$
        GeV) données par les valeurs propres de la matrice de
        corrélation.  Lorsque la dispersion des inputs augmente, les
        CEPs semblent disparaître par le bas, vers des températures de
        plus en plus faible.  }}
  \end{center}
\end{figure}

Le \reftab{tab:in-medium-qual-results} donne les moyennes et
déviations standard des coordonnées du CEP. Le rapport de la déviation
sur la moyenne peut paraître faible comparé à la valeur de la
sensibilité. C'est en fait un artéfact de la méthode dû aux CEPs
manquants qui ne rentrent pas en compte dans le calcul de la moyenne
et de la déviation standard. L'on peut remarquer que ce ratio est
étonnement faible pour $\mucep$.

Ces résultats sont en accord avec les valeurs des sensibilités à
dispersion infinitésimale que nous avons trouvées pour le CEP. Comme
déjà remarqué, le problème est suffisement linéaire autour des valeurs
moyennes pour que le calcul de la sensibilité fait à dispersion finie
garde son sens.


\begin{table}[h]
  \centering
  \begin{tabular}{|l|r|}
    \hline
    $ \overline{\Tcep}$   & $(0. 0303 \pm 0. 0001)$ (GeV) \\ \hline
    $ \sigma (\Tcep) $     & $(0. 0107 \pm 0.0001)$ (GeV) \\ \hline
    $\sigma(\Tcep)/\overline{\Tcep}$ & $35.25$ $(\%)$ \\ \hline\hline
    %    &\\ \hline
    $ \overline{\mucep}$  & $(0. 3280 \pm 0.0001)$ (GeV)\\ \hline
    $ \sigma(\mucep)$     & $(0. 0018 \pm 0.0001)$   (GeV)\\ \hline
    $ \sigma(\mucep)/ \overline{\mucep}$ & $0.54 $ $(\%)$\\ \hline
  \end{tabular}
  \caption{ \label{tab:in-medium-qual-results} {\small Résultats
      obtenus pour les prédictions dans le milieu, \idest concernant
      la position du CEP, dans le cas de la distribution uniforme et
      d'un nombre de configurations d'inputs de $n^3 = 20^3$, à
      dispersion $p = 1\%$. Ces résultats ne comprennent que les CEPs
      trouvés, et les résultats présentés sont biaisés (voir
      texte). Si les CEPs manquant avaient été utilisés les déviations
      standard relatives auraient été plus grandes.  }}
\end{table}


\begin{figure}[!ht]
  \begin{center}
    \includegraphics[width=0.7\textwidth]{cep-density.pdf}
    \caption{ \label{fig:cep-density} {\small \emph{Kernel Density
          Approximation} (KDE) de la distribution des CEPs, obtenue dans
        les mêmes conditions que dans
        \reftab{tab:in-medium-qual-results}. La couleur représente l'axe
        $z$ du graphique dont l'unité est le GeV$^{-2}$. Toutes les
        paramétrisations donnent un point dans le diagramme. Les points
        situés sur l'axe $T=0$ représentent le fait qu'un CEP n'a pas pu
        être trouvé pour une paramétrisation (voir texte).}}
  \end{center}
\end{figure} 




%%%%%%%%%%%%%%%
\paragraph{Correlation de la position du CEP et des observables}

Comme expliqué dans l'introduction de cette partie, il peut être
intéressant de regarder les corrélations entre les prédictions d'un
modèle et les inputs. 

Les corrélations entre deux quantités $A$ et $B$, pouvant aussi bien
être des inputs que des paramètres, que des prédictions, peut être
obtenue grâce aux coefficient de la matrice de corrélation $C_{AB}$
définie comme \getpaperref{34}: 
\begin{equation}
  \label{eq:definition-correlation-coef}
  C_{AB} =  \frac{ \left| \textrm{Covar}(A,B) \right| }  {\sigma(A)
    \sigma(B) } \;,
\end{equation}
où $\textrm{Covar}(A,B)$ est le coefficient hors-diagonale de covariance
de $A$ et $B$. Explicitement, si $A_i$ et $B_i$ sont des points
générés aléatoirement constituant les ensembles $A$ et $B$, l'élement
correspondant de la matrice de covariance est donné par:
\begin{equation}
  \textrm{Covar}(A,B) = \frac 1 {n - 1}
  \sum_{i = 1}^{n} (A_i - \bar A) (B_i - \bar B) \;,
\end{equation}
où $\bar A$ et $\bar B$ sont les valeurs moyennes des deux ensembles
$A$ et $B$. Cette matrice est symétrique, $\textrm{Covar}(B,A) =
\textrm{Covar}(A,B)$, et les élements diagonaux sont les variances,
$\textrm{Covar}(A,A) = \sigma^2(A)$.

Quand le coefficient de corrélation est proche de $1$, les deux
quantités sont très corrélées (par exemple, $C_{AA} = 1$). Au
contraire, lorsqu'il est nul, les deux quantités sont non corrélées. 

Ces définitions s'appliquent aux problèmes linéaires. Lorsque le
problème est non linéaire, comme ici, les conclusions peuvent changer
et doivent être prise avec précautions. Par exemple, dans le cas non
linéaire, un coefficient de corrélation quasi nul entre deux quantités
ne signifie plus nécessairement que ces quantités sont quasi
indépendantes. Dans le cas non linéaire, il est donc important de
vérifier les conclusions faites par observations des coefficients de
corrélation, en traçant les densités de probabilités jointes des
ensembles $A$ et $B$.

La \reffig{fig:correlations-inputs} représente les corrélations entre
les coordonnées du CEP et les inputs. Comme la distribution initiale
des inputs est uniforme, une corrélation parfaite entre deux quantités
est représentée par une ligne, où la densité de point est
constante. L'indépendance de deux quantités est représentée par un
rectangle où la densité de point est constante.

Comme nous l'avons expliqué plus haut, l'étude des corrélations est
importante pour pouvoir déceler quelles sont les observables qui
contraignent le plus les prédictions. Sur la
\reffig{fig:correlations-inputs}, les corrélations sont représentées
pour une dispersion de $1\%$. Le \reftab{tab:correl} donne les valeurs
des coefficients de corrélation entre les coordonnées du CEP avec les
inputs et les paramètres, calculés avec
l'\refeq{eq:definition-correlation-coef}.


\begin{figure*}[!ht]
  \begin{center}
    \begin{tabular}{lr}
      \includegraphics[width=0.5\textwidth]{cor-Tcep-mpi.pdf} &
      \includegraphics[width=0.5\textwidth]{cor-mucep-mpi.pdf} \\
      \includegraphics[width=0.5\textwidth]{cor-Tcep-fpi.pdf} &
      \includegraphics[width=0.5\textwidth]{cor-mucep-fpi.pdf} \\
      \includegraphics[width=0.5\textwidth]{cor-Tcep-qq.pdf} &
      \includegraphics[width=0.5\textwidth]{cor-mucep-qq.pdf} \\
    \end{tabular}
  \end{center}
  \caption{ \label{fig:correlations-inputs} {\small Corrélation de
      $\Tcep$ (colonne de gauche) et de $\mucep$ (colonne de droite)
      avec les inputs, dans le cas de la distribution uniforme pour une
      dispersion de $1\%$, avec un nombre de configuration d'input $n =
      20^3$. La méthode KDE est utilisée pour représenté la densité de
      probabilité des deux ensembles considérés. La couleur représente
      l'axe $z$ dont l'unité est le GeV$^{-2}$. }}
\end{figure*}


Nous remarquons que la position du CEP est quasiment indépendante de
la valeur de la masse du pion. Cela résulte du fait que les quantités
que ne dépendent pas de $m_0$ sont assez peu sensible à la valeur
précise de $m_\pi$. Cette quasi indépendance peut se voir sur le
graphique, avec une forme quasi rectangulaire, et dans le tableau,
avec des coefficients de l'ordre de $0.1$. On remarque également que
la corrélation entre $\Tcep$ et $\mucep$ avec la constante de
désintégration du pion est forte (très forte pour $\mucep$). 

Nous pouvons voir sur la figure les points où un CEP n'a pas
été trouvé. Ces points sont surtout corrélés avec les faibles valeurs
de la constante de désintégration du point. Cette remarque n'aurait
pas pu être faite si les coefficients de corrélation seuls avaient été
calculés. L'analyse des graphiques de corrélations peut donner accés à
des informations inaccéssible avec la matrice de corrélation.

La seule différence qualitative concernant les corrélations entre les
coordonnées du CEP et les inputs provient des corrélations avec le
condensat de quark. $\Tcep$ est environ $4$ fois plus corrélé avec lui
que ne l'est $\mucep$. Comme déjà mentionné précédement, ce genre de
relation n'aurait pas pu être trouvée sur la base d'argument physique,
ce qui illustre encore une fois l'utilité de l'analyse des
corrélations. Cette différence entre $\Tcep$ et $\mucep$ est peut être
à l'origine de la grande sensibilité de la prédiction de $\Tcep$
comparée à la faible sensibilité de $\mucep$.


\begin{table}
  \begin{center}
    \begin{tabular}{|l|c|c|c|}
      \hline
      Corrélations de & & $T_{CEP}$ & $\mu_{CEP}$  \\
      \hline
      \multirow{3}{*}{avec les inputs} 
      & $m_\pi$   &  $0.021$ &  $0.123$ 
      \\ \cline{2-4} 
      & $f_\pi$   &  $0.646$ &  $0.987$ 
      \\ \cline{2-4}
      & $\qbarq$ &  $0.591$ &  $0.130$ 
      \\ \hline        
      \multirow{3}{*}{avec les paramètres} 
      & $m_0$   &  $0.797$ &  $0.494$ 
      \\ \cline{2-4} 
      & $\Lambda$    &  $0.933$ &  $0.445$
      \\ \cline{2-4}
      & $G\Lambda^2$ &  $0.975$ &  $0.686$ 
      \\ \hline
    \end{tabular}
  \end{center}
  \caption{ \label{tab:correl} {\small Coefficients de corrélation
      entre les coordonnées du CEP et les inputs ou les paramètres du
      modèle, calculé dans la cas de la distribution uniforme pour une
      dispersion de $1\%$ et un nombre de configurations d'inputs $n =
      20^3$.}}
\end{table}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Conclusions}

Dans cette étude, nous avons étudié l'effets de variations
infinitésimales des inputs, utilisés pour fixer les paramètres d'un
modèle NJL, grâce à un paramètre de sensibilité.

Le paramètre de sensibilité tel que nous l'avons définie donne un
moyen systématique de savoir quantitativement la qualité d'une
prédiction. Lorsque ce paramètre est grand ($\gg 1$), la prédiction
dépend fortement de la valeur des inputs utilisés pour fixer les
paramètres du modèle : la prédiction est alors instable. Au contraire,
lorsque ce paramètre est de l'ordre de l'unité, la prédiction ne
dépend pas de la valeur précise des inputs utilisés : la prédiction
faite est stable.

Dans la plupart des études cherchant à connaitre la stabilité d'une
prédiction donnée, les auteurs font varier les paramètres directement,
et analyse l'influence de ces variations sur les prédictions. En se
basant sur la théorie du problème inverse, nous avons préféré faire
varier directement les inputs, c'est à dire les données à partir
desquelles les paramètres sont fixés. L'interêt de cette méthode
réside dans le fait que les non linéarités du problème peuvent mettre
en exergue des relations non trivialles qu'il aurait été impossible
d'obtenir par simple variation des paramètres. 

D'une manière générale, la résolution du problème inverse est une
tâche difficile, car elle revient à résoudre un système d'équations
non linéaires. Il se trouve que dans le cas du modèle utilisé,
l'inversion peut se faire de manière exacte. Nous avons pu alors
accéder aux sensibilités de prédictions faites dans le vide (la masse
du méson sigma, et la constante de couplage effective
pion-quark-antiquark), et dans le milieu (coordonnées du CEP dans le
plan $(T,\mu)$), en considérant des variations infinitésimales des
inputs. Nous avons montré que les prédictions faites dans le vide sont
relativement stables. Nous avons également montré que la prédictions
de la température du CEP est très instable, alors que celle concernant
le potentiel chimique du CEP est très stable.

Pour comprendre quel est l'impact de la stabilité d'une prédiction,
nous avons ensuite autorisé des variations finies des inputs de
l'ordre de $1\%$. Cette variation est relativement grande comparée aux
erreurs expérimentales connues des inputs, mais est faible en
comparaison des corrections apportées lorsque les calculs sont fait à
l'ordre supérieur. Nous avons montré qu'une variations des inputs de
cet ordre suffit à avoir des paramétrisations, \idest le problème
inverse a une solution, qui ne donnent pas de CEP. La largeur de la
fenêtre en température du CEP est de l'ordre de $60$ MeV, alors
qu'elle n'est que de quelques MeV pour le potentiel chimique. Cette
observation est certainement dû à la différence de sensibilité de ces
deux prédictions.

Pour obtenir des informations complémentaires, nous avons également
fait une analyse des corrélations en suivant des méthodes bien connues
et utilisée en physique nucléaire. Nous avons montré que, dans le cas
où le problème est non linéaire, la connaissance seule des
coefficients de la matrice de corrélation ne suffit pas. Il est
également nécessaire de tracer les graphiques de corrélation. Nous
avons vu que les coordonnées du CEP ne sont que peu corrélées à la
masse du pion, mais sont particulièrement corrélées à la constante de
désintégration du pion. La seule différence qualitative concernant les
corrélations, et celles avec le condensat de quark. L'on a vu que
$\Tcep$ est corrélée avec le condensat alors que $\mucep$ l'est
moins. Ceci est peut être la cause de la forte sensibilité de la
tempéture du CEP.

Pour réaliser cette étude, nous avons d'une part réaliser que le
problème inverse pouvait se résoudre analytiquement, et d'autre part,
nous avons élaboré un algorithme de recherche de CEP rapide et efficace,
nécessaire pour pouvoir calculer un grand nombre de CEP en un temps
fini. 

La résultat le plus remarquable de cette étude est que l'existence
même du CEP est remise en question lorsque les inputs sont variés de
$0.6\%$. Cette remise en question est dû au fait que la température du
CEP prédite par le modèle est très sensible à la valeur des inputs
utilisées. Etonnement, la prédiction du potentiel chimique du CEP dans
ce modèle est une prédiction très stable, plus stable encore que les
prédictions faites dans le vide. Ce résultat est étonnant bien sûr,
puisque l'intuition nous dicte qu'un modèle dont les paramètres sont
fixés sur des quantités mesurées dans le vide donnera de meileures
résultats pour des prédictions dans le vide. La stabilité de $\mucep$
est assez contre-intuitive.

La stabilité de la prédiction du potentiel chimique du CEP ne signifie
pas que la valeur prédite soit correcte. Il y a d'ailleurs fort à
parier que l'inclusion des intéractions dans les canaux vecteurs
modifie cette valeur. 

\needinfo{A finir, modifier, selon ce que j'ai le temps de faire}






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Inversion analytique du modèle NJL}
\label{app:inverse-pb}

Le modèle NJL que nous considérons ici, dont le lagrangien est donné
par l'\refeq{eq:NJL-Lagrangian} a trois paramètres $m_0$, $\Lambda$ et
$G$, dont les valeurs sont ajustées pour reproduire la masse du pion,
$m_\pi$, la constante de désintégration du pion, $f_\pi$, et le
condensat de quark $\qbarq$.

Le système d'équation définissant le problème inverse est donné par
les équations \refeq{eq:mpi}, \refeq{eq:fpi} and
\refeq{eq:cond}. Quand $(m_\pi,\ f_\pi, \qbarq)$ sont fixés à leur
valeur phénoménologique, le sytème : 
\begin{align}
  \label{eq:app-inverse-problem-sys-njl-1}
  m_\pi^2   & =  - \frac{m_0}{m} \frac{1}{4 i G N_c N_f I_2(0)} \;, \\
  \label{eq:app-inverse-problem-sys-njl-2}
  f_\pi^2   & =   -4i N_c m^2 I_2(0)  \;, \\
  \label{eq:app-inverse-problem-sys-njl-3}
  \qbarq   & =  \frac{m_0 - m}{2G}\;,
\end{align}
doit être résolu pour les paramètres $\Lambda$, $m_0$ et $G$,
conjointement avec la résolution de l'équation du gap,
\refeq{eq:gap-equation}, permettant d'accéder à la masse dynamique des
quarks $m$: 
\begin{equation}
  \label{eq:app-gap-equation}
  m_0 - m + 8iGN_cN_f m I_1 = 0 \;.
\end{equation}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Réduction du système}


L'idée est de réduire le système en utilisant des quantités
adimensionalisées. Pour ce faire, nous réécrivons les \refeq{eq:I1}
and \refeq{eq:I2} : 
\begin{align}
  I_1 &= -i \Lambda^2 i_1(m/\Lambda), \nonumber \\
  \label{eq:app-definition-i1}
  \mbox{avec } i_1 (x) &\equiv 
  \int^1 \frac{\dd^3 p }{(2\pi)^3} 
  \frac{1}{2\sqrt{p^2+x^2}}  \;,
\end{align}
et 
\begin{align}
  I_2(0) &= \frac i 4 i_2(m/\Lambda), \nonumber \\
  \mbox{avec } i_2(x) &\equiv 
  \int^1 \frac{\dd^3 p }{(2\pi)^3} 
  \frac{-1}{ \left({p^2+x^2}\right)^{3/2}} \;.
  \label{eq:app-definition-i2}
\end{align}

Les intégrales $i_1$ et $i_2$ sont analytiques et sont données par : 
\begin{align}
  \label{eq:app-integral-i1-analytic}
  i_1(x) & =  \frac{1}{8\pi^2} \left[ 
    \Lambda_E + x^2 \log \left( \frac{x}{1+ \Lambda_E} \right) \right]\;, \\
  i_2(x) &=  \frac{1}{2\pi^2} \left[ 
    \frac{1}{\Lambda_E} +  \log \left( \frac{x}{1 + \Lambda_E} \right) \right]\;,
\end{align}
où $\Lambda_E = \sqrt{x^2 + 1}$.

Pour résoudre le système, l'échelle $\Lambda$ est utilisée pour
adimensionnaliser les variables :
\begin{equation}
  x = m / \Lambda \quad \textrm{and}\quad x_0 = m_0 / \Lambda \;.
\end{equation}
Avec ces variables, le sytème peut se réécrire comme : 
\begin{align}
  \label{eq:app-inverse-problem-sys-njl-1-new}
  \frac{m_\pi^2}{\Lambda^2} &= \frac{x_0}{x} \frac{1}{G\Lambda^2 N_c N_f i_2(x)} \;,\\
  \label{eq:app-inverse-problem-sys-njl-2-new}
  \frac{f_\pi^2}{\Lambda^2} &= N_c x^2 i_2(x) \;,\\
  \label{eq:app-inverse-problem-sys-njl-3-new}
  \frac{\qbarq}{\Lambda^3} &= \frac{x_0 - x}{2 G \Lambda^2} \;,
\end{align}
et l'équation du gap devient : 
\begin{equation}
  0 = x_0 - x + 8 G \Lambda^2 N_c N_f x i_1(x) \;.
\end{equation}
Cette équation est automatiquement résolue si 
\begin{equation}
  \label{eq:app-equation-for-Gamma}
  G\Lambda^2 = \frac{x-x_0}{8 N_c N_f x i_1(x)} \;.
\end{equation}
Cette forme pour $G\Lambda^2$ peut être introduite dans les équations
\refeq{eq:app-inverse-problem-sys-njl-2-new} et
\refeq{eq:app-inverse-problem-sys-njl-3-new}. En introduisant la
nouvelle variable :
\begin{equation}
  \delta = \frac{x - x_0}{x_0} \;,
\end{equation}
le sytème s'écrit :
\begin{align}
  \label{eq:app-Sys-1}
  \frac{m_\pi^2}{\Lambda^2} &= \frac{8 i_1(x)}{\delta i_2(x)} \;,\\
  \label{eq:app-Sys-2}
  \frac{f_\pi^2}{\Lambda^2} &= N_c x^2 i_2(x) \;,\\
  \label{eq:app-Sys-3}
  \frac{\qbarq}{\Lambda^3} &= -4 N_c N_f x i_1(x) \;.
\end{align}

Sous cette forme, le système peut se résoudre en calculant dans un
premier temps le ratio: 
\begin{equation}
  \label{eq:app-def-alpha}
  \alpha = \frac{f_\pi^3}{\qbarq} \;.
\end{equation}
Ce ratio est une constante phénoménologique indépendente de l'échelle
$\Lambda$. Ensuite, il faut trouver une solution pour $x$, qui est la
solution d'un système d'une équation à une inconnue ($\alpha$) si
l'on considère le quotient de \refeq{eq:app-Sys-2} et
\refeq{eq:app-Sys-3} :
\begin{equation}
  \label{eq:app-decomp-sys-1}
  G_\alpha(x) = 0 \;,
\end{equation}
avec: 
\begin{equation}
  G_\alpha(x) = \frac{\sqrt{N_c}}{4N_f} \frac{x^2 (i_2(x))^{3/2}}{i_1(x)} + \alpha  \;.
\end{equation}
Une fois que \refeq{eq:app-decomp-sys-1} est résolue, l'on peut
calculer $\delta$ en calculant le ratio $f_\pi^2/m_\pi^2$ :
\begin{equation}
  \label{eq:app-decomp-sys-2}
  \delta = \frac{f_\pi^2}{m_\pi^2} \frac{8i_1(x)}{N_c(xi_2(x))^2} \;.
\end{equation}
Enfin, il suffit de remettre le système à l'échelle en calculant:
\begin{equation}
  \label{eq:app-decomp-sys-3}
  \Lambda = \frac{f_\pi}{x \sqrt{N_ci_2(x)}} \;.
\end{equation}
Les valeurs de $m_0$ et $G$ sont alors données par: 
\begin{align}
  \label{eq:app-m0-equation}
  m_0 &= \Lambda \frac{x}{\delta + 1} \;,\\
  \label{eq:app-G-equation}
  G &= \frac{1}{\Lambda^2} \frac{x - x_0}{ 8  N_c  N_f x  i_1(x)} \;.
\end{align}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Solution pour $x$}


\begin{figure}[!ht]
  \begin{center}
    \includegraphics[width=0.5\textwidth]{Galpha.pdf}
    \caption{ {\small Fonction $G_\alpha(x)$ pour trois valeurs
        typique du paramètre $\alpha$, toujours négatif pour des
        valeurs phénoménologiques physiques des inputs. $0 > \alpha >
        \alpha_c$ (courbe du haut): il existe deux solutions. $\alpha
        = \alpha_c$ (courbe du mileu): il existe une solution
        dégénérée au maximum. $\alpha < \alpha_c$ (courbe du bas): il
        n'existe pas de solutions. Lorsque $\alpha > \alpha_c$, le
        système a deux solutions, l'une est physique, lorsque $x <
        x_{\textrm{max}}$, et l'autre ne l'est pas, lorsque
        $x>x_{\textrm{max}}$, où $x_{\textrm{max}}$ est l'abcisse du
        maximum de $G_\alpha$. }}
    \label{fig:app-Galpha} 
  \end{center}
\end{figure}

Si aucun des paramètres n'est nul, alors le système donnés par les
équations \refeq{eq:app-Sys-1}, \refeq{eq:app-Sys-2}, et
\refeq{eq:app-Sys-3} est équivalent à l'équation
\refeq{eq:app-decomp-sys-1}. Cette équation doit être résolue pour
$x$, et suivant la valeur de la constante $\alpha$ (voir
l'\refeq{eq:app-def-alpha}), le système peut être résolue ou non. La
\reffig{fig:app-Galpha} donne un représentation de la fonction
$G_\alpha$ tracée en fonction de $x$ pour trois valeurs typique du
paramètre $\alpha$. Nous voyons graphiquement que si $\alpha >
\alpha_c$, où $\alpha_c$ est définie comme la valeur de $\alpha $ pour
lequel la valeur de la fonction $G_\alpha$ pour $x = x_{\textrm{max}}$
est nulle:
 \begin{equation}
  \label{eq:app-definition-of-alpha-c}
  \alpha_c \quad \Leftrightarrow \quad G_{\alpha_c}(x_\textrm{max}) = 0 \;.
\end{equation}
 alors l'équation \refeq{eq:app-decomp-sys-1} possède des
 solutions.

Comme l'abcisse du maximum de $G_{\alpha}(x)$ est indépendante de
$\alpha$, il suffit de calculer une seule fois la valeur critique
$\alpha_c = -0.0283275$. Pour toutes les valeurs de $\alpha$
respectant la condition $\alpha > \alpha_c$, le système possède deux
solutions. La première peut être trouvée numériquement dans
l'intervalle $[0 , x_{\textrm{max}}]$. Cette solution correspond à la
masse dynamique ``physique'' des quarks $x = m/\Lambda <
x_{\textrm{max}}$. La seconde racine correspond à une solution non
physique\footnote{Cette seconde solution est un autre moyen de voir le
  phénomène décrit dans \getpaperref{43} (figure 2.6).} pour la masse
$m/\Lambda > x_{\textrm{max}}$. Si l'on s'intéresse à cette solution non
physique, un moyen de la trouver est de calculer l'asymptote de
$G_{\alpha}(x)$ en $x\to +\infty$ donnée par: 
\begin{equation}
  G^{\infty}_{\alpha}(x) = \sqrt{\frac{N_c}{3}} \frac{x^{-3/2}}{4\pi N_f} + \alpha \;.
\end{equation}
La solution de $G^{\infty}_{\alpha}(x) = 0$ est analytique et s'écrit: 
\begin{equation}
  x^{\infty} = - \left(\frac{N_c}{24}\right)^{1/3} (\alpha \pi N_f)^{-2/3} \;.
\end{equation}
La solution non physique peut alors être chercher numériquement dans
l'intervalle \mbox{$[x_{\textrm{max}} , x^{\infty}]$}.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Algorithme rapide pour le calcul de la position du CEP}
\label{app:cep-calc}

Pour étudier les propriétés dans le milieu du modèle NJL, il faut
généraliser l'équation du gap \refeq{eq:app-gap-equation}, écrite dans
le vide, à température et potentiel chimique finis. L'équation du gap
à résoudre alors est:
 \begin{equation}
  \label{eq:finite-T-mu-MFE}
  g_m(m,T,\mu ) = 0 \;,
\end{equation}
où:
\begin{equation}
  \label{eq:finite-T-mu-gm}
  g_m = m_0 - m + 8 G N_c N_f m  
         \left[ i I_1(m,\Lambda) - I_\beta(m,T,\mu) \right]  \;,
\end{equation}
avec $I_1$ l'intégrale donnée par l'\refeq{eq:app-definition-i1}, et
$I_\beta$ l'intégrale définie par:
\begin{equation}
  \label{eq:integral-I-beta}
  I_\beta = \int^{\infty} \frac{\dd^3 p }{(2\pi)^3} \frac{1}{2E_p} 
           \left[ f(p) + \bar{f}(p) \right]  \;,
\end{equation}  
où $E_p =\sqrt{p^2 + m^2}$ et $f$ et $\bar f$ sont les distributions
de Fermi-Dirac pour les quarks et les antiquarks respectivement: 
\begin{align}
  f(p) &= \frac{1}{1+\exp\left(\beta( E_p - \mu ) \right)} \nonumber
  \;,\\ \bar f(p) &= \frac{1}{1+\exp\left(\beta(E_p + \mu) \right)}
  \nonumber \;,
\end{align}
avec $\beta = T^{-1}$ et $k_B = 1$. La solution de
l'\refeq{eq:finite-T-mu-MFE} est tracée en fonction du potentiel
chimique pour plusieurs températures en \reffig{fig:m-of-Tmu}. 

\begin{figure}[!ht]
  \begin{center}
    \includegraphics[width=0.6\textwidth]{m-of-Tmu.pdf}
    \caption{\label{fig:m-of-Tmu} {\small Solution $m$ de l'équation
        du gap dans le milieu en fonction du potentiel chimique pour
        trois valeurs de la température. Le comportement dans les
        trois cas sont décrit dans le texte.}}
  \end{center}
\end{figure}

La solution $m$ des équations du gap dans le milieu possède trois
comportements différents:
\begin{itemize}
\item Lorsque $T > T_{\textrm{CEP}}$, il n'y a qu'une solution $m(T,\mu)$
  caractéristique d'une transition de type crossover entre la phase à
  symétrie chirale spontanément brisée ($\qbarq \neq 0$) vers la phase
  à symétrie chirale restaurée  ($\qbarq \simeq 0$) ;
\item Lorsque $T < T_{\textrm{CEP}}$, il existe, dans une certaine
  fenêtre de potentiel chimique, trois solutions pour $m$,
  caractéristique d'une transition de premier ordre, dont les
  solutions sont caractérisées de stable, métastable et instable; 
\item Lorsque $T = T_{\textrm{CEP}}$, il existe une unique solution
  pour $m$. Cependant, si $\mu = \mu_{\textrm{CEP}}$, alors la
  tangente à la courbe $m(T,\mu)$ en ce point est infinie. Au CEP, la
  transition est de second ordre.
\end{itemize}

Pour calculer les coordonnées du CEP, il faut calculer la température
et le potentiel chimique pour lesquels la courbe $m$ possède une
tangente infinie unique\footnote{Dans le cadre du modèle NJL considéré
  ici, il est équivalent de travailler avec la masse ou avec le
  condensat de quark, qui est le ``vrai'' paramètre d'ordre de la
  transition.}: 
\begin{equation}
  \left. \frac{\dd m}{\dd \mu} \right|_{T=\Tcep} = + \infty \quad
  \Leftrightarrow \quad \left.  \frac{\dd \mu }{\dd
    m}\right|_{T=\Tcep} = 0 \;,
\end{equation}
où la fonction $\mu(m,T)$ est une solution implicite de l'équation
\mbox{$g_m(m,T,\mu(m,T)) = 0$}. Il est plus simple d'étudier la
fonction $\mu(m,T)$ car elle est toujours uniquement définie, à
l'inverse de la fonction $m(T,\mu)$ qui peut être multivaluée.

A cause de l'unicité de la tangente infinie au CEP, ce point est
également un point d'infléxion. Ainsi, les coordonnées du CEP sont
trouvées en résolvant le système d'équations suivant:
\begin{align}
  \label{eq:sys-CEP-mfe}
  g_m(m_{\textrm{CEP}},T_{\textrm{CEP}},\mu_{\textrm{CEP}}) &= 0 \; , \\
  \label{eq:sys-CEP-derivative}
  \left.\frac{\dd \mu}{\dd m}\right|_T (m_{\textrm{CEP}},T_{\textrm{CEP}},\mu_{\textrm{CEP}}) 
                          &= 0 \; , \\
  \label{eq:sys-CEP-inflexion}
  \left.\frac{\dd^2 \mu}{\dd m^2}\right|_T (m_{\textrm{CEP}},T_{\textrm{CEP}},\mu_{\textrm{CEP}}) 
                          &= 0 \;.
\end{align}

La méthode traditionnelle pour calculer les coordonnées du CEP est de
résoudre numériquement l'\refeq{eq:sys-CEP-mfe}, et ensuite de trouver
le maximum de $\dd \mu / \dd m$ quelque soit $T$, et enfin de trouver
$T$ tel que $\dd \mu / \dd m$ soit nulle. Cette méthode marche bien,
si les dérivées ne sont pas calculées numériquement, et si les valeurs
initiales données à l'algorithme ne sont pas trop éloignées des
solutions. Le temps nécessaire pour trouver les coordonnées d'un CEP
en utilisant cette méthode est de l'ordre de la demi seconde. Dans la
suite, nous présentons un algorithme permettant d'accélerer les
calculs.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Réécriture des équations du gap}

Nous introduisons les nouvelles variables:
\begin{equation}
\sigma = \beta \mu \qquad \textrm{and} \qquad x = \beta m \;.
\end{equation}
À la différence de l'adimensionnalisation pratiquée précedement,
l'échelle utilisée ici est la température. Nous gardons les mêmes noms
de variables car aucune confusion n'est possible. En utilisant le
changement de variable $p\to\beta p$ dans l'intégrale donnée par
l'\refeq{eq:integral-I-beta}, l'on peut écrire:
\begin{equation}
  \label{eq:app-integrales-I-beta-et-i-beta}
  I_\beta ( m , T , \mu ) = T^2 i_\beta ( \sigma , x ) \;,
\end{equation}
où $i_\beta$ est donnée par: 
\begin{equation}
  \label{eq:app-definition-i-beta}
 i_\beta(\sigma,x) = 
              \int^\infty \frac{\dd^3 p }{(2\pi)^3} \frac{1}{2E} 
                          \left[ f(p) + \bar f (p) \right] \;,
\end{equation}
où $E = \sqrt{p^2 + x^2}$, et $f$ et $\bar f$ sont les distributions
de Fermi-Dirac $f = [ 1 + \exp(E\pm\sigma)]^{-1}$.

L'équation au champ moyen, donnée par l'\refeq{eq:sys-CEP-mfe}, peut
être réécrite en utilisant les nouvelles variables $x_0 = \beta m_0$,
$\gamma = GT^2$, et $\lambda = \beta\Lambda$ comme: 
\begin{equation}
  \label{eq:app-mfe-for-CEP-calc}
  0 = x_0 - x - 8 \gamma N_c N_f x 
  \left[ i_\beta(\sigma,x) - \lambda^2i_1 (x/\lambda) \right] \;.
\end{equation}
Si l'on introduit la variable $\eta = x / \lambda$, l'équation
précédente peut être écrite comme:
\begin{equation}
  8 \gamma N_c N_f \eta^2 i_1 (\eta^{-1}) = 
          8 \gamma N_c N_f \frac{i_\beta(\sigma,x)}{x^2} 
                        - \frac{x_0}{x^3} + x^{-2} \;.
\end{equation}
En utilisant la définition des nouvelles variables, l'on peut calculer: 
\begin{equation}
  \frac{1}{\gamma x^2} = 
                  \frac{\eta^2}{G\Lambda^2} 
                          \quad \textrm{et} \quad 
  \frac{x_0}{\gamma x^3} = 
                  \eta^3 \frac{m_0}{\Lambda} \frac{1}{G\Lambda^2} \;,
\end{equation}
et, en introduisant: 
\begin{equation}
  \label{eq:app-definition-of-coef-a-and-b}
  a = ( 8 G\Lambda^2 N_c N_f )^{-1} 
                 \quad \textrm{et} \quad 
  b = \frac {m_0} \Lambda \;,
\end{equation}
l'équation du gap devient:
\begin{equation}
  \label{eq:app-mfe-change-var}
  a \eta^2 ( b \eta -1) + \eta^2 i_1 ( \eta^{-1} ) = 
              \frac{i_\beta(\sigma , x )}{x^2} \;.
\end{equation}
Nous pouvons alors introduire la fonction $F(\eta)$ telle que:
\begin{equation}
  \label{eq:definition-F-eta}
  F(\eta) = a \eta^2 ( b \eta -1) + \eta^2 i_1 ( \eta^{-1} ) \;,
\end{equation}
et la fonction $Z(\sigma ,x)$ telle que:
\begin{equation}
  \label{eq:definition-Z-sigma-x}
  Z(\sigma ,x) =  \frac{i_\beta(\sigma , x )}{x^2} \;.
\end{equation}
L' \refeq{eq:app-mfe-change-var} s'écrit alors simplement:
\begin{equation}
  F(\eta) =   Z(\sigma ,x) \;.
\end{equation}

Comme l'intégrale donnée par l'\refeq{eq:app-definition-i-beta} est
calculée numériquement, calculer son inverse prend du temps. Au
contraire, l'intégrale $i_1(x)$ étant analytique, la fonction inverse
$F^{-1}(\eta)$ peut être calculée efficacement en utilisant un simple
algorithme de recherche de zéro. Lorsque $\sigma$ et $x$ sont fixés,
la solution $\eta$ de l'équation du gap est donnée par:
\begin{equation}
  \label{eq:app-inverse-of-F-eta-solution}
  \eta_{\textrm{MFE}}(\sigma, x) = F^{-1} \circ Z ( \sigma , x ) \;,
\end{equation}
où $\eta_{\textrm{MFE}}$ est le $\eta$ solution de l'équation ; solution
à partir de laquelle peuvent être calculés la masse, la température et
le potentiel chimique en utilisant les relations :
\begin{equation}
  m = \frac{\Lambda}{\eta_{\textrm{MFE}}} \;\; ; \quad 
  T = \frac{\Lambda}{x\eta_{\textrm{MFE}}} \;\; ; \quad 
  \mu = \frac{\Lambda \sigma}{x \eta_{\textrm{MFE}}} \;.
\end{equation}

Il peut paraître étonnant de travailler avec ces variables, mais
l'utilisation de ces variables est en fait équivalent à choisir une
trajectoire sur la surface définie par :
\begin{equation}
  g_m(m,T,\mu) = 0 \;.
\end{equation}
Par exemple, à $\sigma$ fixé, calculer $\eta(x)$ et ensuite
\mbox{$m(x) = \Lambda / \eta(x)$} est équivalent à calculer
$g_m(m,T,\mu) = 0$ sous la contrainte $\sigma = \textrm{cst} = \mu/T$.

La courbe paramétrique $\left\{m=m(x)\, , \, \mu = \Lambda\sigma /
[x\eta(x)]\right\}$ à $\sigma$ fixé est simplement la solution de: 
\begin{equation}
\label{eq:app-gap-parametric}
g_m \left( m(\mu) , T = \frac{\mu}{\sigma} , \mu\right) = 0 \;,
\end{equation}
\idest, la solution de l'équation du gap sur les lignes $T = \mu /
\sigma$ dans le plan $(T,\mu)$.

Comme le CEP est définie comme le point où la courbe de $m$ a une
dérivée par rapport aux paramètres thermodynamques $T$ et $\mu$, il
faut être prudent lorsque l'on cherche à calculer sa position. D'après
l'\refeq{eq:app-gap-parametric}, calculer les coordonnées du CEP est
equivalent à résoudre le système données par les équations
\refeq{eq:sys-CEP-mfe}, \refeq{eq:sys-CEP-derivative}, et
\refeq{eq:sys-CEP-inflexion} à $\sigma$ fixé. L'on peut remarquer que
travailler à $x$ fixé donnerait lieu à des équations plus simple, mais
il est possible de vérifier que la solution trouvée ne correspondrait
pas au CEP. En effet la trajectoire suivie sur la surface $S = \{
g_m(m,T,\mu) = 0 \}$ n'est pas triviale, et le lient entre cette
solution et CEP n'est pas évident.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Trouver la position du CEP}

En utilisant les précédentes notations, le sytème peut être écrit comme:
\begin{align}
  \label{eq:app-sys-CEP-new-MFE}
  F(\eta) &= Z(\sigma , x) \;, \\
  \label{eq:app-sys-CEP-new-derivative}
  \left. \frac{\dd \mu}{\dd m} \right|_{\sigma} &= 0 \;, \\
  \label{eq:app-sys-CEP-new-inflexion}
  \left. \frac{\dd^2 \mu}{\dd m^2} \right|_{\sigma} &= 0 \;,
\end{align}
où: 
\begin{align}
  \frac{\mu}{\Lambda} ( \sigma , x )  &= \frac{\sigma}{\eta x} \; , \\
  \frac{m}{\Lambda} ( \sigma , x ) &= \eta^{-1} \; .
\end{align}
À $\sigma$ fixé, on peut calculer la différentielle totale: 
\begin{align}
  \label{eq:app-d-mu-over-lambda}
  \dd \left( \frac{\mu}{\Lambda} \right)&= 
            - \frac{\sigma}{x\eta^2} \dd \eta 
            - \frac{\sigma}{\eta x^2} \dd x \; , \\
  \label{eq:app-d-m-over-lambda}
  \dd \left( \frac{m}{\Lambda} \right)  &= - \frac{\dd\eta}{\eta^2} \;. 
\end{align}
À partir de l'\refeq{eq:app-sys-CEP-new-MFE}, et en utilisant
l'\refeq{eq:app-d-mu-over-lambda} et l'\refeq{eq:app-d-m-over-lambda},
l'on trouve:
\begin{equation}
  \label{eq:app-sys-CEP-reex-der}
 \eta F'(\eta)   + x Z_x(\sigma,x) = 0  \;,
\end{equation}
où $F'$ est la dérivée de la fonction $F$ par rapport à $\eta$, et
$Z_x$ est la dérivée partielle de la fonction $Z$ par rapport à
$x$. De la même manière, nous pouvons écrire:
\begin{align}
  0 &= \left.\frac{\dd^2 \mu}{\dd m^2}\right|_\sigma \nonumber \\
    &= \frac{\dd}{\dd m} 
            \left[ - \frac{\sigma}{\eta x} 
                          \left( 
                                  \eta^{-1} + x^{-1} \frac{\dd x}{\dd \eta} 
                          \right) 
                  (-\eta^2)\right] \;.
\end{align}

Après quelques manipulations, et en utilisant
l'\refeq{eq:app-sys-CEP-reex-der}, l'on trouve:
\begin{equation}
\eta^2 F''(\eta) - x^2 Z_{xx}(\sigma,x) - 2x Z_{x}(\sigma,x) = 0\;,
\end{equation}
où  $Z_{xx}$ est la seconde dérivée partielle de la fonction $Z$ par rapport à $x$.

Le système à résoudre s'écrit alors comme:
\begin{align}
  \label{eq:app-sys-fin-CEP-1}
  F(\eta) - Z(\sigma , x) &= 0 \; ,  \\
  \label{eq:app-sys-fin-CEP-2}
  \eta F'(\eta)  + x Z_{x}(\sigma,x) &= 0 \;, \\
  \label{eq:app-sys-fin-CEP-3}
  \eta^2 F''(\eta) - x^2 Z_{xx}(\sigma,x) - 2x Z_{x}(\sigma,x) &= 0\;. 
\end{align}

Avec une initialisation correcte, en particulier si la valeur initiale
de $\eta$ est déjà solution de
l'\refeq{eq:app-inverse-of-F-eta-solution} pour les valeurs initiales
de $x$ et de $\sigma$, un simple algorithme de recherche de zéro
trouvera la solution en quelques millisecondes, soit environ cent fois
plus rapide que l'algorithme usuellement utilisé.

Suivant la valeur des paramètres $a$ et $b$, donnés par
l'\refeq{eq:app-definition-of-coef-a-and-b}, le CEP peut ne pas
exister. Dans ce cas, l'on ne doit pas essayer de résoudre le système
donné par les équations \refeq{eq:app-sys-fin-CEP-1},
\refeq{eq:app-sys-fin-CEP-2}, et \refeq{eq:app-sys-fin-CEP-3}.

Pour détecter si un CEP existe ou non, il suffit de regarder si une
solution métastable existe à température nulle, \idest, en résolvant
pour $m$ l'équation $\dd^2 \mu / \dd^2 m = 0$ et en calculant la
valeur de $\dd \mu / \dd m$ en ce point. Un CEP existe si $\dd \mu /
\dd m > 0$.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Calcul analytique de la sensibilité de la masse du méson sigma}

Pour calculer la sensibilité de la masse du méson sigma: 
\begin{equation}
  m_\sigma^2 = 4m^2 + m_\pi^2 \;,
\end{equation}
l'on peut utiliser le précédent changement de variable $m = \Lambda
x$. Ainsi: 
\begin{equation}
m_\sigma^2 = 4 \Lambda^2 x^2 + m_\pi^2 \;.
\end{equation}
Si $x$ est solution de l'\refeq{eq:app-decomp-sys-1}, alors $\Lambda$
est donné par l'\refeq{eq:app-decomp-sys-3}, et la masse du sigma peut
s'écrire comme:
\begin{equation}
m_\sigma = \sqrt{\frac{4}{N_c} \frac{f_\pi^2}{i_2} + m_\pi^2} \;.
\end{equation}

Avec $c^3 = -\qbarq$, il est possible d'écrire la différentielle suivante: 
\begin{equation}
 d i_2 = \frac{d i_2}{dx} dx
 = - \frac{d i_2}{dx} \frac{1}{G'_\alpha(x)} d (f_\pi^3 / c^3) .
 \end{equation}
Il est assez simple de calculer $\dd m_\sigma^2 = 2m_\sigma \dd
m_\sigma$ ainsi que les dérivées partielles nécessaires. On trouve: 

 \begin{align}
  \label{eq:dmp-ms}
  \frac{\partial m_\sigma}{\partial m_\pi} &=\frac{m_\pi}{m_\sigma}\;,\\
  \label{eq:dfp-ms}
  \frac{\partial m_\sigma}{\partial f_\pi} &= \frac{2 f_\pi}{m_\sigma
    N_c i_2} \left( 2 + \frac{3 \alpha}{i_2} \frac{\dd i_2}{\dd x}
  \frac{1}{G'_\alpha} \right)\;,\\
  \label{eq:dc-ms}
\frac{\partial m_\sigma}{\partial c} &= -6 \frac{f_\pi^2}{m_\sigma N_c
  i_2^2}\frac{\alpha}{c}\frac{\dd i_2}{\dd x} \frac{1}{G'_\alpha} \;.
\end{align}
Avec les valeurs des inputs données dans un précédent paragraphe, on trouve: 
\begin{equation}
\dd m_\sigma = 0.21 \dd m_\pi + 34 \dd f_\pi - 8.2 \dd c \;.
\end{equation}
Lorsque la dispersion relative est infinitésimale, on trouve alors: 
\begin{equation}
\Sigma(m_\sigma) = 6.42 \;.
\end{equation}
La valeur trouvée avec l'algorithme Monte-Carlo est $\Sigma(m_\sigma)
= 6.41$ (voir le \reftab{tab:sensitivity}) et est en bon accord.

La difficulté vient du fait que l'équation pour $x$ est
implicite. Dans la cas que nous étudions, toutes les quantités
apparaissant dans les équations définissant le problème inverse ne
dépendent que de la solution de l'\refeq{eq:app-decomp-sys-1} qui ne
dépend qur du ratio $\alpha = f_\pi^3 / c^3$. L'on peut alors avoir
accès aux sensibilités des prédictions dans le vide de manière assez
simple. Ce n'est pas la cas pour les prédictions dans le milieu, et ce
ne sera plus le cas lorsque la résolution du problème inverse ne sera
plus équivalente à la résolution d'une équation d'une inconnue. Dans
ces cas, l'utilisation de la méthode Monte-Carlo est une bonne
alternative.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Calcul analytique de la composition de distributions de probabilité}


Pour vérifier si les résultats obtenus avec la simulation Monte-Carlo
sont corrects, nous les comparons aux distributions de probabilité
théorique lorsqu'il est facilement possible de les calculer.

Dans le cas d'une seule variable, la composition de deux distributions
de probabilité se fait de la manière suivante. Soient $X$ et $Y$ deux
variables aléatoires, suivant respectivement les distributions de
probabilité $\rho_X$, et $Y = f(X)$, \idest, la variable aléatoire $Y$
est une fonction de la variable aléatoire $X$. Soient $x$ et $y$ les
réalisations des variables aléatoires $X$ et $Y$ suivant leur
distributions de probabilité.

Si la fonction $f$ est monotone croissante, la probabilité de trouver
$x$ entre $x_1$ et $x_2$ (avec $x_1 < x_2$) est égale à la probabilité
de trouver $y$ entre $y_1 = f(x_1)$ et $y_2 = f(x_2)$:
\begin{equation}
  \textrm{P} ( x_1 \leq x \leq x_2 ) = \textrm{P} \left( y_1 = f(x_1 ) \leq
  y \leq y_2 = f(x_2) \right) \;.
\end{equation}

Par définition de la distribution de probabilité, les égalités
suivantes sont vérifiées:
\begin{align}
  \textrm{P} ( x_1 \leq x \leq x_2 ) &= \int_{x_1}^{x_2} \rho_X(x) \dd x \; ; \\
  \textrm{P} ( y_1 \leq y \leq y_2 ) &= \int_{f(x_1)}^{f(x_2)} \rho_Y(x) \dd y \; .
\end{align}

L'on peut alors exprimer $\rho_X$ comme: 
\begin{equation}
  \rho_X ( x ) = \left( \rho_Y \circ f \right) (x) f'(x) \;,
\end{equation}
impliquant: 
\begin{equation}
\label{eq:app-rho-1v}
  \rho_Y ( y ) = 
           \left(\rho_X \circ f^{-1}\right) (y) 
           \left[ \left(f' \circ f^{-1}\right)(y) \right]^{-1} \;.
\end{equation}

Pour illustrer le cas de deux variables, nous donnons le résultat de
la distribution $\rho_\alpha$. En suivant la même procédure que dans
le cas d'une seule variables, et en utilisant la notation $f$ pour
$f_\pi$ et $c$ pour $\qbarq$, on trouve:
\begin{align}
  \textrm{P} ( \alpha_1 \leq \alpha  \leq \alpha_2 ) 
%
        &= \textrm{P} \left( \frac{f^3}{c} \in [\alpha_1 ,
    \alpha_2]\right) \nonumber \\
%
        &= \textrm{P} \left( \frac{f^3}{c} \geq \alpha_1 \;\;\wedge\;\;
  \frac{f^3}{c}\leq \alpha_2) \right) \nonumber \\
%
        &= \textrm{P} \left( f \in \mathbb{R}^+ \;\; \wedge \;\;
  \frac{f^3}{\alpha_2} \leq x \leq \frac{f^3}{\alpha_1} \right)
  \nonumber \\
%
        &= \int_{\mathbb{R}^+} \dd f
  \int_{f^3/\alpha_2}^{f^3/\alpha_1} \dd c \; \rho_f(f)\rho_c(c) \;.
\end{align}

Cette probabilité peut se réécrire en utilisant la distribution de
probabilité $\rho_\alpha$:
\begin{align}
  \textrm{P} ( \alpha_1 \leq \alpha  \leq \alpha_2 ) &= 
        \int_{\alpha_1}^{\alpha_2} \rho_\alpha(\alpha) \dd \alpha \;,
\end{align}
nous permmetant d'obtenir une expression pour $\rho_\alpha$:
\begin{equation}
  \rho_\alpha(\alpha) = \frac{\dd}{\dd \alpha} \int_{\mathbb{R}^+} \dd
  f \int_{f^3/\alpha}^{f^3/\alpha^*} \dd c \; \rho_f(f)\rho_c(c) \:,
\end{equation}
où  $\alpha^*$ est une constante. Finalement, $\rho_\alpha$ peut s'écrire: 
\begin{equation}
  \label{eq:app-rho-alpha-theo}
  \rho_\alpha(\alpha) = \frac{1}{\alpha^2} \int_{\mathbb{R}^+} \dd f
  \; f^3 \rho_f(f)\rho_c(f^3/\alpha) \:.
\end{equation}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Kernel density approximation}


Lorsqu'un CEP a été calculé pour toutes les configurations de
paramètres solution du problème inverse, l'on a accès à une
distribution de CEP. Cette distribution peut être représentée
graphiquement, voir la \reffig{fig:CEP-dist}. Cependant, il est
également intéressant d'avoir accès à la densité de points dans le
plan $(T,\mu)$, et ainsi avoir accés à la distribution de probabilité
de CEP. Il est possible de construire une densité à partir de données
en utilisant la technique de \emph{Kernel Density approximation} (KDE)
utilisant des kernels gaussiens.

Suivant les équations (4), (5), (6) et (7) de la référence
\getpaperref{52}, l'on peut reconstruire la densité $\rho(T_{CEP},
\mu_{CEP})$. Cet algorithme normalise la densité ($\int \rho(T,\mu)\,
dT\, d\mu = 1$) de façon à obtenir une distribution de probabilité
dont la dimension est dans notre cas des GeV$^{-2}$. 

Pour ce faire, il faut d'abord calculer la matrice de covariance $S$
et transformer les données de manière à obtenir un ensemble de données
de moyenne nulle, et de déviation standard unitaire. Cette
transformation se fait en applicant aux données la racine de la
matrice de covariance $S^{1/2}$.

Ensuite, chaque point transformé est remplacé par une gaussienne de
variance choisie telle que la sa déviation standard soit suffisement
grande pour chevaucher les autres points, mais suffisement petite pour
ne pas créer de longues queues qui n'existe pas dans les
données. Cette procédure de lissage est contrôlée par le paramètre $h$
donné dans le papier \getpaperref{52}. Nous avons vérifié qu'avec ce
paramètre, nous étions capable de reconstruire correctement une
distribution gaussienne à deux dimensions avec seulement une centaine
de point (ce qui est le cas simple dans le cas où les kernels utilisés
sont gaussiens), et aussi de reconstuire une distribution uniforme à
deux dimension relativement bien avec une centaine de points.

Enfin, la densité reconstruite est la somme des données lissées sur
lesquelles la matrice $S^{-1/2}$ est appliquée pour retrouver les
données originale.


\end{document}


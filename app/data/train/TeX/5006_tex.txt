\chapter{State of the Art}

In this section, the background to the state of the art in machine learning and recommendation systems will be presented.
In particular, the use of machine learning algorithms as the content filtering component of recommendation systems will be discussed.
The different approaches to machine learning and recommendation systems will also be introduced and then discussed in detail.

\section{Background}
In 1947, Alan Turing presented a lecture to the London Mathematical Society in which he theorised that it would be possible for a machine to learn from it's experiences.
In Turing's example, he proposed that learning was a prerequisite for a true intelligent system \cite{Turing1946}.
Turing further expanded on the concept of an intelligent machine in his 1950 paper in which he proposed a theoretical test, called the "imitation game", to identify whether machines could be considered intelligent \cite{Turing1950}.
This test has since become known as the Turing test.

Machine learning and recommendation systems can be viewed as semi-intelligent machines which try to learn from data fed into them\cite{mlTutorial}.
These machines are replacements for human operators who would have had to classify data or suggest recommendations in the past.
One of the main reasons machines are used, instead of humans for this task is largely due to the volume of data available\cite{lops}.
Another factor to consider is the quality of results obtained from the system.
Humans, for example, can possibly be biased in their opinion when presenting their results or findings.
A machine on the other hand should generate it's results using the model it has created using only the data that has been presented to it.
This approach is less likely to be skewed by a bias when the model has been created using a balanced dataset\cite{FProvost2000}.

\section{Machine Learning}
Machine learning is an interdisciplinary field concerned with the study of self learning systems.
Machine learning has applications in fields such as: statistics, mathematics, computer vision, game theory, information retrieval, software engineering, sentiment analysis, and artificial intelligence\cite{mlTutorial}.

Machine learning algorithms try to build a model of a dataset by learning the patterns in the data.
The resulting model created by many machine learning algorithms can then output seemingly intelligent results for data that it has never seen before.

Machine learning can be split into three different types of learning:
\begin{itemize}
    \item Supervised learning
    \item Reinforced learning
    \item Unsupervised learning
\end{itemize}

\subsection{Supervised learning}
Supervised learning is a machine learning technique which tries to create a model which maps inputs to desired outputs.
This can be represented as a function f(x) which maps an input \(x_i\) to an output \(y_i\).
This is achieved by using a training set T = \((x_i, y_i)\) and a learning algorithm.
The learning algorithm produces a function \(\hat{f}(x_i)\) which can then be modified in response to the difference between \(y_i - \hat{f}(x_i)\)\cite{mlTutorial}.

\subsection{Reinforced learning}
Reinforced learning is a machine learning technique where the machine interacts with an environment and produces actions \(a_i\).
This set of actions interact with the environment which in turn results in the machine receiving rewards \(r_i\) from a rewards function.
The machine tries to learn how to create actions which maximizes the future return on rewards\cite{mlTutorial}.

\subsection{Unsupervised learning}
Unsupervised learning is a machine learning technique which tries to find hidden patterns in data.
An unsupervised learning algorithm receives inputs \(x_i\) but receives no desired outputs nor rewards.
The machine tries to build a probabilistic model of the data or it uses clustering to partition the data in categories\cite{mlTutorial}.

The main difference between unsupervised techniques and other techniques is the lack of a clear measure of success.
This poses a problem when comparing the accuracy of different unsupervised techniques.
The effectiveness of unsupervised techniques therefore relies heavily on heuristic approaches when judging their quality\cite{elementsStat}.

The research conducted in this project is focused on the unsupervised approach to machine learning.
Due to the fact that there is no gold standard to compare the quality of the similarity results, their effectiveness will be a matter of opinion.
Where possible the results will also be compared against any possible meta-data to help judge their effectiveness.

\section{Recomendation Systems}
Recommendation systems are algorithms and techniques which try to generate recommendations for users.
The design of recommendation systems is an interdisciplinary field which touches upon information retrieval, human computer interaction, machine learning, data mining, etc.
These systems often try to generate recommendations based on similarities between users or between content.
There are two main approaches used in recommendation systems\cite{recHandbookIntro}:

\begin{itemize}
    \item Collaborative-filtering
    \item Content-filtering
\end{itemize}

\subsection{Collaborative-filtering}
Collaborative-filtering is an approach used by recommendation systems which tries to generate recommendations by comparing user data with other users.
The rationale behind this approach is to find similarities and differences  between different groups of users and build a user model.
Once users have been grouped together, recommendations can be generated based on what similar users have liked and disliked.
This type of recommendation system has been used extensively by Amazon when generating recommendations\cite{recHandbookIntro}.

The main problem with this approach is that it requires a huge amount of initial data in order to generate appropriate recommendations.

\subsection{Content-filtering}
Content-filtering is an approach used by recommendation systems which learns to generate recommendations based on a users previous interactions with an item.
This approach tries to learn the features of items in order to generate recommendations for items with similar features\cite{recHandbookIntro}.

Unlike collaborative-filtering a huge amount of user data is not required for content-filtering.
The recommendations for this approach can be generated by building a model using the individual item features.

The research conducted in this project is focused entirely on the information filtering component of a content-filtering recommendation system.
The research project conducted was based on a potential user being a student.
However all of the algorithms investigated could be used in content-filtering recommendation systems to filter recommendations based on the content of the items.

\section{State of the Art}
This research project is investigating the performance of Latent Dirichlet Allocation (LDA), k-Nearest-Neighbour (k-NN) and Word2Vec as the content-filtering component of a recommendation system.
Each of these algorithms have applications in information retrieval and are not just confined to the field of recommendations systems nor to an educational corpus.

\subsection{Latent Dirichlet Allocation}
LDA is a probabilistic model for any discrete data. In this research project and in most of the literature reviewed, textual data has been used.
LDA was introduced in 2004 by Blei, Ng and Jordan as an improvement on Deerwester's Latent Semantic Indexing (LSI) and on Hofmann's later Probabilistic Latent Semantic Indexing (pLSI).
LDA was created in response to two problems identified with pLSI; the number of parameters in the model grew linearly with the size of the corpus, and it was unclear as to how to assign a probability to a document that the model had never seen before\cite{LDAintro}.

LDA is hierarchical model which assumes that documents contain a mixture of topics and that topics are a mixture of word probabilities.
LDA is also a bag-of-words (bow) model which assumes that the topics are generated first and then documents are generated from these topics.
These assumptions are used when inferring topics as it is a reversing of the generation process\cite{LDAintro}.

LDA has been used in the past on a corpus of 17,000 articles from the magazine Science, with 100 topics.
The LDA model that was generated using this corpus was fed an unseen article on genome mapping and sequencing.
The distribution of topics in the article were then calculated and graphed.
It was found that the topics which seemed to be about; genetics, evolution, disease, and computers had a high concentration in the article\cite{ACMTopicModel}.

In a similar study to the above, LDA was applied to 21,434 articles from Science to create 50 topics.
The purpose of this experiment was to infer the most relevant topics from a document and then to find the most similar documents.
An article on Statistical Significance in Protein and DNA sequencing was used to generate recommendations for articles related to protein sequencing, genome sequencing and sampling strategies.

This research project used the LDA implementation from the Gensim Python library.
Gensim is a library which was originally created in 2008 to find similar articles in the Czech Digital Mathematics Library.
Gensim was chosen as it allows the creation of an LDA model which is memory independent of the size of the input corpus.
As the original intention for Gensim was to generate recommendations for academic articles, this influenced the choice of the library.\cite{rehurek_lrec}


\subsection{k-Nearest-Neighbours}
k-Nearest-Neighbours is a model free algorithm for classification and pattern recognition.
In k-Nearest-Neighbour classifiers when given a query point \textit{x}, the \textit{k} nearest points in distance to \textit{x} are used to classify \textit{x}.
When \textit{k} is 1, the query point \textit{x} is classified as being the same as the point nearest to it.
When \textit{k} is a value greater than 1 a simple majority vote of the \textit{k} nearest points can be used to classify \textit{x}.\cite{elementsStat}

When k-Nearest-Neighbours is applied to high-dimensional feature space, the distance between the k-nearest-neighbours can be quite high, causing bias and a degradation of performance.\cite{elementsStat}
An approach to try and overcome the distance problem is to weight each of the k nearest neighbours vote based on their distance to the query point \textit{x}.\cite{top10datamining}

Despite the simplicity of the approach used by k-NN it has been featured as one of the top algorithms to use in data-mining.\cite{top10datamining}
For this research project k-NN was not used to classify documents into predefined classes.
Instead k-NN was used to find the k-nearest documents to a query document.
This clustering of documents is based on the assumption that the k-nearest documents in feature space will be similar to the query document.

\subsection{Word2Vec}
Word2Vec is an algorithm introduced in 2013 by Mikolov which uses a simple neural network to learn continuous vector representations of words.
These vector representations of words can be used to describe the similarity between words.
Words which are spatially close in this representation can described as being semantically close, while words which are spatially distant can be described as being semantically different.
We also have the benefit of being able to apply algebraic operations to these  word vectors.
This was shown intuitively by Mikolov as: \textit{King - Man + Woman” = Queen} and by \textit{Madrid - Spain + France = Paris}.

By training the word vectors using a neural network we are able to learn the multiple degrees of similarity between words\cite{Mikolov1}.
The result is a vector representation which can encode linguistic patterns and regularities.\cite{Mikolov2}
A small problem with Mikolov's original word2vec paper was that the algorithm created a word based model.
The algorithm was later extended by Mikolov to allow word and phrase based models.\cite{Mikolov2}

The research project conducted used the word2vec implementation from the Gensim Python library.
The Gensim library contains a word2vec and a doc2vec implementation. The word2vec implementation follows Mikolov's original word base model while the doc2vec implementation is based on Mikolov's later phrase base model.\cite{radimDoc2Vec}
The later doc2vec implementation is the one which is being investigated by this project.


\section{Conclusion}
The field of machine learning and recommendation systems is vast and the number of techniques and approaches which could be used is equally as vast.
From the research conducted the scope of this project has been narrowed down to focus on the performance of three relevant algorithms: LDA, k-NN, and word2vec.
These three algorithms are quite different in their implementations and have different applications outside the scope of this project.
It is hoped that by focusing the research on these algorithms, it will be possible to successfully analyse the performance of some of the best algorithms, to use for the content-filtering component of a recommendation system.

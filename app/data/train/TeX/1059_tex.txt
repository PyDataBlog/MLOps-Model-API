\input{header_beamer}
\usepackage{etex}
%\include{macros}
%\documentclass[usenames,dvipsnames]{beamer}
%\usepackage{beamerthemesplit}
%\usepackage{graphics}
%\usepackage{amsmath}
%\usepackage{rotating}
%\usepackage{array}
%\usepackage{nth}
\usepackage{xcolor}
\usepackage{textcomp}
\input{matlab_setup}

\usepackage{tabularx}
\usepackage{picins}
\usepackage{tikz}

\usetikzlibrary{shapes.geometric,arrows,chains,matrix,positioning,scopes,calc}
\tikzstyle{mybox} = [draw=white, rectangle]

\definecolor{camlightblue}{rgb}{0.601 , 0.8, 1}
\definecolor{camdarkblue}{rgb}{0, 0.203, 0.402}
\definecolor{camred}{rgb}{1, 0.203, 0}
\definecolor{camyellow}{rgb}{1, 0.8, 0}
\definecolor{lightblue}{rgb}{0, 0, 0.80}
\definecolor{white}{rgb}{1, 1, 1}
\definecolor{whiteblue}{rgb}{0.80, 0.80, 1}

\newcolumntype{x}[1]{>{\centering\arraybackslash\hspace{0pt}}m{#1}}
\newcommand{\tabbox}[1]{#1}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Some look and feel definitions
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\setlength{\columnsep}{0.03\textwidth}
\setlength{\columnseprule}{0.0018\textwidth}
\setlength{\parindent}{0.0cm}

%\include{macros}
\usepackage{preamble}
\hypersetup{colorlinks=true,citecolor=blue}
%\pdfmapfile{+sansmathaccent.map}

\title{Automated Model Construction through Compositional Grammars
\\
%\small with applications to nonparametric regression and image modeling
}

\author{
\includegraphics[width=0.7\textwidth]{matrix_figures/modeltree}
\\
David Duvenaud
}
\institute{
%\includegraphics[width=0.4\textwidth]{figures/spiral_main}
\\Cambridge University
\\Computational and Biological Learning Lab}
%\date{}


\begin{document}

\frame[plain] {
\titlepage
}

\setbeamercolor{toc}{fg=black}

\frame[plain] {
\frametitle{Outline}
%\tableofcontents

\begin{itemize} 
	\item Motivation
	\item Automated structure discovery in regression
	\begin{itemize} 
		\item Gaussian process regression
		\item Structures expressible through kernel composition
		\item A massive missing piece
		\item grammar \& search over models
		\item Examples of structures discovered
	\end{itemize}
	\item Automated structure discovery in matrix models
	\begin{itemize} 
		\item expressing models as matrix decompositions
		\item grammar \& special cases
		\item examples of structures discovered on images
	\end{itemize}
\end{itemize}   

}


\frame[plain]{
\frametitle{Credit where credit is due}

Talk based on two papers:
	\begin{itemize}
		\item Structure Discovery in Nonparametric Regression through Compositional Kernel Search [ICML 2013]
		\\
		{David Duvenaud, James Robert Lloyd, Roger Grosse, Joshua B. Tenenbaum, Zoubin Ghahramani}
		\item Exploiting compositionality to explore a large space of model structures [UAI 2012]
		\\
		Roger B. Grosse, Ruslan Salakhutdinov, \\William T. Freeman, Joshua B. Tenenbaum
	\end{itemize}
}


\frame[plain]{
\frametitle{Motivation}
	\begin{itemize} 
	\item Models today built by hand, or chosen from a fixed set.
	\begin{itemize} 
		\item Fixed set sometimes not that rich
		\begin{itemize} 
			\item Just being nonparametric sometimes isn't good enough 
			\item to learn efficiently, need to have a rich prior that can express most of the structure in your data.
		\end{itemize}
		\item Building by hand requires expertise, understanding of the dataset.
		
		\item Follows cycle of: propose model, do inference, check model fit
		\begin{itemize} 
			\item Propose new model
			\item Do inference
			\item Check model fit
		\end{itemize}
	\end{itemize}
	\item Andrew Gelman asks:  How would an AI do statistics?
	\item It would need a language for describing arbitrarily complicated models, a way to search over those models, nad a way of checking model fit.
\end{itemize}
}



\frame[plain]{
\frametitle{Finding Structure in GP Regression}
\begin{center}
  \input{figures/fig_mauna2.tex}
\end{center}
}

\frame[plain]{
\frametitle{Gaussian Process Regression}
Assume $\vX, \vy$ is generated by $\vy = \vf(\vX) + \epsilon_\sigma$

A GP prior over $\vf$ means that, for any finite set of points $\vX$,

\begin{equation*}
p( \vf(\vx) ) = \mathcal{N}( \mu(\vX), K(\vX, \vX))
\end{equation*}

where
\begin{equation*}
K_{ij} = k( \vX_i, \vX_j ) 
\end{equation*}
is the \textit{covariance function} or \textit{kernel}.  $k( x, x' ) = \cov\left[f(x), f(x')\right]$

\pause
\vspace{0.5cm}
Typically, kernel says that nearby $x_1, x_2$ will have highly correlated function values $f(x_1), f(x_2)$:
%

\begin{columns}
\begin{column}[r]{2cm}
\begin{equation*}
k_{\SE}( x, x' ) = \exp ( -\frac{1}{2\theta} | x - x' |_2^2 )
\end{equation*}
\end{column}
\begin{column}[l]{2cm}
\includegraphics[width=3cm]{figures/kernel}
\end{column}
\end{columns}
%

}

\frame[plain]{
\frametitle{Sampling from a GP}

\hspace{-1cm}
\begin{columns}
\begin{column}[r]{6cm}
\lstset{language=Matlab} 
\lstset{columns=fullflexible}

\lstinputlisting{simple_gp_sample.m} 
\end{column}
\begin{column}[r]{4cm}
    \begin{figure}
    	\hspace{-1cm}
        \includegraphics<1>[width=4cm]{figures/simple_draw4}    	
        \includegraphics<2>[width=4cm]{figures/simple_draw2}
        \includegraphics<3>[width=4cm]{figures/simple_draw3}
        \includegraphics<4>[width=4cm]{figures/simple_draw}                
    \end{figure}                            
\end{column}
\end{columns}
}

\frame[plain]{
\frametitle{Sampling from a GP}

\hspace{-1cm}
\begin{columns}
\begin{column}[r]{6cm}
\lstset{language=Matlab} 
\lstset{columns=fullflexible}

\lstinputlisting{sin_gp_sample.m} 
\end{column}
\begin{column}[r]{4cm}
    \begin{figure}
    	\hspace{-1cm}
        \includegraphics<1>[width=4cm]{figures/sin_draw}
    \end{figure}                            
\end{column}
\end{columns}
}


\frame[plain]{
\frametitle{Conditional Posterior}

\begin{align*}
f(x^*) | \vX, \vy \sim \mathcal{N}( & k( x^*, \vX ) K^{-1} \vy, \\
& k( x^*, x^* ) - k( x^*, \vX ) K^{-1} k( \vX, x^* ) )
\end{align*}


With SE kernel:
    \begin{figure}
        \includegraphics<1>[width=6cm]{figures/gp_demo/1d_posterior_and_0_data}
        \includegraphics<2>[width=6cm]{figures/gp_demo/1d_posterior_and_1_data}
        \includegraphics<3>[width=6cm]{figures/gp_demo/1d_posterior_and_2_data}
        \includegraphics<4>[width=6cm]{figures/gp_demo/1d_posterior_and_3_data}
        \includegraphics<5>[width=6cm]{figures/gp_demo/1d_posterior_and_4_data}
    \end{figure}
}


\frame[plain]{
\frametitle{Kernel Choice is Important}
\begin{itemize} 
	\item Kernel determines almost all the properties of the prior.
	\item Many different kinds, with very different properties:
\input{tables/simple_kernels_table_v3}
\end{itemize}
}


\frame[plain]{
\frametitle{Kernels can be composed}
\begin{itemize} 
	\item Two main operations: adding, multiplying
	\input{tables/comp1}
\end{itemize}
}

\frame[plain]{
\frametitle{Kernels can be composed}
\begin{itemize} 
	\item Can be composed across multiple dimensions
	\input{tables/comp2}
\end{itemize}
}



\frame[plain]{
\frametitle{Special Cases}
\begin{center}
  \begin{tabular}{l|l}
  Bayesian linear regression & $\Lin$ \\
  %Bayesian quadratric regression & $\Lin \times \Lin$ \\
  Bayesian polynomial regression & $\Lin \times \Lin \times \ldots$\\
  Generalized Fourier decomposition & $\Per + \Per + \ldots$ \\
  Generalized additive models & $\sum_{d=1}^D \SE_d$ \\
  Automatic relevance determination & $\prod_{d=1}^D \SE_d$ \\
  Linear trend with deviations & $\Lin + \SE$ \\
  Linearly growing amplitude & $\Lin \times \SE$
  \end{tabular}
\end{center}
}


\frame[plain]{
\frametitle{Appropriate kernels are necessary for extrapolation}
\begin{itemize} 
	\item SE kernel $\rightarrow$ basic smoothing.
	\item Richer kernels means richer structure can be captured.
\end{itemize}
\begin{center}
  \input{figures/fig_synth_extrap.tex}
\end{center}
}


\frame[plain]{
\frametitle{Kernels are hard to choose}
\begin{itemize}
	\item Given the diversity of priors available, how to choose one?
	\item Standard GP software packages include many base kernels and means to combine them, but \emph{no default kernel}
	\item Software can't choose model for you, you're the expert (?)
\end{itemize}
}

\frame[plain]{
\frametitle{Kernels are hard to construct}
\begin{itemize}
	\item Carl devotes 4 pages of his book to constructing a custom kernel for CO2 data
	\item requires specialized knowledge, trial and error, and a dataset small and low-dimensional enough that a human can interpret it.
	\item In practice, most users can't or won't make custom kernel, and SE kernel became \emph{de facto} standard kernel through inertia.
\end{itemize}
}


\frame[plain]{
\frametitle{Recap}
\begin{itemize}
	\item GP Regression is a powerful tool
	\item Kernel choice allows for rich structure to be captured - different kernels express very different model classes
	\item Composition generates a rich space of models
	\item Hard \& slow to search by hand
	\item Can kernel specification be automated?
\end{itemize}
}



\frame[plain]{
\frametitle{Compositional Structure Search}
\begin{itemize}
	\item Define grammar over kernels:
	\begin{itemize}
		\item $ K \rightarrow K + K$ 
		\item $ K \rightarrow K \times K$ 
		\item $ K \rightarrow \{ \SE, \RQ, \Lin, \Per \}$
	\end{itemize}
	\item Search the space of kernels greedily by applying production rules, checking model fit (approximate marginal likelihood).
\end{itemize}
}

\frame[plain]{
\frametitle{Compositional Structure Search}
\begin{center}
\input{figures/fig_search_tree}
\end{center}
}





\frame[plain]{
\frametitle{Example Search: Mauna Lua CO$_2$}
\begin{center}
  \input{figures/fig_mauna_growth.tex}
\end{center}
}




\frame[plain]{
\frametitle{Example Decomposition: Mauna Loa CO$_2$}
\begin{center}
  \input{figures/fig_mauna2.tex}
\end{center}
}

\frame[plain]{
\frametitle{Compound kernels are interpretable}
Suppose functions ${f_1, f_2}$ are draw from independent \gp{} priors, ${f_1 \sim \GP(\mu_1, k_1)}$, ${f_2 \sim \GP(\mu_2, k_2)}$.
Then it follows that $${f := f_1 + f_2 \sim \GP(\mu_1 + \mu_2, k_1 + k_2)}$$
Sum of kernels is equivalent to sum of functions.
Distributivity means we can write compound kernels as sums of products of base kernels:
$${\SE \times (\RQ + \Lin) = \SE \times \RQ + \SE \times \Lin}.$$
}

\frame[plain]{
\frametitle{Example Decomposition: Airline }
\begin{center}
  \input{figures/fig_airline.tex}
\end{center}
}

\frame[plain]{
\frametitle{Example: Sunspots}
\begin{center}
	\includegraphics[width=0.8\textwidth]{figures/solar}
\end{center}
}

\frame[plain]{
\frametitle{Changepoint Kernel}
Can express change in covariance:
\begin{center}
\only<1>{
	\includegraphics[width=0.5\textwidth]{figures/periodic_change_to_se_cov_1}
	\includegraphics[width=0.5\textwidth]{figures/periodic_change_to_se_draw_1}
	\\
	Periodic changing to SE
	}
\only<2>{	
	\includegraphics[width=0.5\textwidth]{figures/lin_change_to_se_cov_2}
	\includegraphics[width=0.5\textwidth]{figures/lin_change_to_se_draw_2}
	\\
	SE changing to linear
	}
\end{center}
}




\frame[plain]{
\frametitle{Summary}
\begin{itemize}
	\item Choosing form of kernel is currently done by hand.
	\item Compositions of kernels lead to more interesting priors on functions than typically considered.
	\item A simple grammar specifies all such compositions, and can be searched over automatically.
	\item Composite kernels lead to interpretable decompositions.
\end{itemize}
}



\frame[plain]{
\frametitle{Grammars for Matrix decompositions}

Previous work introduced idea of grammar of compositions:
	\begin{itemize}
		\item Exploiting compositionality to explore a large space of model structures [UAI 2012]
		\\
		Roger B. Grosse, Ruslan Salakhutdinov, \\William T. Freeman, Joshua B. Tenenbaum
		\item Slides that follow are from Roger Grosse
	\end{itemize}
}


\frame[plain]{
\frametitle{Matrix decomposition}
\begin{center}
	\includegraphics[width=0.8\textwidth]{matrix_figures/input}
\end{center}
}


\frame[plain]{
\frametitle{Recursive Matrix decomposition}

\begin{center}
	\includegraphics[width=0.9\textwidth]{matrix_figures/fig1}
\end{center}

\begin{itemize}
	\item Main idea:  Matrices can be recursively decomposed
	\item Example: Co-clustering by clustering cluster assignments.
\end{itemize}
}

\frame[plain]{
\frametitle{Building Blocks}
\begin{center}
	\includegraphics[width=1.05\textwidth]{matrix_figures/buildingblocks}
\end{center}
}

\frame[plain]{
\frametitle{Matrix decomposition: Grammar}

\begin{center}
	\includegraphics[width=1.05\textwidth]{matrix_figures/grammar}
\end{center}
}

\frame[plain]{
\frametitle{Matrix decomposition: Special cases}

\begin{center}
	\includegraphics[width=1.1\textwidth]{matrix_figures/modeltree}
\end{center}
}


\frame[plain]{
\frametitle{Evolution of Image Models}
\begin{center}
	\includegraphics[width=1.05\textwidth]{matrix_figures/cartoon}
\end{center}
}


\frame[plain]{
\frametitle{Application to Natural Image Patches}
\begin{center}
	\includegraphics[width=1.05\textwidth]{matrix_figures/images}
\end{center}
}




\frame[plain]{
\frametitle{Conclusions}

	\begin{itemize}
		\item Model-building is currently done mostly by hand.
		\item Grammars over composite structures are a simple way to specify open-ended model classes.
		\item Composite structures often imply interpretable decompositions of the data.
		\item Searching over these model classes is a step towards automating statistical analysis.
	\end{itemize}
	
	\pause
	\centering
	{
		\hfill
		Thanks!
				\hfill
	}
}


\frame[plain]{
\frametitle{Related Work}
\begin{center}
	\includegraphics[width=0.9\textwidth]{matrix_figures/related}
\end{center}
}


\frame[plain]{
\frametitle{Extrapolation}
\begin{center}
	\includegraphics[width=0.7\textwidth]{matrix_figures/extrap}
\end{center}
}

\frame[plain]{
\frametitle{Multi-D Interpolation}
\begin{center}
	\includegraphics[width=0.9\textwidth]{figures/numbers}
\end{center}
}



\frame[plain]{
\frametitle{Grammars for Matrix decompositions}

\begin{center}
	\only<1>{\includegraphics[width=0.9\textwidth]{matrix_figures/gm}}
	\only<2>{\includegraphics[width=0.9\textwidth]{matrix_figures/bc}}
\end{center}
}

\end{document}
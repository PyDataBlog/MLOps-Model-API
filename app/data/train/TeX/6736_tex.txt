The most famous neural network is you. Or, in other words, the human brain.\\
It is, simply put, a clever arrangement of smallest units capable of processing easy logic.\\
These smallest units are called \textbf{neurons}, and our brain consists of approximately 100 billion of them.\\
They are interweaved through a complex series of incomming and outgoing extensions called dendrites and axons, respectively, of which some transport electricity faster than others. Most of the components of a brain are unfortunately still not understood well enough to be used productively in computer science.

An \textbf{artificial neural network} (ANN) tries to emulate the immense success of its biological counterpart by abstracting the complex chemical reactions responsible for our thoughts to much more graspable math.\\
The feedforward version of such an ANN consists of two simple components: neurons and connections.

Each neuron has inputs, which are the incoming connections. It applies a simple mathematical operation to this set of inputs and returns the result.

Connections connect neurons to each other. Each connection has a weight, which determines how weak or strong the connection is.

The neurons are typically organized into layers. The first is referred to as the input layer and the last one as the output layer. The remaining layers are called hidden layers. \cite{Anderson1995}

Here is a basic example of a neural network:

{\centering
	\begin{neuralnetwork}[height=3, nodespacing=1.5cm]
		\newcommand{\nodelabel}[2]{
			\ifnum#1=0 $x_#2$ \fi
			\ifnum#1=1 $y_#2$ \fi
			\ifnum#1=2 $z_#2$ \fi
		}
		\setdefaultnodetext{\nodelabel}
		\inputlayer[count=2, bias=false, title=Input]
		\hiddenlayer[count=3, bias=false, title=Hidden] \linklayers
		\outputlayer[count=1, title=Output] \linklayers
	\end{neuralnetwork}
\par}

Each connection is represented as an arrow and has an associated weight. Every neuron is connected to all neurons in the previous and in the next layer.

The configuration of how all neurons and layers are interconnected, as well as the number of layers, is called the \textbf{topology} of the network.\cite{Anderson1995}


For simple networks, you can also write down the inputs and the corresponding outputs.

\[
	\begin{vmatrix} 0 & 0 \\ 0 & 1 \\ 1 & 0 \\ 1 & 1 \end{vmatrix}
	\rightarrow
	\begin{vmatrix} 0.03 \\ 0.76 \\ 0.87 \\ 0.10 \end{vmatrix}	
\]
This network was trained to solve the XOR problem, which can be simplified as "are my inputs different?".\\
We defined the output to represent \emph{yes} if its $>= 0.5$ and \emph{no} otherwise.

It's also possible for a neural network to have multiple outputs.\\
We will use the assumption that our network has one output per possible answer for the rest of the documentation.\\
Example: We have a picture of a flower. It can either be a poppy, a lilly or a dandelion. Our neural network looking at the flower would have three outputs.

In this case we still wish to have one definitive output.
For this we'll use the \textbf{softmax} function, which squashes all our outputs in matter that lets them add up to exactly one. One can think of it as a normalization that represents confidence.\cite{Anderson1995}\\
It is defined as follows: 
$$\sigma(\mathbf{z})_j = \frac{e^{z_j}}{\sum_{k=1}^K e^{z_k}}$$ 

for j = 1, \textellipsis{}, K.

Example: If we our outputs are $[1,2,3,4,1,2,3]$, the softmax of that is $[0.024, 0.064, 0.175, 0.475, 0.024, 0.064, 0.175]$

We then simply take the highest one as our main output. This is called the \emph{winner takes all principle} and is modeled after how the brain works \cite{Anderson1995}\\


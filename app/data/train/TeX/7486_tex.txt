\section{Experiments}

We have measured the overhead of the PCAST OpenACC autocompare implementation to demonstrate its usability.
We used ten of the SPEC ACCEL v1.2 benchmarks, using the \emph{test} dataset.
In each case, the program has an outer time step loop containing the main computation.
The times shown are in seconds, and these are officially SPEC \emph{estimates}, since they were not run in the SPEC harness.
%The host machine was a 6-core Intel Haswell (core i7-5820K) with a 3.30GHz clock, with an NVIDIA Tesla Kepler K40c GPU.
The host machine was a dual socket 16-core Intel Haswell (E5-2698 Xeon, 32-cores total) with a 2.30GHz clock, with an NVIDIA Tesla Pascal P100 GPU.
We used the default autocompare options, but set a relative tolerance.
The execution times are in seconds, measured with /usr/bin/time. The values shown in Table~\ref{res1} are:
%The values shown in Table~\ref{res1} and Figure~\ref{fig:sle_figure} are:

%\begin{table}
%\begin{center}
%\begin{tabular}{rrrrl}
%\hline
% 303.ostencil & 304.olbm & 363.swim & \\
%\hline
% 3.40s &  3.27s & 1.80s & CPU time (sequential) \\
% 3.27s &  2.79s & 1.72s & GPU time \\
% 8.80s &  3.97s & 1.88s & redundant execution on CPU and GPU \\
% 1.80s &  1.69s & 0.09s & CPU to GPU data copy time \\
%64.38s & 23.95s & 2.80s & autocompare time \\
%202 & 61 & 258 & variables and arrays compared \\
%3,388,997,632 & 1,586,800,000 & 67,897,602 & values compared \\
%\hline
%\end{tabular}
%\end{center}
%\caption{Results showing overhead of OpenACC autocompare.}
%\label{res1}
%\end{table}

\begin{table}
\begin{center}
\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|}
\hline

Benchmark & \parbox[c]{1.9 cm}{\centering CPU time \\(sequential)} &  \parbox[c]{1.1 cm}{\centering GPU \\ time} & \parbox[c]{3.5 cm}{\centering Redundant execution \\ on CPU and GPU} & \parbox[c]{2.75 cm}{\centering CPU to GPU \\  data copy time} &  \parbox[c]{2 cm}{\centering Autocompare \\   time} \\


\hline
\hline
%303.ostencil  & 3.40s & 3.25s & 4.34s & 1.14s & 17.58s \\
%304.olbm      & 2.44s & 1.54s & 3.20s & 0.96s & 22.32s \\
%363.swim      & 1.64s & 1.13s & 1.15s & 0.04s & 2.06s \\

ostencil  & 3.51s & 1.82s & 4.22s & 1.03s & 17.19s\\
\hline
olbm      & 2.19s & 1.30s & 3.03s & 0.96s & 19.09s \\
\hline
omriq     & 1.49s & 0.88s & 2.05s & 0.03s & 2.08s\\
\hline
palm      & 2.75s & 1.45s & 3.75s & 0.50s & 15.75s\\
\hline
ep        & 2.50s & 0.98s & 3.19s & 0.11s & 3.21s\\
\hline
miniGhost & 0.87s & 1.07s & 1.69s & 1.23s & 13.17s\\
\hline
cg        & 62.98s & 28.74s & 64.86s & 0.28s & 68.43s\\
\hline
csp       & 2.78s & 1.20s	& 3.64s & 26.69s & 309.99s\\
\hline
ilbdc     & 160.62s & 2.10s & 160.39s & 27.41s & 615.26s\\
\hline
bt        & 5.92s & 1.27s & 7.27s & 9.13s & 119.28s\\        
\hline
\end{tabular}
\end{center}
\caption{Results showing overhead of the PCAST OpenACC autocompare feature.}
\label{res1}
\end{table}

\begin{itemize}
\item Time to run the test data set sequentially on the CPU.
\item Time to run the test data set in parallel on the GPU.
\item Time to run the test data set redundantly on both CPU and GPU without the autocompare feature enabled.
\item Time to copy the data from GPU to CPU before comparing, measured by nvprof.
\item Time to run the test data set redundantly on both CPU and GPU using the autocompare feature.
\end{itemize}



%\begin{figure*}[t]
%    \centering
%    \includegraphics [width=1\linewidth] {Table1.pdf}
%    \caption{Results showing overhead of the PCAST OpenACC autocompare feature.}
%    \label{fig:sle_figure}
%\end{figure*}

\begin{figure*}
    \centering
    \includegraphics [width=1\linewidth] {npic3.pdf}
    \caption{Results showing overhead of the PCAST OpenACC autocompare feature.}
    \label{fig:sle_figure}
\end{figure*}


Figure~\ref{fig:sle_figure} breaks down the time spent in the autocompare run into:
\begin{itemize}
\item Compute Time: the max of the CPU time and GPU time from Table~\ref{res1}.
\item Redundancy Overhead: the difference between the redundant execution time from Table~\ref{res1} and Compute Time.
\item Download Time: the time spent downloading data as measured using nvprof.
\item Compare Time: the difference between autocompare time from Table~\ref{res1} and the sum of the above three times.
\end{itemize}



The costs of the autocompare feature are running the compute region on both CPU and GPU, and downloading and comparing the values.
The cost of redundant execution is less than the sum of the CPU and GPU times, because the GPU code executes asynchronously while the CPU executes the corresponding code.
Since this is a feature used during code development and debugging, we consider this to be relatively low overhead.
The cost of doing the many floating point comparisons is significant, and appears to be directly related to the number of data items compared, and unrelated to the number of arrays or variables being compared.
However, using this feature to find where a GPU computation diverges moves the cost from the programmer to the computer, so it could be invaluable regardless of the overhead. 



In Table~\ref{res2} 
we show some statistics about the benchmarks we used, such as:
\begin{itemize}
\item Number of variables or arrays compared.
\item Number of data values compared.
\item Number of variables or arrays that had some differences.
\item Number of data values that were different.
\end{itemize}

%%%%%%%%%%%%%%
%% Table 3 start
%%%%%%%%%%%%%%
\begin{table}
\begin{center}
\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|}
\hline
Benchmark &  \parbox[c]{3 cm}{\centering Variables and \\ arrays compared} &  \parbox[c]{2 cm}{\centering Values \\ compared} &  \parbox[c]{3 cm}{\centering variables and arrays \\     with differences} & \parbox[c]{2 cm}{\centering Differences \\      tolerated}\\
\hline
\hline
%303.ostencil & 202 & 3,388,997,632 & 0 & 0 \\
%304.olbm     & 61 & 1,586,800,000 & 59 & 520,634,266 \\
%363.swim     & 259 & 67,897,602 & 259 & 22,336,658 \\

ostencil  & 202 & 3388997632 & 0 & 0\\
\hline
olbm      & 61 & 586800000 & 59 & 520634266\\
\hline
omriq     & 3 & 68608 & 2 & 53240 \\
\hline
palm      & 31244 & 1532482935 & 14784 & 374679922\\
\hline
ep        & 4 & 13 & 2 & 2 \\
\hline
miniGhost & 2506 & 1844059545 & 175 & 175\\
\hline
cg        & 186 & 621600195 & 168 & 4858272\\
\hline
csp       & 4057 & 40132155677	& 3897 & 5693059\\
\hline
ilbdc     & 3001 & 53818895200 & 2000 & 35305830600 \\
\hline
bt        & 5036 & 15041440200 & 4798 & 38931891 \\
\hline
\end{tabular}
\end{center}
\caption{Results showing number of variables and values processed by the PCAST OpenACC autocompare.}
\label{res2}
\end{table}
%%%%%%%%%%%%%%
%% Table 3 end
%%%%%%%%%%%%%%


\begin{figure*}[ht]
    \centering
    \includegraphics [width=1\linewidth] {GBperSec.pdf}
    \caption{Results showing amount of data the autocompare feature process per unit time.}
    \label{fig:gbps_figure}
\end{figure*}


An interesting observation we came across was that on all benchmarks we tested it roughly took one second to compare a gigabyte of data as can be seen in Figure~\ref{fig:gbps_figure}.
This can be used as a rough estimate of how long it will take to test a code for correctness as long as the user knows the size of the data set being used.


One side note: the \emph{test} datasets used here are relatively small.
Even so, we had to set the relative tolerance to avoid the comparisons detecting differences, mostly due to different summation accumulation order.
Surprisingly, those differences propagated to about $\frac{1}{3}$ of the results in two of the ten benchmarks that we show here.
This implies that the naive approach to reduce the number of values being compared and therefore reducing the runtime cost by computing and comparing a quick checksum or signature before downloading and comparing all the data would frequently have little or no benefit.

%% stripped down version of "bare_jrnl.tex" for use in Casey's Circuits I class.
%% Original version has very good comments for use. You should check it out at
%% http://www.ieee.org/publications_standards/publications/authors/authors_journals.html
%% I run GNU/Linux so I downloaded the "Unix LaTeX2e Transactions Style File" package
%% and based my work off of the sample tex file named "bare_jrnl.tex".

% original author info below (this guy's a rockstar for making his comments so easy to use :P)
%% 2007/01/11
%% by Michael Shell
%% see http://www.michaelshell.org/
%% for current contact information.

\documentclass[journal]{IEEEtran}
% make sure "IEEEtran.cls" is in the path of the tex file you are working on

%graphics package for adding images
\ifCLASSINFOpdf
  \usepackage[pdftex]{graphicx}
\else
   \usepackage[dvips]{graphicx}
\fi

%math package for math equations
\usepackage[cmex10]{amsmath}

%float package for putting images where I fucking tell them to go
\usepackage{float}

%For hyperlinks
\usepackage{hyperref}

\begin{document}

% paper title
% can use linebreaks \\ within to get better formatting as desired
\title{CS 3339 Computer Architecture Homework One}
\author{Preston~Maness}

% header
\markboth{Texas State University, Dr. Burtscher, CS3339, Fall 2013}%
{}

% make the title area
\maketitle

\tableofcontents

\section{Problem 1}

\textit{Can processors natively excute assembly instructions? If so, how? If
not, why not?}

Well, strictly speaking, the assembler will convert the assembly language 
into relevant opcodes that the target architecture understands. I am assuming
here that the assembly instructions mentioned in the question match the chip's
architecture. Amd64 ISA compiled down to opcodes won't run if fed into an i386.

Typically the relationship between an architecture's instruction set and the
opcodes generated by the assembler is nearly one to one. So it could be said
that processors can natively excute assembly instructions, assuming of course
that the instructions are from the Instruction Set Architecture of the target
chip. However, any given chip may or may not support all of the instructions in
the ISA, as ISAs grow and evolve over time. ISAs tend to be forward compatible,
but not necessarily backward compatible. 

For example, i386 is forward compatible. Code written for i386 will run on
i686. However, special instructions added into the ISA later might not be
backward compatible with older chips. So an i386 chip might not be able to
natively excute an instruction that was added to the ISA later for newer
generations such as the i686.

\section{Problem 2}

\textit{What are the two main architectural components of a CPU?}

There are more than two, though the main two are bolded in the below list:

\begin{itemize}
\item
\textbf{Datapath}
\item
\textbf{Control}
\item
Memory
\item
Input
\item
Output
\end{itemize}

\section{Problem 3}

\textit{What does it mean for a newer processor to be binary compatible with an
older processor?}

It means that the newer chip can account for all of the instructions that 
the older chip could. A self-contained program compiled down to a binary for an
older iteration of the ISA (i386, for example) should be able to run without
recompiling on a newer chip within the same ISA (i686, for example).

\section{Problem 4}

\textit{These days, how do microprocessor manufacturers primarily use
additional transistors that become available with every new chip generation?}

Much of it gets thrown into various levels of caching and, more recently, 
into additional CPU cores or even other system components on the same die 
(System-on-chip, aka SoC). As an example, intel has recently begun including 
GPUs on-die with their CPUs. These GPUs do not have the same performance 
characteristics as those of their competitors (Intel, AMD) but are far 
cheaper than discrete graphics cards and deliver acceptable performance for 
many consumers.

\section{Problem 5}

\textit{To what clock frequency (in GHz) does a 125 ps clock cycle time
correspond?}

\begin{center}
$f = \frac{1}{T} = \frac{1}{125\cdot 10^{-12}} = 8000000000 Hz = 8 GHz$
\end{center}

\section{Problem 6}

\textit{What is the best metric for measuring CPU performance?}

It depends on the design goals, of which there are many possible performance 
indicators:

\begin{itemize}
\item
Power Consumption
\item
Clock Speed
\item
Cost
\item
Response Time
\item
Throughput
\end{itemize}

All of these may be expressed in terms of their relation to each other as 
well. That is to say, one might be concerned with the number of watts 
generated per 100 MHz of clock speed. Or one might want the best clock speed 
per dollar spent.

As an example, data centers and other big number crunchers will be more
concerned with power consumption efficiency. Each device that draws current
should draw as few amps per common performance indicator as possible (often
speed).  This lowers both the costs of powering the devices and of cooling
them. It is often cheaper to power many mid-tier devices and combine their 
capabilities than it is to attempt to purchase as few upper-tier devices as 
possible.

\textbf{The lecture slides say ``time to execute program'' is the best 
metric.}

\section{Problem 7}

\textit{Explain how it is possible for a longer sequence of instructions to 
execute faster than a shorter sequence (on the same processor) that 
computes the same result.}

Different instructions in the ISA can have vastly different clock cycle counts.
The longer sequence of instructions might be composed mainly of instructions 
that require very few cycles to execute while the shorter sequence of 
instructions might have several instructions that require many cycles to 
finish.

Mathematically, we have:

\begin{align*}
T_{execution}&=\frac{CPI\cdot IC}{f}\\
&=\displaystyle\sum\limits_{i}^{n}\frac{IC_{i}\cdot
CC_{i}}{IC}\cdot IC\cdot \frac{1}{f} \\
\end{align*}

So, even if $IC$ is larger, we can see that the total execution time might 
be lower if, for example, $CC_{i}$ is lower for the instructions in the 
longer list. 

\section{Problem 8}

\textit{If architectural enhancement A speeds up 29\% of the executed 
instructions by 3 cycles and architectural enhancement B speeds up 44\% of 
the executed instructions by 2 cycles, which ehancement provides more 
speedup? Show your calculations.}

Recall our equation for CPU time:

\begin{center}
$T_{execution} = \displaystyle\sum\limits_{i}^{n}\frac{IC_{i}\cdot
CC_{i}}{IC}\cdot IC \cdot\frac{1}{f}$
\end{center}

It is assumed that the frequency and total instruction count remains the same,
so we are interested in how these two changes will affect the following
expression:

\begin{center}
$\displaystyle\sum\limits_{i}^{n}\frac{IC_{i}\cdot CC_{i}}{IC}$
\end{center}

One could decrease $T_{execution}$ by reducing this sum. Whichever enhancement 
decreases this sum more is the better enhancement, assuming of course that both
execution times are initially the same (If they aren't, then the higher
before/after execution time ratio is the better enhancement). We shall denote
$T_{ia}$, the original CPU time for a, and $T_{ib}$, the original CPU time for
b, as $k$ and $l$ respectively. $k'$ and $l'$ will denote the respective CPU
times after speedup. Notice that, for enhancement A, we are told 29 percent of
instructions are sped up by reducing clock cycle count by 3 cycles --we denote
these cycles as $CC_{1}$-- while the other 71 percent --denoted as $CC_{2}$--
are presumably the same. Let us investigate enhancement A prior to and after
its activation: 

\begin{align*}
k &\propto 0.29\cdot CC_{1} + 0.71\cdot CC_{2}\\
k' &\propto 0.29\cdot (CC_{1} - 3) + 0.71\cdot CC_{2}\\
&\propto 0.29\cdot CC_{1} - 3\cdot 0.29 + 0.71\cdot CC_{2}\\
&\propto 0.29\cdot CC_{1} + 0.71\cdot CC_{2} - 0.87
\end{align*}

We now perform the same analysis for enhancement B, where we are told that 
44 percent of instructions are sped up by reducing cycle count by 2 cycles 
--we denote these as $CC_{1}$-- and the remaining 56 percent of instructions 
--denoted as $CC_{2}$-- are presumably the same.

\begin{align*}
l &\propto 0.44\cdot CC_{1} + 0.56\cdot CC_{2}\\
l' &\propto 0.44\cdot (CC_{1} - 2) + 0.56\cdot CC_{2}\\
&\propto 0.44\cdot CC_{1} - 2\cdot 0.44 + 0.56\cdot CC_{2}\\
&\propto 0.44\cdot CC_{1} + 0.56\cdot CC_{2} - 0.88
\end{align*}

An observation: the question doesn't actually provide values for $CC_{1}$ and 
$CC_{2}$ for either enhancement A or B. Without this information, I don't 
think you can actually answer this question. Depending on those values, the
relationship between $k'$ and $l'$ changes. As well, we aren't told
\textit{which} instructions are being sped up in each case at all. $CC_{1}$ 
for $k'$ might be of a different value than $CC_{1}$ for $l'$, meaning that 
any arbitrary value could be substituted in for each of them to arrive at 
whatever conclusion one wished to arrive at.

Consider: let us assume that $CC_{1}$ and $CC_{2}$ are the same value for 
each enhancement. What this essentially boils down to is a system of two 
equations. These equations are:

\begin{align*}
k' &\propto 0.29\cdot (CC_{1} - 3) + 0.71\cdot CC_{2}\\
l' &\propto 0.44\cdot (CC_{1} - 2) + 0.56\cdot CC_{2}
\end{align*} 

But there are three unknowns here: $CC_{1}$, $CC_{2}$ and $T_{execution}$ 
($k'$ and $l'$ are both notations for the time of execution).  This can't even
be solved as it is, much less if $CC_{1}$ and $CC_{2}$ are independent in each
enhancement. There would be five unknowns then. One could only definitively 
answer the question of which enhancement was ``better'' if more information 
was provided.

However, this whole excercise has been assuming that the total instruction 
count $IC$ was finite. Suppose the CPU is permitted to run indefinitely. How 
might one measure the performance improvement over time? Here, we are going to 
\textit{attempt} to sidestep the too many variables problem by simply comparing
the ratios $k/k'$ and $l/l'$, since these will be the only meaningful sources
of differing $T_{execution}$ as total instruction count $IC$ grows.

%This still doesn't address the overlapping improvement problem. But that
%problem really only comes into play when attempting to implement multiple
%enhancements at once. Since we are only considering one at a time, we need not
%address the overlapping issue. Consider the $k/k'$ ratio:

Let us consider the $k/k'$ ratio:

\begin{align*}
\frac{k}{k'} &= \frac{0.29\cdot CC_{1} + 0.71\cdot CC_{2}}
{0.29\cdot (CC_{1} - 3) + 0.71\cdot CC_{2}}\\
&= \frac{0.29\cdot CC_{1} + 0.71\cdot CC_{2}}
{0.29\cdot CC_{1} + 0.71\cdot CC_{2} - 0.87}\\
&= \frac{k}{k-0.87}
\end{align*}

By similar work, we arrive at $l/l'$:

\begin{center}
$\displaystyle\frac{l}{l'}=\frac{l}{l-0.88}$
\end{center}

This is getting closer, but we can certainly reason that $k$ and $l$ are 
still unknown. So we haven't really eliminated the too many variables problem.
Perhaps we should try asking an interesting question. We are told that 
enhancement A decreased $CC_{1}$ by 3 clock cycles. Let us assume that 
$CC_{1}$ was originally 3 clock cycles. If this is the case, then we have 
essentially eliminated $CC_{1}$ as a source of slowdown completely, and are 
left only with whatever is left for $CC_{2}$. What would such a ratio for 
$k/k'$ look like then? This would, in principal, be the absolute maximum 
speed-up that could possibly arise from decreasing $CC_{1}$ by 3 clock cycles.
Observe:

\begin{align*}
\frac{k}{k'} &= \frac{0.29\cdot 3 + 0.71\cdot CC_{2}}
{0.29\left(3-3\right) + 0.71\cdot CC_{2}}\\
&= \frac{0.87 + 0.71\cdot CC_{2}}{0.71\cdot CC_{2}}\\
&= \frac{0.87}{0.71\cdot CC_{2}} + 1
\end{align*}

And similar work for $l/l'$:

\begin{center}
$\displaystyle\frac{l}{l'} = \frac{0.88}{0.56\cdot CC_{2}} + 1$
\end{center}

Well, we still have one unknown, but at this point, we have decent expressions 
for the absolute maximum increase in speedup for each enhancement. What's 
better, they are solely dependent on just how slow $CC_{2}$ was for each 
enhancement. So, essentially, we now have a system of two linear equations. 
We can graph each of these and, given $CC_{2}$, can state which enhancement 
could deliver the absolute best increase in speed if applied iteratively.

\textbf{We cannot determine speedup in any single iteration given only an
improvement in cycle count, because such a speedup is inherently dependent on
what the initial cycle count was.} If the initial cycle count was a billion
cycles, then there would be virtually no improvment by removing 3 cycles. If
the initial cycle count was 6 cycles, then there would be a significant
speedup. Now, if we had been provided a \textit{factor} of improvement, then we
could figure out the speed-up, for such a factor inherently includes the
necessary information that relates the initial number of clock cycles to the
new number.

\textbf{And we cannot say with certainty which enhancement would be ``better''
on the whole without knowing just how fast or slow the remaining instructions
are in each case.} At that point, the remainder of the program operation is
solely dependent on the portion that was not sped up. It's easy to know that
the lower $CC_{2}$ will be faster, but that doesn't tell us which
\textit{program} will experience a better improvement from where it started. 

\section{Problem 9}

\textit{If 30\% of the instructions take 1 cycle to execute, 20\% take 2 
cycles, 30\% take 3 cycles, and 20\% take 4 cycles, what is the average CPI?}

Recall:

$CPI = \sum\limits_{i}^{n}\frac{IC_{i}\cdot CC_{i}}{IC} $

In our case we are provided with percentages which accounts for the 
instruction-count-per-instruction-type $IC_{i}$ with respect to the total 
number of instructions $IC$, and so we simply have

$Weighted CPI = \displaystyle\sum\limits_{i}^{n}IC_{i,percentage}\cdot CC_{i} $

And we calculate

\begin{align*}
Weighted CPI &= \sum\limits_{i}^{n}IC_{i,percentage}\cdot CC_{i}\\
&= 0.3\cdot 1 + 0.2\cdot 2 + 0.3 \cdot 3 + 0.2\cdot 4\\
&= 2.4\\
\end{align*}

\section{Problem 10}

\textit{What was the main reason that ended the decades-long rapid increase 
in single-core performance?}

Power consumption. There's no good way to manage the heat a 20 GHz processor
would give off. Even if there were a good way to cool the chip, parasitic
capacitances become unmanageable as frequencies climb; the transistors can't
switch fast enough to keep up. So even if you could hook up a constant source
of sufficient cooling to your chip, it still wouldn't work well. Increasing the
die size is a no-go, because the chip fab processes' error rates climb as the
die size increases, leading to lower yields. Lower yields means fewer chips to
sell.

There are also physical limits to increasing the clock rate. The higher the 
rate, the more difficult it is to distinguish signal from heat noise (which 
there will be a bunch of given the high switching speed). Increasing the 
voltage strengthens the signal but also increases power output while the
transistors switch. Decreasing the voltage causes the threshold voltage to also
decrease, leading to leakage current when the transistor cannot switch to a
fully ``off'' state. Decreasing the size of the transistor helps mitigate this
effect but there's only so many atoms you can take away before quantum
effects start occuring. 

\end{document}

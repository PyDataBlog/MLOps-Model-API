% Chapter 4
\chapter{Model Analysis Toolbox} % Main chapter title

\label{Chapter4} % For referencing the chapter elsewhere, use \ref{Chapter1} 

\lhead{Chapter 4. \emph{Model Analysis Toolbox}} % This is for the header on each page - perhaps a shortened title

%-----------------------------------------------------------------------------------------------------------------

\par The purpose of this chapter is to present several mathematical tools used in the analysis of the different models in the
field of opinion dynamics. We will present the ideas behind these theorems and specify their overall contribution in the field.
While there are many interesting scientific results that utilize ideas not presented in this chapter, we believe the collection
of tools presented here consists of some of the most fundamental and important ideas that were used to provide us with the most
significant results. This chapter is quite important as, in our opinion, one needs to be familiarized with almost all the following
concepts in order to undertake research in the field of opinion dynamics.

\par We begin by defining the potential games as those that admit a specific potential function and show that the point where
this function reaches its optimum value is exactly the Nash equilibrium of a system. We continue by presenting certain fixed-point
theorems that are utilized to prove the existence of a Nash equilibrium, and maybe offer some ideas on how to prove convergence.
Then, we focus our attention on concave games, where the cost functions of the agents are concave, and state one of the most
important theorems about them, proved by Rosen \cite{Rosen}, before continuing with gradient descent-like methods that are
useful for minimizing specific functions. Finally, we present the innovative ideas of the energy approach to a system, that can be
used to circumvent several problems arising with step-by-step methods.

\section{Potential Functions} \label{Section4.1}

\par The concept of a potential function was first used in the analysis of congestion games. Specifically, Rosenthal proved, in 1973,
that every congestion game has a function which can be used to prove the existence of, and sometimes convergence to, a Nash equilibrium
\cite{Rosenthal}. We call this function the \textit{potential function} of the game and the idea behind the concept is to provide
a sense of quantifiable distance from the game's equilibrium point. In addition, games that possess such functions are call \textit{
potential games}. Monderer and Shapley utilized potential functions in 1996 to prove the converse; for every potential game, there
exists a congestion game with the same potential function \cite{Monderer-Shapley}.

\par While the concept of the potential function was, at first, closely associated with congestion games, it has since grown as a
mathematical tool and is utilized in general optimization problems, due to its simplicity and usefulness. In our field, we can consider
the agents, each one trying selfishly to minimize her own cost, as having a global, unified potential function that they are
working together to optimize. Thus, the potential function in opinion dynamics is closely associated with the cost that each agent
incurs in the model.

\par We will continue by defining the basic types of potential functions before presenting one that has provided interesting results
in the field. Consider a model with $n$ agents that can express any opinion $x_i \in \mathbb{R}$, and have cost functions
$C_i : \mathbb{R}^n \to \mathbb{R}$.

\begin{definition} [Exact Potential Function] \label{def4:pot-fun:exact}
If, in our model, there exists a function $\Phi : \mathbb{R}^n \to \mathbb{R}$ such that for every agent $i$ and every $x_i$, $x_i'$

\begin{equation} \label {eq4:pot-fun:exact}
\Phi (x_i', \bm{x}_{-i}) - \Phi (x_i, \bm{x}_{-i}) = C_i(x_i', \bm{x}_{-i}) - C_i(x_i, \bm{x}_{-i})
\end{equation}

then $\Phi$ is called an \textit{exact potential function} of our model.
\end{definition}

\par Intuitively, the exact potential function has the property that when any agent $i$ switches from an expressed opinion $x_i$
to $x_i'$ with all other agents fixed, the change in the potential function is exactly equal to the change in $i$'s cost. We can
generalize the concept by introducing a weight that makes the two values not exactly equal but proportional.

\begin{definition} [Weighted Potential Function] \label{def4:pot-fun:weighted}
If, in our model, there exists a function $\Phi : \mathbb{R}^n \to \mathbb{R}$ and a vector $\bm{w} \in \mathbb{R}^n_{++}$, such that for
every agent $i$ and every $x_i$, $x_i'$

\begin{equation} \label {eq4:pot-fun:weighted}
\Phi (x_i', \bm{x}_{-i}) - \Phi (x_i, \bm{x}_{-i}) = w_i(C_i(x_i', \bm{x}_{-i}) - C_i(x_i, \bm{x}_{-i}))
\end{equation}

then $\Phi$ is called a \textit{weighted potential function} of our model.
\end{definition}

\par We can further generalize the concept by merely requiring the signs of the differences in the values above to be equal.

\begin{definition} [Ordinal Potential Function] \label{def4:pot-fun:ordinal}
If, in our model, there exists a function $\Phi : \mathbb{R}^n \to \mathbb{R}$, such that for every agent $i$ and every $x_i$, $x_i'$

\begin{equation} \label {eq4:pot-fun:ordinal}
C_i(x_i', \bm{x}_{-i}) - C_i(x_i, \bm{x}_{-i}) > 0 \iff \Phi (x_i', \bm{x}_{-i}) - \Phi (x_i, \bm{x}_{-i}) > 0
\end{equation}

then $\Phi$ is called an \textit{ordinal potential function} of our model.
\end{definition}

\par Intuitively, in a game with an ordinal potential function with all other agents fixed, when agent $i$'s cost increases, $\Phi$
increases as well, and when $i$'s cost decreases, $\Phi$ decreases as well, for any agent $i$. Before we continue, we will present
a significant result on potential functions concerning their relation to pure Nash equilibria.

\begin{theorem} \label{theor4:pot-fun:nash-eq}
Every potential game admits to at least one pure strategy Nash equilibrium. Furthermore, if $\Phi$ is the game's potential function,
every Nash equilibrium is a local optimum of $\Phi$.
\end{theorem}

\par This theorem was proved by Monderer and Shapley in 1996 \cite{Monderer-Shapley}. However, we will skip the proof as it is fairly
simple and follows directly from our definition of the potential function and the finiteness of each agent's sequence of improvement
steps. It is understood now that potential functions are extremely helpful tools in optimization problems, when they exist, and,
besides guaranteeing the existence of a pure Nash equlibrium in our model, they also provide us with significant insight and interesting
convergence properties.

\subsection{Application to Opinion Dynamics} \label{Section4.1.1}

\par In this section, we continue by presenting the link between potential games and opinion dynamics. The well-studied properties of
potential functions have been utilized to provide deep insight and results in the field \cite{Kleinberg-Bindel, Ferraioli-Goldberg}.
It should be clear by now that potential functions are extremely useful for the analysis of an opinion formation model, when they
exist. However, the important question remains open; when do opinion formation models admit to a potential function? While this question
is not resolved for the general case of ordinal potential functions, we present here a necessary condition for an opinion formation model
to admit to an exact potential function.

\begin{theorem} \label{theor4:pot-fun:mixed-der}
Consider an opinion formation model where each agent $i$ expresses opinion $x_i \in \mathbb{R}$ and has a cost function $C_i :
\mathbb{R}^n \to \mathbb{R}$ that is continuous and twice differentiable. Then, the model admits to an exact potential function if
and only if for any two agents $i$ and $j$

\begin{equation} \label{eq4:pot-fun:mixed-der}
\frac {\partial^2 C_i(\bm{x})} {\partial x_i \partial x_j} = \frac {\partial^2 C_j(\bm{x})} {\partial x_i \partial x_j}
\end{equation}
\end{theorem}

\par We continue by proving that the undirected FJ model (Section \ref{Section2.1.2}) admits a potential function used to provide tight
bounds on the Price of Anarchy \cite{Kleinberg-Bindel}. Indeed, the cost functions in the undirected FJ model satisfy the necessary
condition of Theorem \ref{theor4:pot-fun:mixed-der}, since $\frac {\partial^2 C_i(\bm{x})} {\partial x_i \partial x_j} = - 2 w_{ij}$,
$\frac {\partial^2 C_j(\bm{x})} {\partial x_i \partial x_j} = -2 w_{ji}$ and $w_{ij} = w_{ji}$. Therefore, the undirected FJ model admits
to an exact potential function as stated by the theorem below.

\begin{theorem} \label{theor4:pot-fun:fj-pot}
Consider an instance of the undirected FJ model with $n$ agents that have intrinsic opinions $\bm{s}$ and expressed opinions $\bm{x}$.
Then, this model admits to an exact potential function

\begin{equation} \label{eq4:pot-fun:fj-pot}
\Phi (\bm{x}) = \sum_{ \{ i, j\} \in E(G)} {w_{ij} {(x_i - x_j)}^2} + \sum_{i = 1}^n {w_{ii} {(x_i - s_i)}^2}
\end{equation}
\end{theorem}

\begin{proof}
Let $i$ be an agent that deviates from his expressed opinion $x_i$ to $x_i'$, while all other agents remain fixed. The cost that $i$
incurs is $C_i(x_i, \bm{x}_{-i}) = \sum_{j \in \mathcal{N}_i} {w_{ij} {(x_i - x_j)}^2} + w_{ii} {(x_i - s_i)}^2$. Therefore, the
difference in $i$'s cost from his deviation is

\begin{align*}
C_i(x_i', \bm{x}_{-i}) - C_i(x_i, \bm{x}_{-i}) & = \sum_{j \in \mathcal{N}_i} {w_{ij} \big( {(x_i' - x_j)}^2 - {(x_i - x_j)}^2 \big) } \\
& + w_{ii} \big( {(x_i' - s_i)}^2 - {(x_i - s_i)}^2 \big)
\end{align*}

\par If we look at the difference in value of the potential function before and after the deviation, we have that all summands of the
form $w_{uv} {(x_u - x_v)}^2$ along with those of the form $w_{uu} {(x_u - s_u)}^2$, where $u, v \neq i$, are negated since both $u$
and $v$ remain fixed. Therefore, we get

\begin{align*}
\Phi (x_i', \bm{x}_{-i}) - \Phi (x_i, \bm{x}_{-i}) & = \sum_{ \{ i, j \} \in E(G)} {w_{ij} \big( {(x_i' - x_j)}^2 - {(x_i - x_j)}^2 \big) } \\
& + w_{ii} \big( {(x_i' - s_i)}^2 - {(x_i - s_i)}^2 \big)
\end{align*}

\par Since we have an instance of the undirected FJ model, $w_{ij} = w_{ji}$ for any two agents $i, j$. This implies that the sets
$A = \bigl\{ j : \{ i, j\} \in E(G) \bigr\}$ and $\mathcal{N}_i$ are equal, hence the two differences above are equal, and $\Phi$ is an
exact potential function of the FJ model.
\end{proof}



\section{Fixed-Point Theorems} \label{Section4.2}

\par In this section, we analyze one of the most fundamental mathematical concepts used to prove the existence, and sometimes
uniqueness, of a Nash equilibrium in game theory, the \textit{fixed point}. We present two of the most significant fixed-point theorems,
by Brouwer and Kakutani, and show their relation to game theory and opinion dynamics in particular. While fixed-point theorems appear
in many different regions of mathematics and their usefulness cannot be overstated, they hold a distinguished place in the field of
game theory, as they were used by Nash in his development of the Nash equilibrium as a solution concept for non-cooperative games.
We begin by defining the concept of a fixed point in the most general setting.

\begin{definition} [Fixed Point] \label{def4:fpts:fix-pt}
Consider a function $F : \mathbb{R}^n \to \mathbb{R}^n$. If there exists a point $\bm{x_0} \in \mathbb{R}^n$ such that $F(\bm{x_0})
= \bm{x_0}$, then $\bm{x_0}$ is called a \textit{fixed point} of $F$.
\end{definition}

\par In general, a fixed-point theorem is a result stating that a function $F$ will have at least one fixed point under certain
conditions on $F$ that can be stated in general terms. Although there exist a significant number of fixed-point theorems in mathematics,
only a handful are of interest in game theory. We present two that we consider the most significant, which played a central role in the
proof of existence of general equilibrium in market economies by Arrow and Debreu \cite{Arrow-Debreu} and in the proof of existence of a
mixed Nash equilibrium in every finite game for any number of players by Rosen \cite{Rosen}, starting with Brouwer's fixed-point theorem.

\subsection{Brouwer's Fixed-Point Theorem} \label{Section4.2.1}

\textit{Brouwer's fixed-point theorem} \cite{Brouwer} stands out among hundreds of others due to its broad range of applications across
numerous fields of mathematics. In its original field, this result is one of the key theorems characterizing the topology of Euclidean
spaces, which gives it a place among the fundamental theorems of topology. Here, we present a simple version of the theorem in the plane
and subsequently generalize it to any convex compact set.

\begin{theorem} \label{theor4:fpts:brouwer-simple}
Let $\mathcal{D} = \{ (x,y) \in \mathbb{R}^2 : {(x - a)}^2 + {(y - b)}^2 \leq r \}$ be a closed disk in $\mathbb{R}^2$, with center
$(a, b)$ and radius $r$, and $f : \mathcal{D} \to \mathcal{D}$ a continuous function. Then, $f$ has at least one fixed point.
\end{theorem}

\par While the theorem's proof is somewhat complicated, it is surprisingly easy to prove in one dimension, thus we will state the proof
for a continuous function $f$ defined on a closed interval $[a, b] \subset \mathbb{R}$ that takes values on the same interval.

\begin{proof}
Consider the function $g(x) = f(x) - x$. We have that $g(a) \geq 0$ and $g(b) \leq 0$. Then, by the intermediate value theorem, there
exists a point $x_0 \in [a, b]$ such that $g(x_0) = 0$. Therefore, $f(x_0) = x_0$, and $x_0$ is a fixed point of $f$.
\end{proof}

\par Intuitively, Theorem \ref{theor4:fpts:brouwer-simple} implies that if one stirs a cup of coffee to dissolve a lump of sugar, there
is always a point without motion. However, this example is not a perfect one as it does not demonstrate the non-uniqueness of the fixed
point. A better example is if one takes two identical horizontal sheets, crumple and flatten one of them and then place it on top of
the other. Brouwer's fixed-point theorem then implies that there exists a point on the crumpled sheet that is in the same place as on
the other sheet.

\par We continue by generalizing Theorem \ref{theor4:fpts:brouwer-simple} to any convex compact set in the Euclidean space.

\begin{theorem} \label{theor4:fpts:brouwer-general}
Let $K$ be a convex compact (closed and bounded) subset of a Euclidean space, and $f : K \to K$ a continuous function. Then, $f$ has at
least one fixed point.
\end{theorem}

\par We should note that each of the preconditions necessary by the theorem is very important, since the violation of any of them
renders the theorem unprovable. Indeed, we provide a counterexample for every case

\begin{itemize}
\item $K$ is convex and closed, but not bounded:

\par Consider the function $f(x) = x + 1$ from $\mathbb{R}$ to itself. Since $\mathbb{R}$ is convex and closed but not bounded, the
theorem does not hold. Indeed, as $f$ shifts each point to the right, it cannot have a fixed point.

\item $K$ is convex and bounded, but not closed:

\par Consider the function $f(x) = \frac{x + 1} {2}$ from the open interval $(-1, 1)$ to itself. Since $(-1, 1)$ is convex and bounded,
but not closed, the theorem does not hold. Indeed, as $f$ again shifts each point to the right, it cannot have a fixed point. Note that
$f$ has a fixed point in the closed interval $[-1, 1]$, namely $f(1) = 1$.

\item $K$ is compact (closed and bounded), but not convex:

\par Consider the function $f(r, \theta) = (r, \theta + \pi / 4)$ in polar coordinates, from the unit circle to itself. Since the unit
circle is compact but not convex (as it has a hole), the theorem does not hold. Indeed, as $f$ shifts each point by $45$ degrees in the
circle, it cannot have a fixed point. Note that $f$ has a fixed point in the unit disk, namely the origin $(0,0)$.
\end{itemize}

\par In addition, it should be noted that Brouwer's fixed-point theorem and Sperner's lemma \cite{Sperner}, an important result in
combinatorics and very useful in game theory, are equivalent, as assuming one of them, we are able to prove the other. Moreover, while
Brouwer's fixed-point theorem proves the existence of a fixed point, it is a non-constructive result and does not give any insight as
to how to find one. Indeed, the problem of finding a Brouwer fixed-point is proven to be PPAD-complete, a complexity class introduced
by Papadimitriou et al \cite{Papadimitriou-PPAD}, and is believed to be a difficult problem.

\subsection{Kakutani's Fixed-Point Theorem} \label{Section4.2.2}

\par In this section, we present \textit{Kakutani's fixed-point theorem}, which is a generalization of Brouwer's fixed-point theorem.
Kakutani extended Brouwer's theorem in 1941 \cite{Kakutani} to include set-valued functions. We begin with a few definitions, then
state the theorem and provide an example in order to assist the reader in the theorem's comprehension.

\begin{definition} [Set Valued Function] \label{def4:fpts:set-val-function}
A \textit{set-valued function} $\phi$ from a set $A$ to a set $B$ is a rule that associates one or more points in $B$ with each point
in $A$. Formally it can be seen just as an ordinary function from $A$ to the power set of $B$, written as $\phi: A \to 2^B$, such that
$\phi(x)$ is non-empty for every $x \in A$.
\end{definition}

\begin{definition} [Closed Graph] \label{def4:fpts:closed-graph}
A set-valued function $\phi : A \to 2^B$ is said to have a \textit{closed graph} if the set $C = \{ (x,y) : y \in \phi(x) \}$ is a
closed subset of the cartesian product $A \times B$.
\end{definition}

\par We also extend our definition of a fixed point (Definition \ref{def4:fpts:fix-pt}) to include fixed points of set-valued functions.

\begin{definition} [Fixed Point of a Set-Valued Function] \label{def4:fpts:set-fix-pt}
Consider a set-valued function $\phi : A \to 2^A$. If there exists a point $x_0 \in A$ such that $x_0 \in \phi(x_0)$, then $x_0$ is
called a \textit{fixed point} of $\phi$.
\end{definition}

\par We are now ready to state Kakutani's fixed-point theorem.

\begin{theorem} \label{theor4:fpts:kakutani}
Let $S$ be a non-empty, compact and convex subset of a Euclidean space $\mathbb{R}^n$, and $\phi : S \to 2^S$ a set-valued function on
$S$ with a closed graph. Also, let $\phi(x)$ be non-empty and convex for all $x \in S$. Then, $\phi$ has at least one fixed point.
\end{theorem}

\par Consider the following example to help with the comprehension of the theorem. Let $f(x)$ be a set-valued function defined on the
closed interval $[0, 1]$ that maps a point $x \in [0, 1]$ to a subset of the closed interval $[1 - x/2, \: 1 - x/4]$. Then, $f$ satisfies
all the assumptions of Theorem \ref{theor4:fpts:kakutani}, thus it has at least one fixed-point. If we plot the function on the closed
interval $[0, 1]$ we get Figure \ref{fig4:fpts:kakutani}.

\begin{figure}[h!]
  \centering
  \includegraphics[width=.45\linewidth]{./Figures/kakutani.png}
  \caption{An example of Kakutani's fixed-point theorem}
  \label{fig4:fpts:kakutani}
\end{figure}

\par Every point in the intersection of the red dotted line and the shaded grey area is a fixed point of $f$, which implies that, in
this case, $f$ has an infinite number of fixed points. Indeed, $x = 0.72$, denoted in Figure \ref{fig4:fpts:kakutani} with the dashed
blue line, is a fixed point, since $[1 - 0.72 / 2, 1 - 0.72 / 4] = [0.64, 0.82]$ and $0.72 \in [0.64, 0.82]$.

\par Kakutani's fixed-point theorem has a significant number of applications to game theory. Specifically, as is discussed in
Kakutani's original paper, the theorem can be used in zero-sum games, where each player's gain or loss in utility is exactly balanced
by the losses or gains in utility of the other players, to prove the minimax theorem. However, its most important contribution
to game theory is perhaps its application in the proof by Nash of the existence of a mixed strategy Nash equilibrium in every finite game
for any number of players, a work that later earned him a Nobel Prize in Economics \cite{Nash}.

\par In such a game, the tuples of mixed strategies chosen by each player constitutes the set $S$, and $\phi(\bm{x})$ is the function
that, for the players' strategies in $\bm{x}$, returns a new tuple where each player's strategy is her best response to the other
players' strategies in $\bm{x}$. It is possible for two or more strategies to be equally good, thus $\phi$ is set-valued. A Nash
equilibrium of this game is defined as a fixed point of $\phi$, specifically a tuple of strategies $\bm{x_0}$ where each player's
strategy is a best response to the strategies of the other players in $\bm{x_0}$. The existence of such a fixed point, therefore a
Nash equilibrium, follows directly from Kakutani's fixed-point theorem.



\section{Concave Games} \label{Section4.3}

In this section we focus our attention on \textit{concave games}. Concave games are characterized by two properties; every strategy
lies inside a convex region of the product space of the individual strategies and each player's payoff function is concave. Before
we continue, we define them formally below

\begin{definition} [Concave Game] \label{def4:convex:conc}
A game with $n$ players is called a \textit{concave game} if and only if it satisfies the following properties:

\begin{itemize}
\item Every joint strategy $\bm{x} = (x_1, x_2, \hdots, x_n)$, represented by a point in the product space of the individual strategy
spaces, lies inside a convex and compact region $R$ of the product space.

\item Each player's payoff function $\phi_i$ is concave in his own strategy $x_i$.
\end{itemize}
\end{definition}

\par Concave games have been studied a lot since they possess interesting properties that simplifies their analysis. Rosen, in his
infamous theorem presented below, proved the existence of equilibrium points for every $n$-person concave game, and specified a certain
property of the players' cost functions necessary for the equilibrium point to be unique. He also showed that if this property holds,
the continuous best-response dynamics of the game converge to the unique equilibrium for any starting point.

\par We continue with the definition of a \textit{socially concave game}, introduced by Even-Dar et al \cite{Even-Dar}.

\begin{definition} [Socially Concave Game] \label{def4:convex:soc-conc}
A game with $n$ players is called a \textit{socially concave game} if and only if it satisfies the following properties:

\begin{itemize}
\item For every agent $i$, there exists a $\lambda_i > 0$ such that $f(\bm{x}) = \sum_i { \lambda_i \phi_i(\bm{x})}$ is concave
in $\bm{x}$.

\item For every agent $i$, the utility function $\phi_i(x_i, \bm{x}_{-i})$ is concave in $x_i$ and convex in $\bm{x}_{-i}$.
\end{itemize}
\end{definition}

\par If, in the definition above, we replace concavity with strict concavity, we define a \textit{strict socially concave game}.
In the next section, we show that strict socially concave games always satisfy the assumptions of Rosen's theorem, thus the
continuous best-response dynamics of these games always converge to an equilibrium point for any starting point.

\subsection{Rosen's Theorem} \label{Section4.3.1}

\par Certainly one of the most important results in Convex Optimization theory, Rosen's theorem sheds light on the convergence
properties of $n$-person concave games. Proved by Rosen in 1965 \cite{Rosen}, it consists of three parts, each one providing deep
insight into the existence and uniqueness of Nash equilibria, and the necessary conditions for convergence of a concave game to them.
We will present each section of Rosen's theorem as an individual theorem, without providing any proof since we prefer to present these
ideas without getting into technical details.

\begin{theorem} \label{theor4:convex:rosen-exist}
For every $n$-person concave game, as defined in Definition \ref{def4:convex:conc}, there exists at least one equilibrium point.
\end{theorem}

\par This theorem follows directly from the application of Kakutani's fixed-point theorem (Section \ref{Section4.2.2}) on the convex
and compact set of players' strategies $R$, along with a point-to-set mapping from $R$ to $R$, that takes each point $\bm{x} \in R$ to
the point where each player chooses her best-response to the strategy of all other players in $\bm{x}$, that maximizes her utility
function.

\par While equilibrium points are guaranteed to exist in concave games by Theorem \ref{theor4:convex:rosen-exist}, their usefulness
is limited since their uniqueness is not guaranteed. In fact, many games possess an infinite number of equilibrium points
\cite{Shapley}. We continue by defining an additional concavity property that, when satisfied, guarantees the uniqueness of the
equilibrium point.

\begin{definition} [Diagonal Strict Concavity] \label{def4:convex:diag-strict-conc}
Consider a $n$-person concave game with the players' strategies lying inside a convex and compact set $R$, a coefficient vector
$\bm{r} \in \mathbb{R}_{++}$ and a weighted nonnegative sum of the players' payoff functions

$$
\sigma(\bm{x}, \bm{r}) = \sum_{i = 1}^n {r_i \phi_i(\bm{x})}.
$$

We also denote by $g(\bm{x}, \bm{r})$ the function

$$
g(\bm{x}, \bm{r}) = \begin{bmatrix} r_1 \nabla_1 \phi_1(\bm{x}) \\ r_2 \nabla_2 \phi_2(\bm{x}) \\ \vdots \\ r_n \nabla_n \phi_n(\bm{x}) \end{bmatrix}
$$

and call it the \textit{pseudogradient} of $\sigma(\bm{x}, \bm{r})$. The function $\sigma(\bm{x}, \bm{r})$ is called
\textit{diagonally strictly concave} for $\bm{x} \in R$ and fixed $\bm{r}$, if the symmetric matrix
$\big[ G(\bm{x}, \bm{r}) + G^T(\bm{x}, \bm{r}) \big]$ is negative definite for $\bm{x} \in R$, where $G(\bm{x}, \bm{r})$ is the
Jacobian matrix of $g(\bm{x}, \bm{r})$.
\end{definition}

\par Rosen utilizes the Karush-Kuhn-Tucker \cite{Karush, Kuhn-Tucker} conditions to show that the diagonal scrict concavity of
$\sigma(\bm{x}, \bm{r})$ is sufficient to prove the uniqueness of the equilibrium point.

\begin{theorem} \label{theor4:convex:rosen-unique}
Consider a $n$-person concave game that has at least one equilibrium point $\bm{x_0}$ from Theorem \ref{theor4:convex:rosen-exist}.
Then, if $\sigma(\bm{x}, \bm{r})$ is diagonally strictly concave for some $\bm{r} \in \mathbb{R}_{++}$, the equilibrium point
$\bm{x_0}$ is unique.
\end{theorem}

\par Finally, we show that diagonal strict concavity on $\sigma(\bm{x}, \bm{r})$ implies the convergence of the continuous best-response
dynamics of a $n$-person concave game to the unique equilibrium point, for any set of initial conditions. To prove this result, Rosen
utilized his previous theorems along with the Karush-Kuhn-Tucker conditions to develop a gradient descent-like approach (Section
\ref{Section4.4}) and prove that the distance of the current point $\bm{x} \in R$ from $\bm{x_0}$ is decreasing with time, therefore it
approaches zero asymptotically, and the system will converge asymptotically to $\bm{x_0}$.

\begin{theorem} \label{theor4:convex:rosen-conv}
Consider a $n$-person concave game, with the property that $\sigma(\bm{x}, \bm{r})$ is diagonally strictly concave for some
$\bm{r} \in \mathbb{R}_{++}$. From Theorem \ref{theor4:convex:rosen-unique}, the game has a unique equilibrium point $\bm{x_0}$,
and the continuous best-response dynamics of the game, where each player changes her strategy to one that maximizes her utility function
given that all other players remain fixed, converge asymptotically to $\bm{x_0}$ for any initial point $\bm{x} \in R$. 
\end{theorem}

\par Strict socially concave games, as defined in Definition \ref{def4:convex:soc-conc} with strict concavity, satisfy the
conditions of Theorems \ref{theor4:convex:rosen-unique} and \ref{theor4:convex:rosen-conv}, therefore, in strict socially concave
games, the continuous best-response dynamics always converge to the unique equilibrium point for any starting point. However, if
the conditions of Theorem \ref{theor4:convex:rosen-conv} do not hold, the best-response dynamics need not converge to any equilibrium.
For example, there exists a $2$-person non-strict socially concave game in which the best-response dynamics do not converge.
Furthermore, if players move towards the point chosen by best-response dynamics but not at a fixed proportional speed, the dynamics
need not converge. In addition, in socially concave games, the sequential best-response dynamics, where each agent chooses her
best response in turns, need not converge, even for the case of $2$ players.

\par It should be noted here that Rosen's theorem was used by Bhawalkar et al \cite{Munagala} to prove the existence of a Nash
equilibrium for the Generalized Asymmetric model (Section \ref{Section3.5.2}) and to prove that the Asymmetric $k$-NN model need not
necessarily converge, since the agents' cost functions are not convex (Section \ref{Section3.4.2}).



\section{Gradient Descent Methods} \label{Section4.4}

\par Up until now, we have analyzed several tools used to prove the existence of equilibrium points for opinion formation models.
While some of them also provide a framework suitable to study the convergence properties of the models, they require strong
assumptions and cannot be generalized properly. Here, we present one of the most significant methods used in optimization problems
in order to locate a local (or global) minimum of a function, called \textit{gradient descent}. Recall that an equilibrium point
of an opinion formation model is always a local minimum of a potential function (Theorem \ref{theor4:pot-fun:nash-eq}), if our model
admits to one. Therefore, we strive to minimize the potential function in order to reach the equilibrium point.

\par The gradient descent method makes use of the observation that the value of a function $F$ decreases fastest if, from a specific
point $x$, one takes steps proportional to the negative of the gradient of $F$ at $x$, $- \nabla F(x)$. Gradient descent, sometimes
also called \textit{steepest descent}, is a first-order technique so significant, it has spawned several first-order variations, which
provide a framework for the analysis of models where simple gradient descent fails. However, all such variations fall under one of the
two main categories of first-order techniques; gradient descent or \textit{mirror descent}. Here, we present the ideas of both methods,
without getting into many technical details, and show their usefulness in the analysis of opinion formation models.

\subsection{Gradient Descent} \label{Section4.4.1}

\par Gradient descent, in its simplest form, is a first-order iterative algorithm used to locate a local minimum of a function.

\begin{definition} [Gradient Descent] \label{def4:grad-desc:grad-desc}
Consider a multi-variable function $F$, and a point $x_n$. Let $F$ be defined and differentiable in a neighborhood of
$x_n$. Then, in the \textit{gradient descent method}, at step $n + 1$ of the algorithm, we move to the point

\begin{equation} \label{eq4:grad-desc:grad-desc}
x_{n + 1} = x_n - \gamma_n \nabla F(x_n), \: n \geq 0
\end{equation}

with $\gamma_n$ a scalar value called the \textit{step size} of the algorithm. Another way to write the above equation is the
following

\begin{equation} \label{eq4:grad-desc:grad-desc2}
x_{n + 1} = arg \min_x \Big( \langle \nabla F(x_n), x \rangle + \frac{1} {\gamma_n} \| x - x_n \|_2^2 \Big), \: n \geq 0
\end{equation}

where $\langle a, b \rangle$ is the inner product of $a$ and $b$.
\end{definition}

\par It is easy to see that for $\gamma_n$ small enough, $F(x_{n + 1}) \leq F(x_n)$. Therefore, we get the sequence
$F(x_0) \geq F(x_1) \geq F(x_2) \geq \hdots $, which hopefully converges to a local minimum. Next, we see a set
of conditions on $F$ and $\gamma$ that guarantee the convergence of the method to a minimum. Note that, for a convex function $F$,
there is only a single global minimum, therefore, if the method converges, it will converge to the global minimum of $F$.

\begin{theorem} \label{theor4:grad-desc:conv}
Let $F$ be a convex function that is differentiable and its gradient $\nabla F$ is Lipschitz continuous. Also, consider a sequence
of step sizes $\gamma_n$ that satisfy the Wolfe conditions \cite{Wolfe}. Then, there exists a point $x^\ast$ such that
$F(x^\ast) \leq F(x)$ for all $x$, and the gradient descent method defined =at Definition \ref{def4:grad-desc:grad-desc} converges
to $x^\ast$.
\end{theorem}

\par While the method of gradient descent is a fundamental tool of convex non-linear optimization, it is not without flaws. There
are certain classes of pathological functions that render gradient descent not a particularly useful technique. In addition, gradient
descent does not converge very fast to the minimum, sometimes requiring many iterations to arrive at a specified distance from the
point of convergence. Indeed, there are many techniques, based on Newton's method, that converge in fewer iterations. However, the
computational cost of each iteration step is significantly higher in these techniques, and its simplicity and variety of applications
make gradient descent almost always the first choice in non-linear optimization.

\par Gradient descent can be used in opinion dynamics to assist in the understanding of a model's convergence properties. It can
provide insight into how the model's update rule will unfold over time and study whether its iteration will eventually converge.
Indeed, gradient descent was used by Rosen in his proof of Theorem \ref{theor4:convex:rosen-conv}, and we can also show that
in undirected linear models, like the Friedkin-Johnsel model (Section \ref{Section2.1.2}), the concurrent best-respone strategy
used by all agents is equivalent to performing a gradient descent method on the model's potential function \cite{Skoulakis}.

\par We proceed to show that the last claim holds. Consider an instance of the undirected FJ model. The game admits to a potential
function

\begin{equation} \label{eq4:grad-desc:pot-fun}
\Phi(\bm{x}) = \sum_{i = 1}^n { x_i \Big( w_{ii} (x_i - s_i) + \sum_{j \neq i}^n {w_{ij} (x_i - x_j)} \Big) }
\end{equation}

\par From (\ref{eq2:fj:averaging}), at each time step, each agent sets his opinion to

\begin{equation} \label{eq4:grad-desc:fj-averaging}
x_i(t+1) = w_{ii}s_i + \sum_{j \neq i}^n {w_{ij}x_j(t)}
\end{equation}

or, in matrix form

\begin{equation} \label{eq4:grad-desc:fj-matrix}
\bm{x}(t+1) = \bm{A}\bm{x}(t) + \bm{B}\bm{s}
\end{equation}

with $w_{ij} = 0$ for $j \notin \mathcal{N}_i$. Then, the potential function can be rewritten in matrix form as

\begin{equation} \label{eq4:grad-desc:pot-fun-matrix}
\Phi(\bm{x}) = \bm{x}^T \bm{L} \bm{x} - \bm{B} \bm{x}
\end{equation}

where $\bm{L}$ is the Laplacian matrix of the model, with $L_{ii} = \sum_{j = 1}^n w_{ij}$ and $L_{ij} = - w_{ij}$ for all
$j \neq i$. Since our model is undirected, $\bm{L}$ is a symmetric positive semidefinite matrix, thus $\Phi$ is a quadratic
and convex function of $\bm{x}$. We can calculate the gradient of $\Phi$

\begin{equation} \label{eq4:grad-desc:pot-fun-grad}
\nabla \Phi(\bm{x}) = \bm{L} \bm{x} - \bm{B}
\end{equation}

\par Since we assume normalized weights, $\sum_{j = 1}^n w_{ij} = 1$ for all agents $i$, we can rework
(\ref{eq4:grad-desc:fj-averaging}) to simulate a gradient descent method

\begin{equation} \label{eq4:grad-desc:fj-grad-desc}
\bm{x}(t+1) = \bm{x}(t) - \nabla \Phi(\bm{x}(t))
\end{equation}

with $\bm{x}(0) = \bm{s}$ in the FJ model. Next, we will prove that the gradient descent method above converges to the equilibrium
$\bm{x^\ast} = \bm{L}^{-1} \bm{B}$. To do that, we first need to prove the following lemmas

\begin{lemma} \label{lem4:grad-desc:dist-shrink}
For any time step $t \geq 0$, we have

\begin{equation} \label{eq4:grad-desc:dist-shrink}
\bm{x}(t+1) - \bm{x^\ast} = \bm{A} (\bm{x}(t) - \bm{x^\ast})
\end{equation}

where $\bm{A}$ is the averaging matrix in (\ref{eq4:grad-desc:fj-matrix}).
\end{lemma}

\begin{proof}
Substituting (\ref{eq4:grad-desc:pot-fun-grad}) into (\ref{eq4:grad-desc:fj-grad-desc}) gives us

$$
\bm{x}(t+1) = \bm{x}(t) - \bm{L} \bm{x}(t) + \bm{B}
$$

\par But $\bm{B} = \bm{L} \bm{x^\ast}$, therefore

\begin{align*}
\bm{x}(t+1) & = \bm{x}(t) - \bm{L} \bm{x}(t) + \bm{L} \bm{x^\ast} \\
& = \bm{x}(t) - \bm{L} (\bm{x}(t) - \bm{x^\ast})
\end{align*}

\par Subtracting $\bm{x^\ast}$ from the equation above gives us

\begin{align*}
\bm{x}(t+1) - \bm{x^\ast} & = \bm{x}(t) - \bm{x^\ast} - \bm{L} \bm{x}(t) + \bm{L} \bm{x^\ast} \\
& = (\bm{I} - \bm{L})(\bm{x}(t) - \bm{x^\ast})
\end{align*}

where $\bm{I}$ is the $n \times n$ identity matrix. We observe that $\bm{I} - \bm{L} = \bm{A}$, where $\bm{A}$ is the
averaging matrix in (\ref{eq4:grad-desc:fj-matrix}), therefore

$$
\bm{x}(t+1) - \bm{x^\ast} = \bm{A} (\bm{x}(t) - \bm{x^\ast})
$$
\end{proof}

\par For simplicity, let $\bm{e}_t = \bm{x}(t) - \bm{x^\ast}$. Next, we show that if $\bm{A}$'s eigenvalues lie in $(-1, 1)$,
then the distance for the equilibrium, $\bm{e}_t$ strictly decreases at each time step $t$.

\begin{lemma} \label{lem4:grad-desc:err-shrink}
If $\max_i {|\lambda_i (\bm{A})|} < 1$, then, for any time step $t \geq 0$, we have

\begin{equation} \label{eq4:grad-desc:err-shrink}
\| \bm{e}_{t+1} \|_2^2 < \| \bm{e}_t \|_2^2
\end{equation}
\end{lemma}

\begin{proof}
Utilizing Lemma \ref{lem4:grad-desc:dist-shrink}, we have that

\begin{align*}
\| \bm{e}_t \|_2^2 - \| \bm{e}_{t+1} \|_2^2 & = \| \bm{e}_t \|_2^2 - \bm{e}_{t+1}^T \bm{e}_{t+1} \\
& = \| \bm{e}_t \|_2^2 - {(\bm{A} \bm{e}_t)}^T (\bm{A} \bm{e}_t) \\
& = \bm{e}_t^T \bm{e}_t - \bm{e}_t^T \bm{A}^2 \bm{e}_t \\
& = \bm{e}_t^T (\bm{I} - \bm{A}^2) \bm{e}_t
\end{align*}

\par All eigenvalues of $\bm{A}$ lie in $(-1, 1)$, thus all eigenvalues of $\bm{A}^2$ lie in $(0, 1)$. Therefore, all
eigenvalues of $\bm{I} - \bm{A}^2$ are strictly positive, thus $\bm{I} - \bm{A}^2$ is a positive definite matrix, which implies
that, for any $\bm{e}_t$, $\bm{e}_{t+1}$

$$
\| \bm{e}_t \|_2^2 - \| \bm{e}_{t+1} \|_2^2 > 0
$$
\end{proof}

\par We can now combine these two lemmas to prove the following theorem

\begin{theorem} \label{theor4:grad-desc:fj-convergence}
The gradient descent method described at (\ref{eq4:grad-desc:fj-grad-desc}) converges to the Nash equilibrium $\bm{x}^\ast$
of the FJ model, which is also the point where the potential function $\Phi$ attains its minimum value.
\end{theorem}

\begin{proof}
Recall that, in the FJ model, there exists at least one agent $i$ that has $w_{ii} > 0$. Therefore, $\bm{A}$ is a substochastic matrix
and its maximum eigenvalue $\max_i {|\lambda_i (\bm{A})|} < 1$. From Lemmas \ref{lem4:grad-desc:dist-shrink} and
\ref{lem4:grad-desc:err-shrink}, the gradient descent method described at (\ref{eq4:grad-desc:fj-grad-desc}) decreases the distance
from $\bm{x^\ast}$ at each iteration step, therefore it converges to the Nash equilibrium $\bm{x^\ast}$ of the FJ model.
\end{proof}

\par Therefore, we observe that, in undirected linear models, concurrent best-response is equivalent to performing a gradient
descent method on the model's potential function, and in these models all agents converge to a stable state.

\subsection{Mirror Descent} \label{Section4.4.2}

\par In an effort to generalize the gradient descent method beyond Euclidean metric spaces, a variant of the method called
\textit{mirror descent} was developed. This method utilizes the concept of \textit{Bregman divergence} instead of the Euclidean norm
as a measure of displacement, which we define below

\begin{definition} [Bregman Divergence] \label{def4:grad-desc:breg-div}
Let $\psi : \Omega \to \mathbb{R}$ be continuously differentiable and strictly convex function, defined on a closed convex set $\Omega$.
Then, for any two points $x, y \in \Omega$, the \textit{Bregman divergence} under $\psi$ is defined as

\begin{equation} \label{eq4:grad-desc:breg-div}
\mathcal{D}_\psi (x, y) = \psi(x) - \psi(y) - \langle \nabla \psi(y), x - y \rangle
\end{equation}
\end{definition}

\par For example, to get the Euclidean distance, we have $\psi(x) = \| x \|_2^2 / 2$. Then,
$\mathcal{D}_\psi (x, y) = \| x - y \|_2^2 / 2$. Intuitively, the Bregman divergence calculates the difference between the value of
$\psi$ at $x$ and the first order Taylor expansion of $\psi$ around $y$, evaluated at $x$. Bregman divergence generalizes the squared
Euclidean distance to a class of distances, all sharing similar properties, and has numerous applications in machine learning and
clustering. It posseses several useful properties, and we present a few of them below

\begin{itemize}
\item Nonnegativity: $\mathcal{D}_\psi (x, y) \geq 0$ for all $x, y$. Specifically, $\mathcal{D}_\psi (x, y) = 0$ if and only if
$x = y$.

\item Asymmetry: In general, we have that $\mathcal{D}_\psi (x, y) \neq \mathcal{D}_\psi (y, x)$.

\item Linearity in $\psi$: For any $a >0$, $\mathcal{D}_{\psi + a \phi} (x, y) = \mathcal{D}_\psi (x, y) + a \mathcal{D}_\phi (x, y)$.
\end{itemize}

\par Now we are ready to define the method of mirror descent

\begin{definition} [Mirror Descent] \label{def4:grad-desc:mirr-desc}
Consider a multi-variable function $F$, a point $x_n$ and a function $\psi$ that is continuously differentiable and strictly convex.
Let $F$ be defined and differentiable in a neighborhood of $x_n$. Then, in the \textit{mirror descent method}, at step $n + 1$ of the
algorithm, we move to the point

\begin{equation} \label{eq4:grad-desc:mirr-desc}
x_{n + 1} = arg \min_x \Big( \langle \nabla F(x_n), x \rangle + \frac{1} {\gamma_n} \mathcal{D}_\psi (x, x_n) \Big), \: n \geq 0
\end{equation}
\end{definition}

\par Notice that, since for $\psi(x) = \| x \|_2^2 / 2$ the Bregman divergence coincides with the Euclidean distance, for this
function $\psi$ the mirror descent method coincides with the gradient descent, as defined in Definition \ref{def4:grad-desc:grad-desc}.
While mirror descent has not seen broad use in the field of opinion dynamics as of yet, we believe that it can be utilized to
provide significant insight on the convergence properties of several models.



\section{Energy as a Generating Function} \label{Section4.5}

\par We conclude our presentation of fundamental techniques used to analyze opinion formation models with the concept of the
\textit{energy of a system}. The energy approach was developed as a way to study the convergence properties of complex models,
where step-by-step methods fail. In several systems, we may lack the guarantees about specific quantities changing between two
consecutive time steps. However, in these cases, we can use the concept of the system's energy as a generating function that bands
together several continuous time steps in order to observe a specific change in a property of the system. This change, for example,
could be a decrease in the distance from an equilibrium point or a decrease in a potential-like function of the model, if one exists.

\par The notion of a system's energy was first introduced by Chazelle \cite{Chazelle-Energy} to study a generalization of opinion
formation models, called \textit{influence systems}. Therefore, the technique, besides powerful, is also quite general and possibly
applicable on numerous models. Consequently, in our opinion, attempts to apply the ideas presented here on several variations of the
Hegselmann-Krause model and other non-linear models in general, would hold significant merit and may provide fascinating new results.

\subsection{Definition of the Total $s$-Energy} \label{Section4.5.1}

\par We begin by defining the concept of the \textit{total $s$-energy} of a multiagent system, introduced by Chazelle, who also proved
that its convergence for any real $s > 0$. \cite{Chazelle-Energy}.

\begin{definition} [Total $s$-Energy] \label{def4:energy:total}
Consider an infinite sequence of graphs $G_0, G_1, G_2,$ $\hdots$, where each $G_t$ has $n$ nodes labeled $1, 2, \hdots, n$, with each
node representing an agent. We assume the agents' opinions lie in a $d$-dimensional Euclidean space and denote agent $i$'s opinion at
time $t$ with $x_i(t)  \in \mathbb{R}^d$. Then, the \textit{total $s$-energy} of this system is defined as

\begin{equation} \label{eq4:energy:total}
E(s) = \sum_{t \geq 0} { \sum_{(i, j) \in G_t} { \| x_i(t) - x_j(t) \|_2^s} }
\end{equation}

where the exponent $s \in \mathbb{C}$ is, in the most general setting, a complex variable.
\end{definition}

\par We observe that $E(s)$ encodes all of the edge lengths for every $G_t$ in the graph sequence. We call this graph sequence that
shares the same nodes the \textit{communication network} of the system, and we make no assumptions about it in our definition above.
In fact, this model is so general, there is no obvious reason why $E(s)$ should ever converge, for any $s$. Later in this chapter we
provide a proof that $E(s)$ converges for any $s \in \mathbb{R}_+$. However, before we continue, we provide some general intuition
behind the concept of $s$-energy, in order to assist in the understanding of the ideas and results presented in this section.

\subsection{Bidirectional Systems}

\par Recall the constraints imposed on the communication network of the Network-HK model presented in Section \ref{Section3.1.1}. We
attempt to generalize them by defining \textit{bidirectional agreement systems} in general. Our definition introduces the model for
the $1$-dimensional case, however it contains all of the necessary ideas and the extension of the model to higher dimensions can be
done in many ways in a straightforward fashion.

\begin{definition} [Bidirectional Agreement System] \label{def4:energy:bidirect}
Consider $n$ agents expressing opinions $x_1(t), x_2(t), \hdots, x_n(t) \in \mathbb{R}$ at time $t$. The communication network consists
of an infinite sequence of graphs $G_0, G_1, G_2, \hdots$, with $G_t$ being a function of the system's configuration at times
$0, \hdots, t - 1$. Let $\mathcal{N}_i(t) = \{ j : (i, j) \in G \}$ denote the set of neighbors of $i$,
$m_i(t) = \min_{j \in \mathcal{N}_i(t)} {x_j(t)}$ the minimum and $M_i(t) = \max_{j \in \mathcal{N}_i(t)} {x_j(t)}$ the maximum opinion
in $i$'s neighborhood at time $t$. Also, let $0 < \rho \leq 1 / 2$ be an \textit{agreement parameter} that is time-invariant and
uniform for all agents. Then, in a \textit{bidirectional agreement system}, at time $t$ each agent $i$ moves to $x_i(t + 1)$, where

\begin{equation} \label{eq4:energy:bidirect}
(1 - \rho) m_i(t) + \rho M_i(t) \leq x_i(t + 1) \leq \rho m_i(t) + (1 - \rho) M_i(t)
\end{equation}
\end{definition}

\par Note that the model does not make any assumptions on the generation of each $G_t$, nor on their connectivity properties. It
is believed that bidirectional systems are the widest class of systems that allow for reasoning on their convergence properties,
since the general case of directed graphs precludes such analysis. In addition, note that the model is nondeterministic, with the
communication network and the agents' motion being completely arbitrary, and it does not imply symmetry among neighbors. Indeed,
the set of constraints that Definition \ref{def4:energy:bidirect} imposes on agent behavior is fairly weaker than the usual set
of constraints associated with bidirectional models.

\par Recall that in non-linear systems, $\bm{x}(t+1) = \bm{A}(t) \bm{x}(t)$, where $\bm{A}$ is a row-stochastic matrix with positive
entries $a_{ij}(t)$ for all $i, j$ where $j \in \mathcal{N}_i(t)$. Then, if we denote by $l_i$ the leftmost and by $r_i$ the rightmost
neighbor of $i$, we get a set of two constraints, which follow directly from (\ref{eq4:energy:bidirect})

\begin{itemize}
\item Mutual confidence: Pairs of $a_{ij}, a_{ji}$ do not have exactly one zero; they are either both positive or both zero.

\item No extreme influence: For any agent $i$ that is not fully-stubborn, $\max\{ a_{i l_i}(t), a_{i r_i}(t) \}$ $\leq 1 - \rho$.
\end{itemize}

\par We immediately see that the above conditions are weaker than those of Section \ref{Section3.1.1}. Intuitively, for
bidirectional systems to converge asymptotically, agents may be influenced a lot by non-extreme positions but must be the
influence that extreme positions exert upon them must be bounded. It has been shown that such bidirectional systems converge
asymptotically \cite{Hendrickx-Blondel-Bidi, Moreau, Lorenz-2005}, and the convergence rate is bounded by
$\rho^{- \mathcal{O}(n)}$ \cite{Chazelle-Energy}.

\subsection{Bounds on the Total $s$-Energy}

\par It is easy to see that understanding opinion formation models, or agreement systems, in their most general setting is
equivalent to understanding backward products of stochastic matrices

$$
\bm{A}(t) \bm{A}(t-1) \hdots \bm{A}(0)
$$

which relate to time-inhomogeneous Markov chains. Although not much is known about such Markov chains, we can make some interesting
observations here. Note that if a product such as the one above converges, then as time goes by, the product will tend to a matrix
of rank one.

\par To see why this is true, consider the following geometric interpretation. Each row of $\bm{A}(0)$ corresponds to a
specific point in $\mathbb{R}^n$. Construct a convex polytope equal to the convex hull of all these points, denoted by
$conv(\bm{A}(0))$. When $\bm{A}(0)$ is multiplied by $\bm{A}(1)$, each row of the product is a convex combination of the rows of
$\bm{A}(0)$, hence each point of the product is a convex combination of the points specified by $\bm{A}(0)$ and lies inside the
convex hull $conv(\bm{A}(0))$. Therefore, $conv(\bm{A}(1) \bm{A}(0)) \subseteq conv(\bm{A}(0))$. Repeating the process, we get a
sequence of convex polytopes

$$
conv(\bm{A}(t) \hdots \bm{A}(0)) \subseteq \hdots \subseteq conv(\bm{A}(1) \bm{A}(0)) \subseteq conv(\bm{A}(0))
$$

that is decreasing in volume. Recall Definition \ref{def3:net-hk:coe} of the coefficient of ergodicity and observe that it
essentially is a measure of how fast this sequence of convex polytopes decreases in volume, and the matrices approach a matrix
of rank one. However, it is a local tool in the sense that it analyzes the model in a step-by-step fashion. In contrast, the notion
of the total $s$-energy presented in this section is a global tool; it monitors the decrease of this sequence over all time steps,
with parameter $s$ essentially playing the role of frequency in Fourier analysis.

\par As stated before, there is no obvious reason as to why the total $s$-energy of a system should ever converge. However,
Chazelle proved in 2010 that, for bidirectional agreement systems where all agents express opinions in $[0, 1]^d$, the total
$s$-energy is bounded for any real $s > 0$ and also provided an upper bound on the convergence rate \cite{Chazelle-Energy}. We
present these two theorems here for the $1$-dimensional case, and we refer to Chazelle's paper for the proofs.

\begin{theorem} \label{theor4:energy:total-conv}
Consider a $n$-agent bidirectional agreement system where each agent expresses an opinion in $[0, 1]$, and let $E_n(s)$ denote
the maximum value of the total $s$-energy, over all times and all $n$-node graph sequences. Then,

\begin{equation} \label{eq4:energy:total-conv}
E_n(s) \leq \begin{cases}
\rho^{- \mathcal{O}(n)} & for \: \: s = 1. \\
s^{1-n} \rho^{-n^2-\mathcal{O}(1)} & for \: \: 0 < s < 1
\end{cases}
\end{equation}

Since no edge length exceeds $1$, $E_n(s) \leq E_n(1)$ for $s \geq 1$, therefore $E_n(s)$ is bounded from above for any real $s > 0$.
\end{theorem}

\par However, if we attempt to bound the convergence rate of the system, we are immediately faced with the obvious difficulty that,
in an adversarial setting, an adversary can always provide us with a graph $G_t$ that is an independent set for as long as it wants
and then at some time into the future connect all edges permanently to the network in order to make the agents reach consensus. We
circumvent this difficulty by defining notion of asymptotic convergence. Specifically, given $0 < \varepsilon < 1 / 2$, we say that
step $t$ is \textit{trivial} if all the edges in $G_t$ have length at most $\varepsilon$. We then proceed to bound the \textit{
communication count} $C_\varepsilon$, defined as the total number of non-trivial steps for a given $\varepsilon$. Intuitively, this
notion of convergence ignores microscopic motions of the agents and specifies defines convergence as the event where all agents
have formed independent clusters of radius $\varepsilon$. From a macroscopic point of view, this notion implies that the system
eventually freezes.

\begin{theorem} \label{theor4:energy:total-comm-cnt}
Consider a $n$-agent bidirectional agreement system where each agent expresses an opinion in $[0, 1]$. Then, the maximum communication
count is bounded by above

\begin{equation} \label{eq4:energy:total-comm-cnt}
C_\varepsilon \leq \min \Bigl\{ \frac{1}{\varepsilon} \rho^{- \mathcal{O}(n)}, (log\frac{1}{\varepsilon})^{n-1} \rho^{-n^2-\mathcal{O}(1)} \Bigr\}
\end{equation}
\end{theorem}

\subsection{Kinetic $s$-Energy} \label{Section4.5.2}

\par While the concept of the total $s$-energy is extremely useful for macroscopic analysis of agreement systems, sometimes variants
of the concept are more convenient, and such is the case with opinion formation models. In particular, we define the \textit{
kinetic $s$-energy} of a system as follows

\begin{definition} [Kinetic $s$-Energy] \label{def4:energy:kinetic}
Consider an infinite sequence of graphs $G_0, G_1, G_2,$ $\hdots$, where each $G_t$ has $n$ nodes labeled $1, 2, \hdots, n$, with each
node representing an agent. We assume the agents' opinions lie in a $d$-dimensional Euclidean space and denote agent $i$'s opinion at
time $t$ with $x_i(t)  \in \mathbb{R}^d$. Then, the \textit{kinetic $s$-energy} of this system is defined as

\begin{equation} \label{eq4:energy:kinetic}
K(s) = \sum_{t \geq 0} \sum_{i = 1}^n {\| x_i(t+1) - x_i(t) \|_2^s}
\end{equation}
\end{definition}

\par Recall our use of the kinetic $s$-energy for $s = 2$ in Section \ref{Section3.3}, to prove the convergence of the HK model
with fully-stubborn agents. Such applications of the concept demonstrate the usefulness of this mathematical tool in the analysis
of opinion formation models and their convergence properties. We believe that, when applied correctly, the concept of the kinetic
$s$-energy can simplify and, most importantly, generalize existent proofs of convergence on several models, as well as provide new
insight on the convergence properties of models we know very little about.

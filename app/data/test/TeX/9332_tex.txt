\documentclass{beamer}
\usetheme{metropolis}           % Use metropolis theme
\usepackage{appendixnumberbeamer}
\usepackage{epigraph}
\usepackage{color}
\usepackage{amsopn}
\usepackage{tabto}

%%% Bibliography
\usepackage[backend=bibtex, style=authoryear]{biblatex}
\AtBeginBibliography{\tiny}
\bibliography{../bibliography.bib}

\setbeamercolor{background canvas}{bg=white}
\setbeamercolor{title}{fg=white}
\setbeamercolor{subtitle}{fg=black}
\setbeamercolor{author}{fg=black}
\setbeamercolor{institute}{fg=white}

\newcommand{\todo}{\alert{TODO}}
\newcommand{\itemBullet}{\scriptsize$\blacksquare$}
\setbeamertemplate{itemize item}{\itemBullet}
\setbeamertemplate{itemize subitem}{\itemBullet}
\setbeamertemplate{itemize subsubitem}{\itemBullet}
\newcommand{\E}{\mathop{\mathbb{E}}}
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\epiParSpace}{\vskip 1.5ex}

\title{AlphaGo: Mastering the game of~Go}
\subtitle{with deep neural networks and tree search}
\date{}                         % no dates
\author{Karel Ha \\ article by Google DeepMind}
\institute{ETH Z\"{u}rich, 9\textsuperscript{th} May 2016}

\begin{document}
  {
    \usebackgroundtemplate{
      \includegraphics[height=\paperheight]{../img/Go_background.jpg}
    }
    \maketitle
  }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  \section{Game AIs}

  {
    \setbeamertemplate{frame footer}{\color{gray}\url{https://xkcd.com/1002/}}
    \begin{frame}[standout]
      \includegraphics[height=\paperheight]{../img/game_AIs.png}
    \end{frame}
  }

  {
    \setbeamertemplate{frame footer}{\cite{Bowling2015heads}}
    \begin{frame}{Heads-up Limit Hold’em Poker Is Solved!}
      \begin{center}
        \includegraphics[width=\textwidth, keepaspectratio]{../img/limit_holdem_poker.jpg}
        \pause

        \textbf{Cepheus} \url{http://poker.srv.ualberta.ca/} \\
        \pause
        {\tiny 0.000986 big blinds per game on expectation}
      \end{center}
    \end{frame}
  }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  \section{Game Tree}
  {
    \setbeamertemplate{frame footer}{\cite{Silver2016mastering}}
    \begin{frame}{Tree Search}
      Optimal value~$v^*(s)$ determines the~outcome of~the game:
      \pause
      \begin{itemize}[<+- | alert@+>]
          \tiny
        \item from every board position or state $s$
        \item under perfect play by~all players.
      \end{itemize}
      \pause

      It is computed by~\textbf{recursively traversing a~search tree} containing approximately $b^d$ possible sequences of moves, where
      \pause
      \begin{itemize}[<+- | alert@+>]
          \tiny
        \item $b$ is the game’s breadth (number of legal moves per position)
        \item $d$ is its depth (game length).
      \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{frame footer}{\cite{Allis1994searching}}
    \begin{frame}{Game tree of~Go}
      Sizes of~trees for~various games:
      \begin{itemize}
        \item chess: $b \approx 35, d \approx 80$
        \item Go: $b \approx 250, d \approx 150$
          \pause
          $\Rightarrow$ more positions than atoms in the universe!
      \end{itemize}
      \pause

      \epigraph{
        That makes Go a \textbf{googol} [$10^{100}$] times more complex than chess.
      }{\tiny\url{https://deepmind.com/alpha-go.html}}
      \pause

      \vskip -2em
      How to handle the size~of the game tree?
      \pause
      \begin{itemize}[<+- | alert@+>]
        \item for the breadth: a~neural network to~select moves 
        \item for the depth: a~neural network to~evaluate the current position
        \item for the tree traverse: Monte Carlo tree search (MCTS)
      \end{itemize}
    \end{frame}
  }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  \section{AlphaGo: Neural Networks}
  {
    \setbeamertemplate{frame footer}{\cite{Silver2016mastering}}
    \begin{frame}{Policy and Value Networks}
      \begin{center}
        \includegraphics[height=.85\textheight]{../img/policy_and_value_network.png}
      \end{center}
    \end{frame}

    \begin{frame}{Training the (Deep Convolutional) Neural Networks}
      \begin{center}
        \includegraphics[width=\textwidth]{../img/neural_nets_pipeline.png}
      \end{center}
    \end{frame}

    \begin{frame}{SL (Supervised Learning) Policy Network}
      \begin{itemize}[<+- | alert@+>]
        \item 13-layer deep convolutional neural network
        \item goal: to~predict expert human moves
        \item task of \textbf{classification}
        \item trained from 30 millions positions from the KGS Go Server
        \item stochastic gradient ascent:
          \[
            \Delta \sigma \propto \frac{\partial \log p_\sigma (a|s)}{\partial \sigma}
          \]
          {\tiny (to~maximize the likelihood of~the human move~$a$ selected in~state~$s$)}
      \end{itemize}
      \pause

      Results:
      \pause
      \begin{itemize}[<+- | alert@+>]
        \item $44.4\%$ accuracy (the state-of-the-art from other groups)
        \item $55.7\%$ accuracy (raw board position + move history as~input)
        \item $57.0\%$ accuracy (all input features)
      \end{itemize}
    \end{frame}

    \begin{frame}{Training the (Deep Convolutional) Neural Networks}
      \begin{center}
        \includegraphics[width=\textwidth]{../img/neural_nets_pipeline.png}
      \end{center}
    \end{frame}

    \begin{frame}{Rollout Policy}
      \begin{itemize}[<+- | alert@+>]
        \item Rollout policy~$p_\pi(a|s)$ is \textbf{faster} but \textbf{less accurate} than SL policy network.
        \item accuracy of~$24.2\%$
        \item It takes $2 \mu$s to~select an~action, compared to $3$~ms in~case of~SL policy network.
      \end{itemize}
    \end{frame}

    \begin{frame}{Training the (Deep Convolutional) Neural Networks}
      \begin{center}
        \includegraphics[width=\textwidth]{../img/neural_nets_pipeline.png}
      \end{center}
    \end{frame}

    \begin{frame}{RL (Reinforcement Learning) Policy Network (1/2)}
      \begin{itemize}[<+- | alert@+>]
        \item identical in~structure to the SL policy network
        \item goal: to~win in~the games of~self-play
        \item task of \textbf{classification}
        \item weights $\rho$ initialized to the same values, $\rho := \sigma$
        \item games of self-play
          \begin{itemize}[<+- | alert@+>]
            \item between the current RL policy network and a~randomly selected previous iteration
            \item to prevent overfitting to~the current policy
          \end{itemize}
        \item stochastic gradient ascent:
          \[
            \Delta \rho \propto \frac{\partial \log p_\rho (a_t|s_t)}{\partial \rho} z_t
          \]
          {\tiny at~time step~$t$, where reward function~$z_t$ is $+1$ for winning and $-1$ for losing.}
      \end{itemize}
      \pause
    \end{frame}

    \begin{frame}{RL (Reinforcement Learning) Policy Network (2/2)}
      Results (retrieved by sampling each move $a_t \sim p_\rho(\cdot | s_t)$):
      \pause
      \begin{itemize}[<+- | alert@+>]
        \item $80\%$ of~win rate against the SL policy network
        \item $85\%$ of~win rate against the strongest open-source Go program, \textbf{Pachi} (\cite{Baudivs2011pachi})
          \begin{itemize}[<+- | alert@+>]
            \item The previous state-of-the-art, based only on~SL of~CNN: \\
              $11\%$ of~``win'' rate against Pachi
          \end{itemize}
      \end{itemize}
    \end{frame}

    \begin{frame}{Training the (Deep Convolutional) Neural Networks}
      \begin{center}
        \includegraphics[width=\textwidth]{../img/neural_nets_pipeline.png}
      \end{center}
    \end{frame}

    \begin{frame}{Value Network (1/2)}
      \begin{itemize}[<+- | alert@+>]
        \item similar architecture to the policy network, but outputs a~single prediction instead of~a~probability distribution
        \item goal: to estimate a~value function
          \[
            v^p(s) = \E [z_t | s_t = s, a_{t \dots T} \sim p]
          \]
          that predicts the outcome from position~$s$ (of~games played by~using policy $p$)
        \item Double approximation: $v_\theta(s) \approx v^{p_\rho}(s) \approx v^*(s)$.
        \item task of~\textbf{regression}
        \item stochastic gradient descent:
          \[
            \Delta \theta \propto \frac{\partial v_\theta (s)}{\partial \theta} (z - v_\theta(s))
          \]
          {\tiny (to~minimize the mean squared error (MSE) between the predicted $v_\theta(s)$ and the true $z$)}
      \end{itemize}
    \end{frame}

    \begin{frame}{Value Network (2/2)}
      Beware of~overfitting!
      \pause
      \begin{itemize}[<+- | alert@+>]
        \item Consecutive positions are strongly correlated.
        \item Value network memorized the game outcomes, rather than generalizing to new positions.
        \item Solution: generate 30 million (new) positions, each sampled from a~\textbf{seperate} game
        \item almost the accuracy of~Monte Carlo rollouts (using $p_\rho$), but $15000$ times less computation!
      \end{itemize}
    \end{frame}

    \begin{frame}{Elo Ratings for~Various Combinations of~Networks}
      \begin{center}
        \includegraphics[height=.85\textheight]{../img/ELO_ratings_various_combinations_of_ANNs.png}
      \end{center}
    \end{frame}
  }

  \section{AlphaGo: The Main Algorithm}
  {
    \setbeamertemplate{frame footer}{\cite{Silver2016mastering}}

    \begin{frame}{MCTS Algorithm}
      The next action is selected by~lookahead search, using simulation:
      \pause
      \begin{enumerate}[<+- | alert@+>]
        \item selection phase
        \item expansion phase
        \item evaluation phase
        \item backup phase (at~end of~all simulations)
      \end{enumerate}
      \pause

      Each edge $(s, a)$ keeps:
      \begin{itemize}[<+- | alert@+>]
        \item action value~$Q(s, a)$
        \item visit count~$N(s, a)$
        \item prior probability~$P(s, a)$ (from SL policy network $p_\sigma$)
      \end{itemize}
      \pause

      The tree is traversed by simulation (descending the tree) from the root state.
    \end{frame}

    \begin{frame}{MCTS Algorithm: Selection}
      \begin{center}
        \includegraphics[height=.6\textheight]{../img/MCTS_selection.png}
      \end{center}
      \pause

      \tiny
      At each time step~$t$, an~action~$a_t$ is selected from state $s_t$
      \[
        a_t = \argmax _a ( Q(s_t, a) + u(s_t, a) )
      \]
      \pause
      where bonus
      \[
        u(s_t, a) \propto \frac{P(s,a)}{1 + N(s,a)}
      \]
    \end{frame}

    \begin{frame}{MCTS Algorithm: Expansion}
      \begin{center}
        \includegraphics[height=.6\textheight]{../img/MCTS_expansion.png}
      \end{center}
      \pause

      \tiny
      A~leaf position may be expanded (just once) by the SL policy network~$p_\sigma$.
      \pause

      The output probabilities are stored as priors $P(s,a) := p_\sigma (a|s)$.
    \end{frame}

    \begin{frame}{MCTS: Evaluation}
      \begin{center}
        \includegraphics[height=.6\textheight]{../img/MCTS_evaluation.png}
      \end{center}
      \pause

      \tiny
      \begin{itemize}[<+- | alert@+>]
        \item evaluation from the value network $v_\theta (s)$
        \item evaluation by the outcome~$z$ using the fast rollout policy $p_\pi$ until the~end of~game
      \end{itemize}
      \pause
      Using a~mixing parameter~$\lambda$, the final leaf evaluation $V(s)$ is
      \[
        V(s) = (1 - \lambda) v_\theta(s) + \lambda z
      \]
    \end{frame}

    \begin{frame}{MCTS: Backup}
      \begin{center}
        \includegraphics[height=.6\textheight]{../img/MCTS_backup.png}
      \end{center}
      
      \tiny
      At the end of~simulation, each traversed edge is \textbf{updated} by accumulating:
        \begin{itemize}[<+- | alert@+>]
          \item the action values $Q$
          \item visit counts $N$
        \end{itemize}
    \end{frame}

    \begin{frame}[standout]
      Once the search is complete, the algorithm chooses \alert{the most visited move} from the root position.
    \end{frame}

    \begin{frame}{Principal Variation (Path with Maximum Visit Count)}
      \begin{center}
        \includegraphics[height=.65\textheight]{../img/principal_variation.png}

        \tiny
        The moves are presented in a numbered sequence.
      \end{center}
      \pause

      \vskip -1ex
      \begin{tiny}
        \begin{itemize}[<+- | alert@+>]
          \item AlphaGo selected the move indicated by the red circle;
          \item Fan Hui responded with the move indicated by the white square;
          \item in his post-game commentary, he preferred the move (labelled 1) predicted by AlphaGo.
        \end{itemize}
      \end{tiny}
      \vskip 1.45em
    \end{frame}
  }

  \section{AlphaGo: Distributed Computing}
  {
    \setbeamertemplate{frame footer}{\cite{Silver2016mastering}}
    \begin{frame}{Scalability}
      \begin{itemize}[<+- | alert@+>]
        \item asynchronous multi-threaded search
        \item simulations on~CPUs
        \item computation of~neural networks on GPUs
      \end{itemize}
      \pause

      AlphaGo (on a~single-machine):
      \begin{itemize}[<+- | alert@+>]
        \item 40 search threads
        \item 40 CPUs
        \item 8 GPUs
      \end{itemize}
      \pause

      Distributed version of AlphaGo (on~multiple machines):
      \pause
      \begin{itemize}[<+- | alert@+>]
        \item 40 search threads
        \item 1202 CPUs
        \item 176 GPUs
      \end{itemize}
    \end{frame}

    \begin{frame}{Elo Ratings for~Various Combinations of~Threads}
      \begin{center}
        \vskip -1ex
        \includegraphics[height=.9\textheight]{../img/ELO_ratings_various_combinations_of_threads.png}
      \end{center}
    \end{frame}
  }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  \section{AlphaGo: Results}

  {
    \setbeamertemplate{frame footer}{\color{gray}\url{http://xkcd.com/1263/}}
    \begin{frame}[standout]
      \includegraphics[width=\textwidth]{../img/reassuring.png}
    \end{frame}
  }

  {
    \setbeamertemplate{frame footer}{\cite{Silver2016mastering}}
    \begin{frame}{Tournament with Other Go Programs}
      \begin{center}
        \vskip -1ex
        \includegraphics[height=.95\textheight]{../img/results_of_tournament.png}
      \end{center}
    \end{frame}
  }

  {
    \setbeamertemplate{frame footer}{\url{https://en.wikipedia.org/wiki/Fan_Hui}}
    \begin{frame}{Fan Hui}
      \begin{center}
        \includegraphics[width=.55\textwidth]{../img/Fan_Hui_profile.jpg}
      \end{center}
      \pause
      \vskip -1.6em
      \begin{itemize}[<+- | alert@+>]
        \item professional 2 dan
        \item European Go Champion in 2013, 2014 and 2015
        \item European Professional Go Champion in 2016 
        \item biological neural network:
          \begin{itemize}[<+- | alert@+>]
            \item 100 billion neurons
            \item 100 up to 1,000 trillion neuronal connections
          \end{itemize}
      \end{itemize}
    \end{frame}
  }

  \begin{frame}{AlphaGo versus Fan Hui}
    \pause
    \begin{center}
      \includegraphics[width=.55\textwidth]{../img/Fan_Hui_loses.jpg}
    \end{center}
    \textbf{AlphaGo won 5:0} in a~formal match in~October 2015.
    \pause

    \vskip -1em
    \epigraph{
      \tiny
      [AlphaGo] is very strong and stable, it seems like a~wall.
      ...
      I know AlphaGo is a~computer, but if no one told me, maybe I would think the player was a~little strange, but a~very strong player, a~real person.
    }{Fan Hui}
  \end{frame}

  {
    \setbeamertemplate{frame footer}{\url{https://en.wikipedia.org/wiki/Lee_Sedol}}
    \begin{frame}{Lee Sedol ``The Strong Stone''}
      \begin{center}
        \includegraphics[width=.55\textwidth]{../img/Lee_Sedol_profile.jpg}
      \end{center}
      \pause
      \vskip -1.6em
      \begin{itemize}[<+- | alert@+>]
        \item professional 9 dan 
        \item the $2^{nd}$ in international titles
        \item the $5^{th}$ youngest (12 years 4 months) to become a~professional Go player in~South Korean history
        \item Lee Sedol would win 97 out of~100 games against Fan Hui.
        \item biological neural network comparable to Fan Hui's (in~number of~neurons and connections)
      \end{itemize}
    \end{frame}
  }

  {
    \usebackgroundtemplate{
      \includegraphics[height=\paperheight]{../img/Lee_Sedol_quotes.jpg}
    }
    \begin{frame}[standout]
      \epigraph{
        \tiny
        I heard Google DeepMind's AI is surprisingly strong and getting stronger, but I am confident that I can win, at least this time.
      }{Lee Sedol}
      \pause

      \epigraph{
        \tiny
        ...even beating AlphaGo by 4:1 may allow the Google DeepMind team to claim its de facto victory and the defeat of him [Lee~Sedol], or even humankind.
      }{interview in JTBC Newsroom}
      \pause
    \end{frame}
  }

  {
    \usebackgroundtemplate{
      \includegraphics[height=\paperheight]{../img/Lee_Sedol_after_match.jpg}
    }
    \setbeamertemplate{frame footer}{\color{white}\url{https://en.wikipedia.org/wiki/AlphaGo_versus_Lee_Sedol}}
    \begin{frame}{AlphaGo versus Lee Sedol}
      \pause

      \vskip 1em
      \color{white}
      In March 2016 \textbf{AlphaGo won 4:1} against the legendary Lee Sedol.
      \pause

      AlphaGo won all but the $4^{th}$ game; all games were won by~resignation.
      \pause

      The winner of~the match was slated to win \$1 million.
      \pause

      Since AlphaGo won, Google DeepMind stated that the prize will be donated to~charities, including~UNICEF, and Go organisations.
      \pause

      Lee received \$170,000 (\$150,000 for participating in all the five games, and an additional \$20,000 for each game won).
    \end{frame}
  }

  \begin{frame}[standout]
    Who's next?
  \end{frame}
  
  {
    \setbeamertemplate{frame footer}{\color{gray} \url{http://www.goratings.org/} (7\textsuperscript{th} May 2016)}
    \begin{frame}[standout]
      \includegraphics[width=\textwidth]{../img/Go_ratings_2016_05_07.png}
    \end{frame}
  }

  {
    \setbeamertemplate{frame footer}{\url{https://en.wikipedia.org/wiki/Ke_Jie}}
    \begin{frame}{AlphaGo versus Ke Jie?}
      \begin{center}
        \includegraphics[width=.55\textwidth]{../img/Ke_Jie_profile.jpg}
      \end{center}
      \pause
      \vskip -1.6em
      \begin{itemize}[<+- | alert@+>]
        \item professional 9 dan 
        \item the $1^{st}$ in (unofficial) world ranking list
        \item the youngest player to win 3 major international tournaments
        \item head-to-head record against Lee Sedol \textbf{8:2}
        \item biological neural network comparable to Fan Hui's, and thus by~transitivity, also comparable to~Lee Sedol's
      \end{itemize}

    \end{frame}
  }

  {
    \usebackgroundtemplate{
      \includegraphics[height=\paperheight]{../img/Ke_Jie_quotes.jpg}
    }
    \begin{frame}[standout, noframenumbering]
      \epigraph{
        \tiny
        I believe I can beat it.
        Machines can be very strong in many aspects but still have loopholes in certain calculations.
      }{Ke Jie}
      \pause

      \epigraph{
        \tiny
        Now facing AlphaGo, I do not feel the same strong instinct of victory when I play a human player, but I still believe I have the advantage against it.
        It's 60 percent in favor of me.
      }{Ke Jie}
      \pause

      \epigraph{
        \tiny
        Even though AlphaGo may have defeated Lee Sedol, it won't beat me.
      }{Ke Jie}
    \end{frame}
  }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  \section{Conclusion}

  {
    \setbeamertemplate{frame footer}{\cite{Silver2016mastering}}
    \begin{frame}{Difficulties of~Go}
      \begin{itemize}[<+- | alert@+>]
        \item challenging decision-making
        \item intractable search space
        \item complex optimal solution

          {\tiny It appears infeasible to directly approximate using a~policy or value function!}
      \end{itemize}
    \end{frame}

    \begin{frame}{AlphaGo: summary}
      \begin{itemize}[<+- | alert@+>]
        \item Monte Carlo tree search
        \item effective move selection and position evaluation 
          \begin{itemize}[<+- | alert@+>]
            \item through deep convolutional neural networks
            \item trained by novel combination of~supervised and reinforcement learning
          \end{itemize}
        \item new search algorithm combining
          \begin{itemize}[<+- | alert@+>]
            \item neural network evaluation
            \item Monte Carlo rollouts
          \end{itemize}
        \item scalable implementation
          \begin{itemize}[<+- | alert@+>]
            \item multi-threaded simulations on CPUs
            \item parallel GPU computations
            \item distributed version over multiple machines
          \end{itemize}
      \end{itemize}
    \end{frame}

    \begin{frame}{Novel approach}
      \pause
      During the match against Fan Hui, AlphaGo evaluated \textbf{thousands of~times fewer} positions than Deep~Blue against Kasparov.
      \pause

      It compensated this by:
      \begin{itemize}[<+- | alert@+>]
        \item selecting those positions \textbf{more intelligently} (policy network)
        \item evaluating them \textbf{more precisely} (value network)
      \end{itemize}
      \pause

      Deep~Blue relied on a~handcrafted evaluation function.
      \pause

      AlphaGo was trained \textbf{directly and automatically} from gameplay.
      It used \textbf{general-purpose} learning.
      \pause

      This approach is not specific to the game of Go.
      The algorithm can be used \textbf{for much wider class} of~(so far seemingly) intractable problems in~AI!
    \end{frame}
  }
  
  {
    \usebackgroundtemplate{
      \includegraphics[height=\paperheight]{../img/ABBA.jpg}
    }
    \begin{frame}[standout]
      \begin{center}
        \Huge
        \color{gray!99}
        \alert{Thank you} \\
        \pause
        \alert{for the music} \\
        \pause
        of~your attention!
        \pause

        \vskip 1em
        Happy to be here, \\
        \pause
        in your \alert{DISCO group!}
      \end{center}
    \end{frame}
  }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  \appendix

  \begin{frame}[standout]
    Backup Slides
  \end{frame}

  {
    \setbeamertemplate{frame footer}{\cite{Silver2016mastering}}

    \begin{frame}{SL Policy Network: Accuracy vs. Win Rate}
      Small improvements in~accuracy led to~large improvements in~playing strength
      \begin{center}
        \includegraphics[width=\textwidth]{../img/SL_policy_accuracy_vs_win_rate.png}
      \end{center}
    \end{frame}

    \begin{frame}{Evaluation Accuracy in~Various Stages of a~Game}
      \includegraphics[width=\textwidth]{../img/policies_move_numbers_vs_MSE.png}

      \vskip -2.4ex
      {\tiny
      \textbf{Move number} is the number of~moves that had been played in the given position.
      }

      Each position evaluated by:
      \begin{itemize}
        \item forward pass of the value network~$v_\theta$
        \item 100 rollouts, played out using the corresponding policy
      \end{itemize}
    \end{frame}

    \begin{frame}{Input features for rollout and tree policy}
      \includegraphics[width=\textwidth]{../img/input_features_for_AlphaGo.png}
    \end{frame}

    \begin{frame}{Selection of~Moves by the SL Policy Network}
      \begin{center}
        \includegraphics[height=.8\textheight]{../img/eval_SL_policy_network.png}

        \tiny
        move probabilities taken directly from the SL policy network $p_\sigma$ (reported as a~percentage if above $0.1\%$).
      \end{center}
    \end{frame}

    \begin{frame}{Selection of~Moves by the Value Network}
      \begin{center}
        \includegraphics[height=.8\textheight]{../img/move_selection_by_value_network.png}

        \tiny
        evaluation of~all successors $s'$ of~the root position~$s$, using~$v_\theta(s′)$
      \end{center}
    \end{frame}

    \begin{frame}{Tree Evaluation from Value Network}
      \begin{center}
        \includegraphics[height=.82\textheight]{../img/tree_eval_from_value_network.png}

        \tiny
        action values~$Q(s, a)$ for~each tree-edge~$(s, a)$ from root position~$s$ (averaged over value network evaluations only)
      \end{center}
    \end{frame}

    \begin{frame}{Tree Evaluation from Rollouts}
      \begin{center}
        \includegraphics[height=.82\textheight]{../img/tree_eval_from_rollouts.png}

        \tiny
        action values $Q(s, a)$, averaged over rollout evaluations only
      \end{center}
    \end{frame}

    \begin{frame}{Percentage of~Simulations}
      \begin{center}
        \includegraphics[height=.8\textheight]{../img/percentage_of_simulations.png}

        \tiny
        percentage frequency with which actions were selected from the root during simulations
      \end{center}
    \end{frame}

    \begin{frame}{Results of a tournament between different Go programs}
      \includegraphics[width=\textwidth]{../img/results_of_the_tournament.png}
    \end{frame}

    \begin{frame}{Results of a tournament between AlphaGo and distributed AlphaGo, testing scalability with hardware}
      \includegraphics[width=\textwidth]{../img/results_of_scalability_tests.png}
    \end{frame}

    \begin{frame}{{\color{white}\underline{AlphaGo}} versus {\color{black}Fan Hui}: Game 1}
      \begin{center}
        \includegraphics[height=.9\textheight]{../img/AlphaGo_vs_Fan_Hui_Game_1.png}
      \end{center}
    \end{frame}

    \begin{frame}{{\color{black}\underline{AlphaGo}} versus {\color{white}Fan Hui}: Game 2}
      \begin{center}
        \includegraphics[height=.9\textheight]{../img/AlphaGo_vs_Fan_Hui_Game_2.png}
      \end{center}
    \end{frame}

    \begin{frame}{{\color{white}\underline{AlphaGo}} versus {\color{black}Fan Hui}: Game 3}
      \begin{center}
        \includegraphics[height=.9\textheight]{../img/AlphaGo_vs_Fan_Hui_Game_3.png}
      \end{center}
    \end{frame}

    \begin{frame}{{\color{black}\underline{AlphaGo}} versus {\color{white}Fan Hui}: Game 4}
      \begin{center}
        \includegraphics[height=.9\textheight]{../img/AlphaGo_vs_Fan_Hui_Game_4.png}
      \end{center}
    \end{frame}

    \begin{frame}{{\color{white}\underline{AlphaGo}} versus {\color{black}Fan Hui}: Game 5}
      \begin{center}
        \includegraphics[height=.9\textheight]{../img/AlphaGo_vs_Fan_Hui_Game_5.png}
      \end{center}
    \end{frame}
  }

  {
    \setbeamertemplate{frame footer}{\url{https://en.wikipedia.org/wiki/AlphaGo_versus_Lee_Sedol}}

    \begin{frame}{{\color{white}\underline{AlphaGo}} versus {\color{black}Lee Sedol}: Game 1}
      \begin{center}
        \includegraphics[width=\textwidth]{../img/AlphaGo_vs_Lee_Sedol_Game_1.png}

        \url{https://youtu.be/vFr3K2DORc8}
      \end{center}
    \end{frame}

    \begin{frame}{{\color{black}\underline{AlphaGo}} versus {\color{white}Lee Sedol}: Game 2 (1/2)}
      \begin{center}
        \includegraphics[width=\textwidth]{../img/AlphaGo_vs_Lee_Sedol_Game_2a.png}

        \url{https://youtu.be/l-GsfyVCBu0}
      \end{center}
    \end{frame}

    \begin{frame}{{\color{black}\underline{AlphaGo}} versus {\color{white}Lee Sedol}: Game 2 (2/2)}
      \begin{center}
        \includegraphics[height=.85\textheight]{../img/AlphaGo_vs_Lee_Sedol_Game_2b.png}
      \end{center}
    \end{frame}

    \begin{frame}{{\color{white}\underline{AlphaGo}} versus {\color{black}Lee Sedol}: Game 3}
      \begin{center}
        \includegraphics[width=\textwidth]{../img/AlphaGo_vs_Lee_Sedol_Game_3.png}

        \url{https://youtu.be/qUAmTYHEyM8}
      \end{center}
    \end{frame}

    \begin{frame}{{\color{black}AlphaGo} versus {\color{white}\underline{Lee Sedol}}: Game 4}
      \begin{center}
        \includegraphics[width=\textwidth]{../img/AlphaGo_vs_Lee_Sedol_Game_4.png}

        \url{https://youtu.be/yCALyQRN3hw}
      \end{center}
    \end{frame}

    \begin{frame}{{\color{white}\underline{AlphaGo}} versus {\color{black}Lee Sedol}: Game 5 (1/2)}
      \begin{center}
        \includegraphics[width=\textwidth]{../img/AlphaGo_vs_Lee_Sedol_Game_5a.png}

        \url{https://youtu.be/mzpW10DPHeQ}
      \end{center}
    \end{frame}

    \begin{frame}{{\color{white}\underline{AlphaGo}} versus {\color{black}Lee Sedol}: Game 5 (2/2)}
      \begin{center}
        \includegraphics[height=.85\textheight]{../img/AlphaGo_vs_Lee_Sedol_Game_5b.png}
      \end{center}
    \end{frame}
  }

  \begin{frame}[allowframebreaks]{Further Reading}
    \tiny
    AlphaGo:
    \begin{itemize}
      \item \textbf{Google Research Blog} \url{http://googleresearch.blogspot.cz/2016/01/alphago-mastering-ancient-game-of-go.html}
      \item an article in \textbf{Nature} \url{http://www.nature.com/news/google-ai-algorithm-masters-ancient-game-of-go-1.19234}
      \item a \textbf{reddit} article claiming that AlphaGo is even stronger than it appears to be: \\
        ``AlphaGo would rather win by less points, but with higher probability.'' \\
        \url{https://www.reddit.com/r/baduk/comments/49y17z/the_true_strength_of_alphago/}
      \item a~video of~how AlphaGo works (put in layman's terms) \url{https://youtu.be/qWcfiPi9gUU}
    \end{itemize}

    Articles by Google DeepMind:
    \begin{itemize}
      \item \textbf{Atari player}: a DeepRL system which combines Deep Neural Networks with Reinforcement Learning (\cite{Mnih2015human})
      \item \textbf{Neural Turing Machines} (\cite{Graves2014neural})
    \end{itemize}

    Artificial Intelligence:
    \begin{itemize}
      \item \textbf{Artificial Intelligence course at MIT} \url{http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-034-artificial-intelligence-fall-2010/index.htm}
      \item \textbf{Introduction to Artificial Intelligence at Udacity} \url{https://www.udacity.com/course/intro-to-artificial-intelligence--cs271}
      \item \textbf{General Game Playing course} \url{https://www.coursera.org/course/ggp}
      \item \textbf{Singularity} \url{http://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html} + Part 2
      \item \textbf{The Singularity Is Near} (\cite{Kurzweil2005singularity})
    \end{itemize}

    Combinatorial Game Theory (founded by John H. Conway to study endgames in Go):
    \begin{itemize}
      \item \textbf{Combinatorial Game Theory course} \url{https://www.coursera.org/learn/combinatorial-game-theory}
      \item On Numbers and Games (\cite{Conway1976number})
      \item Computer Go as a sum of local games: an application of combinatorial game theory (\cite{Muller1995computer})
    \end{itemize}

    Chess:
    \begin{itemize}
      \item \textbf{Deep Blue beats G. Kasparov in 1997} \url{https://youtu.be/NJarxpYyoFI}
    \end{itemize}

    Machine Learning:
    \begin{itemize}
      \item \textbf{Machine Learning course} \url{https://youtu.be/hPKJBXkyTK://www.coursera.org/learn/machine-learning/}
      \item \textbf{Reinforcement Learning} \url{http://reinforcementlearning.ai-depot.com/}
      \item \textbf{Deep Learning} (\cite{Lecun2015deep})
      \item \textbf{Deep Learning course} \url{https://www.udacity.com/course/deep-learning--ud730}
      \item \textbf{Two Minute Papers} \url{https://www.youtube.com/user/keeroyz}
      \item \textbf{Applications of Deep Learning} \url{https://youtu.be/hPKJBXkyTKM}
    \end{itemize}

    Neuroscience:
    \begin{itemize}
      \item \url{http://www.brainfacts.org/}
    \end{itemize}
  \end{frame}

  \begin{frame}[allowframebreaks]{References}
    \tiny
    \printbibliography[heading=none]
  \end{frame}

\end{document}
